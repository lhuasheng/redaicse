{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# CNN Assignment\n",
    "\n",
    "In this assignement, we transfer learn a model that was trained on the 40 class to the 200 class network. We also created a generator to handle the large training set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "from keras.models import load_model\n",
    "from keras.layers import *\n",
    "import keras\n",
    "import numpy as np\n",
    "import keras"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "model = load_model('./quickDrawModel.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_1 (InputLayer)         (None, 784)               0         \n",
      "_________________________________________________________________\n",
      "reshape_1 (Reshape)          (None, 28, 28, 1)         0         \n",
      "_________________________________________________________________\n",
      "conv2d_1 (Conv2D)            (None, 28, 28, 64)        640       \n",
      "_________________________________________________________________\n",
      "batch_normalization_2 (Batch (None, 28, 28, 64)        256       \n",
      "_________________________________________________________________\n",
      "dropout_1 (Dropout)          (None, 28, 28, 64)        0         \n",
      "_________________________________________________________________\n",
      "activation_1 (Activation)    (None, 28, 28, 64)        0         \n",
      "_________________________________________________________________\n",
      "max_pooling2d_1 (MaxPooling2 (None, 14, 14, 64)        0         \n",
      "_________________________________________________________________\n",
      "conv2d_2 (Conv2D)            (None, 14, 14, 64)        36928     \n",
      "_________________________________________________________________\n",
      "batch_normalization_3 (Batch (None, 14, 14, 64)        256       \n",
      "_________________________________________________________________\n",
      "dropout_2 (Dropout)          (None, 14, 14, 64)        0         \n",
      "_________________________________________________________________\n",
      "activation_2 (Activation)    (None, 14, 14, 64)        0         \n",
      "_________________________________________________________________\n",
      "max_pooling2d_2 (MaxPooling2 (None, 7, 7, 64)          0         \n",
      "_________________________________________________________________\n",
      "conv2d_3 (Conv2D)            (None, 7, 7, 128)         73856     \n",
      "_________________________________________________________________\n",
      "batch_normalization_4 (Batch (None, 7, 7, 128)         512       \n",
      "_________________________________________________________________\n",
      "dropout_3 (Dropout)          (None, 7, 7, 128)         0         \n",
      "_________________________________________________________________\n",
      "activation_3 (Activation)    (None, 7, 7, 128)         0         \n",
      "_________________________________________________________________\n",
      "max_pooling2d_3 (MaxPooling2 (None, 3, 3, 128)         0         \n",
      "_________________________________________________________________\n",
      "flatten_1 (Flatten)          (None, 1152)              0         \n",
      "_________________________________________________________________\n",
      "Dense_1 (Dense)              (None, 512)               590336    \n",
      "_________________________________________________________________\n",
      "dropout_4 (Dropout)          (None, 512)               0         \n",
      "_________________________________________________________________\n",
      "Dense_2 (Dense)              (None, 512)               262656    \n",
      "_________________________________________________________________\n",
      "Outputlayer (Dense)          (None, 40)                20520     \n",
      "=================================================================\n",
      "Total params: 985,960\n",
      "Trainable params: 985,448\n",
      "Non-trainable params: 512\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\n",
    "getInter = model.get_layer('flatten_1').output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "x = Dense(4096,activation='relu',name='fcnew')(getInter)\n",
    "x=Dropout(0.2,name='drnew')(x)\n",
    "x = Dense(4096,activation='relu',name='fcnew1')(x)\n",
    "x=Dropout(0.5,name='drnew1')(x)\n",
    "x = Dense(200,activation='softmax',name = 'output')(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from keras.models import Model\n",
    "newModel = Model(model.input,x)\n",
    "newModel.compile(loss='categorical_crossentropy',\n",
    "              optimizer='adam',\n",
    "              metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "num_examples_per_class = 5000\n",
    "\n",
    "\n",
    "classes = ['aircraft carrier','airplane','alarm clock','ambulance','angel','ant',\n",
    "'anvil','apple','axe','banana','bandage','barn','baseball bat',\n",
    "'baseball','basket','basketball','bathtub','beach','bear','beard',\n",
    "'bed','bee','belt','bicycle','binoculars','birthday cake',\n",
    "'blueberry','book','boomerang','bottlecap','bowtie','bracelet',\n",
    "'brain','bread','broom','bulldozer','bus','bush','butterfly',\n",
    "'cactus','cake','calculator','calendar','camel','camera','campfire',\n",
    "'candle','cannon','canoe','car','carrot','cat','cello','chandelier',\n",
    "'clock','cloud','coffee cup','compass','computer','cookie','couch',\n",
    "'cow','crab','crayon','crocodile','crown','cup','diamond','dog',\n",
    "'dolphin','donut','dragon','dresser','drill','drums','duck','dumbbell','ear','elbow','elephant','envelope','eraser',\n",
    "'eye','eyeglasses','face','fan','feather','fence','finger','fire hydrant',\n",
    "'fireplace','firetruck','fish','flamingo','flashlight','flip flops',\n",
    "'floor lamp','flower','flying saucer','foot','fork','frog','frying pan',\n",
    "'garden hose','garden','giraffe','goatee','golf club','grapes','grass',\n",
    "'guitar','hamburger','hammer','hand','harp','hat','headphones',\n",
    "'hedgehog','helicopter','helmet','hexagon','hockey puck',\n",
    "'hockey stick','horse','hospital','hot air balloon','hot dog',\n",
    "'hot tub','hourglass','house plant','house','hurricane',\n",
    "'ice cream','jacket','jail','kangaroo','key','keyboard','knee',\n",
    "'knife','ladder','lantern','laptop','leaf','leg','light bulb',\n",
    "'lighter','lighthouse','lightning','line','lion','lipstick','lobster',\n",
    "'lollipop','mailbox','map','marker','matches','megaphone',\n",
    "'mermaid','microphone','microwave','monkey',\n",
    "'moon','mosquito','motorbike','mountain','mouse','moustache',\n",
    "'mouth','mug','mushroom','nail','necklace','nose','ocean',\n",
    "'octagon','octopus','onion','oven','owl',\n",
    "'paint can','paintbrush','palm tree','panda','pants',\n",
    "'paper clip','parachute','parrot','passport','peanut',\n",
    "'pear','peas','pencil','penguin','piano','pickup truck',\n",
    "'picture frame','pig','pillow']\n",
    "x_data = np.load(\"./x_data_200_classes_5k.npy\")\n",
    "\n",
    "labels = [np.full((num_examples_per_class,), classes.index(qdraw)) for qdraw in classes]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "## Concat the arrays together\n",
    "y_data = np.concatenate(labels,axis=0)\n",
    "y_data.shape\n",
    "\n",
    "\n",
    "y_data = keras.utils.to_categorical(y_data,200)\n",
    "\n",
    "dataOrder =  np.random.permutation(x_data.shape[0])\n",
    "trainIdx = dataOrder[0:np.floor(0.8*dataOrder.shape[0]).astype('int32')]\n",
    "testIdx = dataOrder[(np.floor(0.8*dataOrder.shape[0]).astype('int32')+1):-1]\n",
    "\n",
    "def myGeneratorBasic(x_data,y_data,batchSize,dataOrder):\n",
    "    while True:\n",
    "        np.random.shuffle(dataOrder)\n",
    "        for i in range(0, len(dataOrder), batchSize):\n",
    "            xTrain = x_data[dataOrder[i:i+batchSize]]\n",
    "            yTrain = y_data[dataOrder[i:i+batchSize]]\n",
    "            yield({'input_1':np.array(xTrain)},{'output':np.array(yTrain)})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/50\n",
      "400/400 [==============================] - 74s - loss: 3.4742 - acc: 0.2446    \n",
      "Epoch 2/50\n",
      "400/400 [==============================] - 66s - loss: 2.4049 - acc: 0.4240    \n",
      "Epoch 3/50\n",
      "400/400 [==============================] - 66s - loss: 2.1983 - acc: 0.4723    \n",
      "Epoch 4/50\n",
      "400/400 [==============================] - 66s - loss: 2.1059 - acc: 0.4961    \n",
      "Epoch 5/50\n",
      "400/400 [==============================] - 66s - loss: 2.0176 - acc: 0.5128    \n",
      "Epoch 6/50\n",
      "400/400 [==============================] - 69s - loss: 1.9641 - acc: 0.5248    \n",
      "Epoch 7/50\n",
      "400/400 [==============================] - 73s - loss: 1.9291 - acc: 0.5405    \n",
      "Epoch 8/50\n",
      "400/400 [==============================] - 71s - loss: 1.8750 - acc: 0.5505    \n",
      "Epoch 9/50\n",
      "400/400 [==============================] - 71s - loss: 1.8444 - acc: 0.5505    \n",
      "Epoch 10/50\n",
      "400/400 [==============================] - 67s - loss: 1.8434 - acc: 0.5590    \n",
      "Epoch 11/50\n",
      "400/400 [==============================] - 71s - loss: 1.8195 - acc: 0.5577    \n",
      "Epoch 12/50\n",
      "400/400 [==============================] - 67s - loss: 1.7925 - acc: 0.5642    \n",
      "Epoch 13/50\n",
      "400/400 [==============================] - 68s - loss: 1.7999 - acc: 0.5666    \n",
      "Epoch 14/50\n",
      "400/400 [==============================] - 71s - loss: 1.7957 - acc: 0.5723    \n",
      "Epoch 15/50\n",
      "400/400 [==============================] - 68s - loss: 1.7611 - acc: 0.5793    \n",
      "Epoch 16/50\n",
      "400/400 [==============================] - 69s - loss: 1.7616 - acc: 0.5693    - ETA: 4s - los\n",
      "Epoch 17/50\n",
      "400/400 [==============================] - 70s - loss: 1.7356 - acc: 0.5816    \n",
      "Epoch 18/50\n",
      "400/400 [==============================] - 66s - loss: 1.7546 - acc: 0.5754    \n",
      "Epoch 19/50\n",
      "400/400 [==============================] - 66s - loss: 1.7680 - acc: 0.5752    \n",
      "Epoch 20/50\n",
      "400/400 [==============================] - 66s - loss: 1.7489 - acc: 0.5810    \n",
      "Epoch 21/50\n",
      "400/400 [==============================] - 66s - loss: 1.7483 - acc: 0.5738    \n",
      "Epoch 22/50\n",
      "400/400 [==============================] - 66s - loss: 1.7078 - acc: 0.5824    \n",
      "Epoch 23/50\n",
      "400/400 [==============================] - 66s - loss: 1.7373 - acc: 0.5819    \n",
      "Epoch 24/50\n",
      "400/400 [==============================] - 67s - loss: 1.6750 - acc: 0.5961    \n",
      "Epoch 25/50\n",
      "400/400 [==============================] - 66s - loss: 1.6841 - acc: 0.5906    \n",
      "Epoch 26/50\n",
      "400/400 [==============================] - 67s - loss: 1.7220 - acc: 0.5833    \n",
      "Epoch 27/50\n",
      "400/400 [==============================] - 66s - loss: 1.6710 - acc: 0.5998    \n",
      "Epoch 28/50\n",
      "400/400 [==============================] - 66s - loss: 1.6664 - acc: 0.5941    \n",
      "Epoch 29/50\n",
      "400/400 [==============================] - 67s - loss: 1.7269 - acc: 0.5891    \n",
      "Epoch 30/50\n",
      "400/400 [==============================] - 67s - loss: 1.7083 - acc: 0.5895    \n",
      "Epoch 31/50\n",
      "400/400 [==============================] - 67s - loss: 1.6846 - acc: 0.5941    \n",
      "Epoch 32/50\n",
      " 15/400 [>.............................] - ETA: 63s - loss: 1.5691 - acc: 0.5958"
     ]
    }
   ],
   "source": [
    "history = newModel.fit_generator(myGeneratorBasic(x_data,y_data,32,trainIdx),400,50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
