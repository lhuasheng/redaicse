{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.0.5\n"
     ]
    }
   ],
   "source": [
    "import keras\n",
    "from __future__ import print_function\n",
    "from functools import reduce\n",
    "import re\n",
    "import tarfile\n",
    "import numpy as np\n",
    "from keras.utils.data_utils import get_file\n",
    "from keras.utils import to_categorical\n",
    "from keras.layers.embeddings import Embedding\n",
    "from keras import layers\n",
    "from keras.layers import *\n",
    "from keras.models import Model\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "from keras.callbacks import ReduceLROnPlateau\n",
    "print(keras.__version__)\n",
    "\n",
    "def tokenize(sent):\n",
    "    '''Return the tokens of a sentence including punctuation.\n",
    "    >>> tokenize('Bob dropped the apple. Where is the apple?')\n",
    "    ['Bob', 'dropped', 'the', 'apple', '.', 'Where', 'is', 'the', 'apple', '?']\n",
    "    '''\n",
    "    return [x.strip() for x in re.split('(\\W+)?', sent) if x.strip()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RNN / Embed / Sent / Query = <class 'keras.layers.recurrent.LSTM'>, 50, 100, 100\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\HuaSheng\\AppData\\Local\\conda\\conda\\envs\\tfKrNv\\lib\\re.py:212: FutureWarning: split() requires a non-empty pattern match.\n",
      "  return _compile(pattern, flags).split(string, maxsplit)\n"
     ]
    }
   ],
   "source": [
    "def parse_stories(lines, only_supporting=False):\n",
    "    '''Parse stories provided in the bAbi tasks format\n",
    "    If only_supporting is true,\n",
    "    only the sentences that support the answer are kept.\n",
    "    '''\n",
    "    data = []\n",
    "    story = []\n",
    "    for line in lines:\n",
    "        #print(line)\n",
    "        #line = line.decode('utf-8').strip()\n",
    "        nid, line = line.split(' ', 1)\n",
    "        #print([nid,line])\n",
    "        nid = int(nid)\n",
    "        if nid == 1:\n",
    "            story = []\n",
    "        if '\\t' in line:\n",
    "            q, a, supporting = line.split('\\t')\n",
    "            q = tokenize(q)\n",
    "            substory = None\n",
    "            if only_supporting:\n",
    "                # Only select the related substory\n",
    "                supporting = map(int, supporting.split())\n",
    "                substory = [story[i - 1] for i in supporting]\n",
    "            else:\n",
    "                # Provide all the substories\n",
    "                substory = [ x for x in story if x]\n",
    "            data.append((substory, q, a))\n",
    "            story.append('')\n",
    "        else:\n",
    "            sent = tokenize(line)\n",
    "            story.append(sent)\n",
    "    return data\n",
    "\n",
    "\n",
    "def get_stories(f, only_supporting=False, max_length=None):\n",
    "    '''Given a file name, read the file, retrieve the stories,\n",
    "    and then convert the sentences into a single story.\n",
    "    If max_length is supplied,\n",
    "    any stories longer than max_length tokens will be discarded.\n",
    "    '''\n",
    "    data = parse_stories(f.readlines(), only_supporting=only_supporting)\n",
    "    #print(data[0])\n",
    "    flatten = lambda data: reduce(lambda x, y: x + y, data)\n",
    "    data = [(flatten(story), q, answer) for story, q, answer in data if not max_length or len(flatten(story)) < max_length]\n",
    "    #print(data)\n",
    "    return data\n",
    "\n",
    "\n",
    "def vectorize_stories(data, word_idx, story_maxlen, query_maxlen):\n",
    "    xs = []\n",
    "    xqsi = []\n",
    "    xqso = []\n",
    "    ys = []\n",
    "    for story, query, answer in data:\n",
    "        x = [word_idx[w] for w in story]\n",
    "        x += [word_idx['\\n']]\n",
    "        xq = [word_idx[w] for w in query]\n",
    "        xqi = [word_idx['\\t']]\n",
    "        xqi = xqi + xq\n",
    "        xqo = xq  +[word_idx['\\n']]\n",
    "        y=word_idx[answer]\n",
    "        xs.append(x)\n",
    "        xqsi.append(xqi)\n",
    "        xqso.append(xqo)\n",
    "        ys.append(y)\n",
    "    return pad_sequences(xs, maxlen=story_maxlen,padding='post'), pad_sequences(xqsi, maxlen=query_maxlen,padding='post'),pad_sequences(xqso, maxlen=query_maxlen), np.array(ys)\n",
    "\n",
    "RNN = recurrent.LSTM\n",
    "EMBED_HIDDEN_SIZE = 50\n",
    "SENT_HIDDEN_SIZE = 100\n",
    "QUERY_HIDDEN_SIZE = 100\n",
    "BATCH_SIZE = 32\n",
    "EPOCHS = 40\n",
    "print('RNN / Embed / Sent / Query = {}, {}, {}, {}'.format(RNN,\n",
    "                                                           EMBED_HIDDEN_SIZE,\n",
    "                                                           SENT_HIDDEN_SIZE,\n",
    "                                                           QUERY_HIDDEN_SIZE))\n",
    "trainFile = open('./qa5_three-arg-relations_train.txt','r')\n",
    "#print(trainFile.readlines())\n",
    "train = get_stories(trainFile)\n",
    "test = get_stories(open('./qa5_three-arg-relations_test.txt'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(['Fred',\n",
       "  'picked',\n",
       "  'up',\n",
       "  'the',\n",
       "  'football',\n",
       "  'there',\n",
       "  '.',\n",
       "  'Fred',\n",
       "  'gave',\n",
       "  'the',\n",
       "  'football',\n",
       "  'to',\n",
       "  'Jeff',\n",
       "  '.'],\n",
       " ['What', 'did', 'Fred', 'give', 'to', 'Jeff', '?'],\n",
       " 'football')"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "vocab = set()\n",
    "vocab |=set(('\\t','\\n'))\n",
    "for story, q, answer in train + test:\n",
    "    vocab |= set(story + q + [answer])\n",
    "vocab = sorted(vocab)\n",
    "\n",
    "# Reserve 0 for masking via pad_sequences\n",
    "vocab_size = len(vocab) + 1\n",
    "word_idx = dict((c, i + 1) for i, c in enumerate(vocab))\n",
    "inv_map = {v: k for k, v in word_idx.items()}\n",
    "story_maxlen = max(map(len, (x for x, _, _ in train + test)))\n",
    "query_maxlen = max(map(len, (x for _, x, _ in train + test)))\n",
    "\n",
    "x, xqi,xqo, y = vectorize_stories(train, word_idx, story_maxlen, query_maxlen)\n",
    "tx, txqi,txqo, ty = vectorize_stories(test, word_idx, story_maxlen, query_maxlen)\n",
    "temp = np.zeros((x.shape[0], query_maxlen, vocab_size),dtype='float32')\n",
    "\n",
    "def token2OHE(xqo):\n",
    "    for i,idx in enumerate(xqo):\n",
    "        for u,v in enumerate(idx):\n",
    "            temp [i,u, v] = 1.\n",
    "    return temp\n",
    "\n",
    "xqo=token2OHE(xqo)\n",
    "txqo=token2OHE(txqo)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Bill', 'travelled', 'to', 'the', 'office', '.', 'Bill', 'picked', 'up', 'the', 'football', 'there', '.', 'Bill', 'went', 'to', 'the', 'bedroom', '.', 'Bill', 'gave', 'the', 'football', 'to', 'Fred', '.', 'Fred', 'handed', 'the', 'football', 'to', 'Bill', '.', 'Jeff', 'went', 'back', 'to', 'the', 'office', '.', '\\n']\n",
      "['\\t', 'Who', 'received', 'the', 'football', '?']\n",
      "Bill\n"
     ]
    }
   ],
   "source": [
    "print([inv_map[i] for i in x[1] if i !=0])\n",
    "print([inv_map[i] for i in xqi[1] if i !=0])\n",
    "print(inv_map[y[1]] )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "vocab = ['\\t', '\\n', '.', '?', 'Bill', 'Fred', 'Jeff', 'Mary', 'What', 'Who', 'apple', 'back', 'bathroom', 'bedroom', 'did', 'discarded', 'down', 'dropped', 'football', 'garden', 'gave', 'give', 'got', 'grabbed', 'hallway', 'handed', 'journeyed', 'kitchen', 'left', 'milk', 'moved', 'office', 'passed', 'picked', 'put', 'received', 'the', 'there', 'to', 'took', 'travelled', 'up', 'went']\n",
      "x.shape = (1000, 627)\n",
      "xq.shape = (1000, 8)\n",
      "xq.shape = (1000, 8, 44)\n",
      "y.shape = (1000,)\n",
      "story_maxlen, query_maxlen = 627, 8\n",
      "Tensor(\"input_4:0\", shape=(?, 627), dtype=float32)\n",
      "Tensor(\"embedding_2/Gather:0\", shape=(?, 627, 80), dtype=float32)\n",
      "Tensor(\"LSTM_01_2/transpose_1:0\", shape=(?, ?, 50), dtype=float32)\n",
      "Tensor(\"time_distributed_2/Reshape_1:0\", shape=(?, 8, 44), dtype=float32)\n",
      "____________________________________________________________________________________________________\n",
      "Layer (type)                     Output Shape          Param #     Connected to                     \n",
      "====================================================================================================\n",
      "decoder_inputs (InputLayer)      (None, 8)             0                                            \n",
      "____________________________________________________________________________________________________\n",
      "input_4 (InputLayer)             (None, 627)           0                                            \n",
      "____________________________________________________________________________________________________\n",
      "embedding_2 (Embedding)          multiple              3520        input_4[0][0]                    \n",
      "                                                                   decoder_inputs[0][0]             \n",
      "____________________________________________________________________________________________________\n",
      "lstm_2 (LSTM)                    [(None, 50), (None, 5 26200       embedding_2[0][0]                \n",
      "____________________________________________________________________________________________________\n",
      "LSTM_01 (LSTM)                   [(None, 8, 50), (None 26200       embedding_2[1][0]                \n",
      "                                                                   lstm_2[0][1]                     \n",
      "                                                                   lstm_2[0][2]                     \n",
      "____________________________________________________________________________________________________\n",
      "time_distributed_2 (TimeDistribu (None, 8, 44)         2244        LSTM_01[0][0]                    \n",
      "====================================================================================================\n",
      "Total params: 58,164\n",
      "Trainable params: 58,164\n",
      "Non-trainable params: 0\n",
      "____________________________________________________________________________________________________\n",
      "Train on 800 samples, validate on 200 samples\n",
      "Epoch 1/50\n",
      "800/800 [==============================] - 18s - loss: 5.7662 - acc: 0.1608 - val_loss: 4.9986 - val_acc: 0.1713\n",
      "Epoch 2/50\n",
      "800/800 [==============================] - 14s - loss: 4.6450 - acc: 0.1952 - val_loss: 4.4151 - val_acc: 0.2994\n",
      "Epoch 3/50\n",
      "800/800 [==============================] - 12s - loss: 4.2325 - acc: 0.3703 - val_loss: 4.1326 - val_acc: 0.3294\n",
      "Epoch 4/50\n",
      "800/800 [==============================] - 12s - loss: 3.9772 - acc: 0.3745 - val_loss: 3.8888 - val_acc: 0.4288\n",
      "Epoch 5/50\n",
      "800/800 [==============================] - 12s - loss: 3.7547 - acc: 0.3789 - val_loss: 3.7077 - val_acc: 0.3006\n",
      "Epoch 6/50\n",
      "800/800 [==============================] - 12s - loss: 3.5672 - acc: 0.3802 - val_loss: 3.5326 - val_acc: 0.4531\n",
      "Epoch 7/50\n",
      "800/800 [==============================] - 12s - loss: 3.4196 - acc: 0.3900 - val_loss: 3.4189 - val_acc: 0.3056\n",
      "Epoch 8/50\n",
      "800/800 [==============================] - 12s - loss: 3.3136 - acc: 0.3852 - val_loss: 3.3055 - val_acc: 0.4888\n",
      "Epoch 9/50\n",
      "800/800 [==============================] - 13s - loss: 3.2102 - acc: 0.4270 - val_loss: 3.2540 - val_acc: 0.3013\n",
      "Epoch 10/50\n",
      "800/800 [==============================] - 12s - loss: 3.1303 - acc: 0.4353 - val_loss: 3.1548 - val_acc: 0.5125\n",
      "Epoch 11/50\n",
      "800/800 [==============================] - 12s - loss: 3.0581 - acc: 0.4602 - val_loss: 3.0881 - val_acc: 0.3963\n",
      "Epoch 12/50\n",
      "800/800 [==============================] - 12s - loss: 3.0048 - acc: 0.4569 - val_loss: 3.0126 - val_acc: 0.5081\n",
      "Epoch 13/50\n",
      "800/800 [==============================] - 12s - loss: 2.9318 - acc: 0.4684 - val_loss: 2.9518 - val_acc: 0.4625\n",
      "Epoch 14/50\n",
      "800/800 [==============================] - 14s - loss: 2.8746 - acc: 0.4930 - val_loss: 2.8836 - val_acc: 0.4906\n",
      "Epoch 15/50\n",
      "800/800 [==============================] - 15s - loss: 2.8357 - acc: 0.4856 - val_loss: 2.8464 - val_acc: 0.4906\n",
      "Epoch 16/50\n",
      "800/800 [==============================] - 13s - loss: 2.7656 - acc: 0.5225 - val_loss: 2.7843 - val_acc: 0.5281\n",
      "Epoch 17/50\n",
      "800/800 [==============================] - 13s - loss: 2.7188 - acc: 0.5219 - val_loss: 2.8025 - val_acc: 0.4506\n",
      "Epoch 18/50\n",
      "800/800 [==============================] - 13s - loss: 2.6646 - acc: 0.5139 - val_loss: 2.6849 - val_acc: 0.5162\n",
      "Epoch 19/50\n",
      "800/800 [==============================] - 14s - loss: 2.6287 - acc: 0.5114 - val_loss: 2.7266 - val_acc: 0.4506\n",
      "Epoch 20/50\n",
      "800/800 [==============================] - 17s - loss: 2.5732 - acc: 0.5136 - val_loss: 2.5982 - val_acc: 0.4844\n",
      "Epoch 21/50\n",
      "800/800 [==============================] - 18s - loss: 2.5146 - acc: 0.5322 - val_loss: 2.6238 - val_acc: 0.4888\n",
      "Epoch 22/50\n",
      "800/800 [==============================] - 12s - loss: 2.5170 - acc: 0.5100 - val_loss: 2.5204 - val_acc: 0.4981\n",
      "Epoch 23/50\n",
      "640/800 [=======================>......] - ETA: 2s - loss: 2.4192 - acc: 0.5402"
     ]
    }
   ],
   "source": [
    "print('vocab = {}'.format(vocab))\n",
    "print('x.shape = {}'.format(x.shape))\n",
    "print('xq.shape = {}'.format(xqi.shape))\n",
    "print('xq.shape = {}'.format(xqo.shape))\n",
    "print('y.shape = {}'.format(y.shape))\n",
    "print('story_maxlen, query_maxlen = {}, {}'.format(story_maxlen, query_maxlen))\n",
    "\n",
    "embeddingLayer =  Embedding(vocab_size, 80)\n",
    "\n",
    "encoder_inputs = Input(shape=( story_maxlen ,)) \n",
    "print(encoder_inputs)\n",
    "eInp =embeddingLayer(encoder_inputs)\n",
    "print(eInp)\n",
    "encoder = LSTM(50,return_state=True)\n",
    "encoder_outputs, state_h, state_c = encoder(eInp)\n",
    "encoder_states = [state_h, state_c]\n",
    "\n",
    "decoder_inputs = Input(shape=(query_maxlen,),name='decoder_inputs')\n",
    "dInp = embeddingLayer(decoder_inputs)\n",
    "decoder_lstm = LSTM(50, return_sequences=True, return_state=True,name='LSTM_01') \n",
    "decoder_outputs, _, _ = decoder_lstm(dInp , initial_state=encoder_states)\n",
    "print(decoder_outputs)\n",
    "decoder_dense = TimeDistributed(Dense(vocab_size, activation='softmax'))\n",
    "decoder_outputs = decoder_dense(decoder_outputs)\n",
    "print(decoder_outputs)\n",
    "model = Model([encoder_inputs, decoder_inputs], decoder_outputs)\n",
    "model.summary()\n",
    "model.compile(optimizer='rmsprop', loss='categorical_crossentropy',metrics=['accuracy'])\n",
    "reduce_LR = ReduceLROnPlateau(monitor='val_loss',factor = 0.9, patience=3,cooldown=2, min_lr = 0.00001)\n",
    "model.fit([x, xqi], xqo,\n",
    "          batch_size=128,\n",
    "          epochs=50,\n",
    "          callbacks=[reduce_LR],\n",
    "          validation_split=0.2)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "model.fit([x, xqi], xqo,\n",
    "          batch_size=49,\n",
    "          epochs=1,\n",
    "          callbacks=[reduce_LR],\n",
    "          validation_split=0.2)\n",
    "model.evaluate([tx, txqi], txqo,\n",
    "          batch_size=49)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "encoder_model = Model(encoder_inputs, encoder_states)\n",
    "decoder_state_input_h = Input(shape=(50,))\n",
    "decoder_state_input_c = Input(shape=(50,))\n",
    "decoder_states_inputs = [decoder_state_input_h, decoder_state_input_c]\n",
    "decoder_outputs, state_h, state_c = decoder_lstm(dInp, initial_state=decoder_states_inputs)\n",
    "decoder_states = [state_h, state_c]\n",
    "decoder_outputs = decoder_dense(decoder_outputs)\n",
    "decoder_model = Model(\n",
    "    [decoder_inputs] + decoder_states_inputs,\n",
    "    [decoder_outputs] + decoder_states)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1, 627)\n",
      "(1, 8)\n",
      "0\n"
     ]
    },
    {
     "ename": "KeyError",
     "evalue": "0",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-9-6166d471b203>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m     42\u001b[0m \u001b[0minput_seq\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mexpand_dims\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0maxis\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     43\u001b[0m \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0minput_seq\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 44\u001b[1;33m \u001b[0mdecoded_sentence\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mdecode_sequence\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0minput_seq\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m<ipython-input-9-6166d471b203>\u001b[0m in \u001b[0;36mdecode_sequence\u001b[1;34m(input_seq)\u001b[0m\n\u001b[0;32m     21\u001b[0m         \u001b[1;31m# Sample a token\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     22\u001b[0m         \u001b[0msampled_token_index\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0margmax\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0moutput_tokens\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mct\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m:\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 23\u001b[1;33m         \u001b[0msampled_char\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0minv_map\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0msampled_token_index\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     24\u001b[0m         \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0msampled_char\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     25\u001b[0m         \u001b[0mdecoded_sentence\u001b[0m \u001b[1;33m+=\u001b[0m \u001b[1;34m\"-\"\u001b[0m \u001b[1;33m+\u001b[0m \u001b[0msampled_char\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyError\u001b[0m: 0"
     ]
    }
   ],
   "source": [
    "def decode_sequence(input_seq):\n",
    "    # Encode the input as state vectors.\n",
    "    states_value = encoder_model.predict(input_seq)\n",
    "\n",
    "    # Generate empty target sequence of length 1.\n",
    "    target_seq = np.zeros(( 1, query_maxlen))\n",
    "    \n",
    "    # Create and pass in the firsat token to start the predictions when combined with the decoeder state.\n",
    "    target_seq[ 0, 0] = word_idx['\\t']\n",
    "    print(target_seq.shape)\n",
    "    # Sampling loop for a batch of sequences\n",
    "    # (to simplify, here we assume a batch of size 1).\n",
    "    stop_condition = False #our stop condition will be '/n' which acts as <EOS>\n",
    "    decoded_sentence = ''\n",
    "    ct =0\n",
    "    while not stop_condition:\n",
    "        \n",
    "        print(ct)\n",
    "        output_tokens, h, c = decoder_model.predict([target_seq] + states_value)\n",
    "        #print(output_tokens)\n",
    "        # Sample a token\n",
    "        sampled_token_index = np.argmax(output_tokens[0,ct, :])\n",
    "        sampled_char = inv_map[sampled_token_index]\n",
    "        print(sampled_char)\n",
    "        decoded_sentence += \"-\" + sampled_char\n",
    "\n",
    "        # Exit condition: either hit max length\n",
    "        # or find stop character.\n",
    "        if (sampled_char == '\\n' or  len(decoded_sentence) > query_maxlen):\n",
    "            stop_condition = True\n",
    "            print( len(decoded_sentence))\n",
    "        ct+=1\n",
    "        # Update the target sequence (of length 1).\n",
    "        target_seq[0, ct] =sampled_token_index\n",
    "\n",
    "        # Update states\n",
    "        states_value = [h, c]\n",
    "\n",
    "\n",
    "    return decoded_sentence\n",
    "\n",
    "input_seq = np.expand_dims(x[0],axis=0)\n",
    "print(input_seq.shape)\n",
    "decoded_sentence = decode_sequence(input_seq)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "inv_map[0]='_'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1, 8)\n",
      "0\n",
      "_\n",
      "1\n",
      "\n",
      "\n",
      "2\n",
      "\n",
      "\n",
      "3\n",
      "\n",
      "\n",
      "4\n",
      "\n",
      "\n",
      "5\n",
      "\n",
      "\n",
      "6\n",
      "\n",
      "\n",
      "8\n",
      "[[ 1.  0.  2.  2.  2.  2.  2.  2.]]\n"
     ]
    }
   ],
   "source": [
    "input_seq = np.expand_dims(x[10],axis=0)\n",
    "states_value = encoder_model.predict(input_seq)\n",
    "target_seq = np.zeros(( 1, query_maxlen))\n",
    "target_seq[ 0, 0] = word_idx['\\t']\n",
    "print(target_seq.shape)\n",
    "stop_condition = False\n",
    "decoded_sentence = ' '\n",
    "ct =0\n",
    "while not stop_condition:\n",
    "    print(ct)\n",
    "    output_tokens, h, c = decoder_model.predict([target_seq] + states_value)\n",
    "    sampled_token_index = np.argmax(output_tokens[0,ct, :])\n",
    "    sampled_char = inv_map[sampled_token_index]\n",
    "    print(sampled_char)\n",
    "    decoded_sentence +=  sampled_char\n",
    "    if (  len(decoded_sentence)+1 > query_maxlen):\n",
    "        stop_condition = True\n",
    "        print( len(decoded_sentence))\n",
    "    ct+=1\n",
    "    # Update the target sequence (of length 1).\n",
    "    target_seq[0, ct] =sampled_token_index\n",
    "\n",
    "    # Update states\n",
    "    states_value = [h, c]\n",
    "print(target_seq)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(sampled_char)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "print(input_seq)\n",
    "print(decoded_sentence)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
