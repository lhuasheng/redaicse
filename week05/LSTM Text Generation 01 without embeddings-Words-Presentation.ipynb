{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import sys\n",
    "\n",
    "\n",
    "import collections\n",
    "import re\n",
    "\n",
    "import keras\n",
    "from keras.models import Sequential, Model, load_model\n",
    "from keras.layers import Dense, LSTM, Dropout, Input\n",
    "from keras.optimizers import RMSprop, Adam\n",
    "\n",
    "from keras.callbacks import ModelCheckpoint,ReduceLROnPlateau\n",
    "from keras.utils import np_utils\n",
    "\n",
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.2.1\n",
      "2.0.6\n"
     ]
    }
   ],
   "source": [
    "print(tf.__version__)\n",
    "print(keras.__version__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# load ascii text and covert to lowercase\n",
    "filename = \"wonderland.txt\"\n",
    "raw_text = open(filename).read()\n",
    "raw_text = raw_text.lower()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "raw_text = raw_text[1:]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "## Create look up tables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total Vocab:  4938\n",
      "words 26438\n",
      "total number of unique words 4938\n"
     ]
    }
   ],
   "source": [
    "# create mapping of unique chars to integers\n",
    "\n",
    "voc_cnt = collections.Counter(raw_text.split())\n",
    "vocab = sorted(voc_cnt, key=voc_cnt.get, reverse=True)\n",
    "words = sorted(list(set(raw_text.split())))\n",
    "total_words = len(raw_text.split())\n",
    "\n",
    "# Lookup tables\n",
    "word_to_int = dict((c, i) for i, c in enumerate(vocab))\n",
    "int_to_word = dict((i, c) for i, c in enumerate(vocab)) \n",
    "\n",
    "# summarize the loaded data\n",
    "\n",
    "n_vocab_words = len(words)\n",
    "\n",
    "print(\"Total Vocab: \", n_vocab_words)\n",
    "print(\"words\",total_words)\n",
    "print(\"total number of unique words\", n_vocab_words)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "wonder\n",
      "229\n",
      "4938\n"
     ]
    }
   ],
   "source": [
    "print(int_to_word[230])\n",
    "print(word_to_int['cat'])\n",
    "print(len(word_to_int))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "## Make the sequences \n",
    "\n",
    "This time we are making the look up tables based on words rather than characters this allows for "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total Patterns:  4908\n"
     ]
    }
   ],
   "source": [
    "# prepare the dataset of input to output pairs encoded as integers\n",
    "seq_length = 30\n",
    "dataX = []\n",
    "dataY = []\n",
    "split_text = raw_text.split()\n",
    "for i in range(0, len(words) - seq_length, 1):\n",
    "    seq_in = split_text[i:i + seq_length]\n",
    "    seq_out = split_text[i + seq_length]\n",
    "    dataX.append([word_to_int[word] for word in seq_in])\n",
    "    dataY.append(word_to_int[seq_out])\n",
    "n_patterns = len(dataX)\n",
    "print(\"Total Patterns: \", n_patterns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "26408"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(raw_text.split()) - seq_length"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "### Lets examine some of these sequences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[280, 2977, 40, 0, 1181, 14, 9, 318, 2, 82, 21, 441, 5, 338, 61, 15, 677, 20, 0, 1634, 1, 5, 333, 164, 2, 4610, 189, 48, 3235, 4]\n",
      "17\n"
     ]
    }
   ],
   "source": [
    "print(dataX[0])\n",
    "print(dataY[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\" chapter i. down the rabbit-hole alice was beginning to get very tired of sitting by her sister on the bank, and of having nothing to do: once or twice she \"\n",
      "had\n"
     ]
    }
   ],
   "source": [
    "print(\"\\\"\", ' '.join([int_to_word[value] for value in dataX[0]]), \"\\\"\")\n",
    "print(int_to_word[dataY[0]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2977, 40, 0, 1181, 14, 9, 318, 2, 82, 21, 441, 5, 338, 61, 15, 677, 20, 0, 1634, 1, 5, 333, 164, 2, 4610, 189, 48, 3235, 4, 17]\n",
      "1144\n"
     ]
    }
   ],
   "source": [
    "print(dataX[1])\n",
    "print(dataY[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\" i. down the rabbit-hole alice was beginning to get very tired of sitting by her sister on the bank, and of having nothing to do: once or twice she had \"\n",
      "peeped\n"
     ]
    }
   ],
   "source": [
    "print(\"\\\"\", ' '.join([int_to_word[value] for value in dataX[1]]), \"\\\"\")\n",
    "print(int_to_word[dataY[1]])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Reshaping the sequences to become timesteps into the LSTM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(4908, 30, 1)\n"
     ]
    }
   ],
   "source": [
    "# reshape X to be [samples, time steps, features]\n",
    "X = np.reshape(dataX, (n_patterns, seq_length, 1))\n",
    "print(X.shape)\n",
    "# normalize\n",
    "X = X / float(n_vocab_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# one hot encode the output variable\n",
    "y = np_utils.to_categorical(dataY)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[  5.67031187e-02]\n",
      " [  6.02875658e-01]\n",
      " [  8.10044552e-03]\n",
      " [  0.00000000e+00]\n",
      " [  2.39165654e-01]\n",
      " [  2.83515593e-03]\n",
      " [  1.82260024e-03]\n",
      " [  6.43985419e-02]\n",
      " [  4.05022276e-04]\n",
      " [  1.66059133e-02]]\n",
      "[ 0.  0.  0. ...,  0.  0.  0.]\n"
     ]
    }
   ],
   "source": [
    "print(X[0][:10])\n",
    "print(y[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "our input shape is  (30, 1)\n"
     ]
    }
   ],
   "source": [
    "inp = Input(shape=(X.shape[1], X.shape[2]))\n",
    "print('our input shape is ',(X.shape[1], X.shape[2]) )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "x = LSTM(256)(inp)\n",
    "x = Dropout(0.2)(x)\n",
    "#x = LSTM(256)(x)\n",
    "#x = Dropout(0.2)(x)\n",
    "output = Dense(y.shape[1], activation ='softmax')(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "generative_model = Model(inputs = inp, outputs=output )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "generative_model.compile(loss='categorical_crossentropy', optimizer='adam')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# define the checkpoint callback\n",
    "filepath=\"checkpoints/Word-gen-text-weights-{epoch:02d}-{loss:.4f}.hdf5\"\n",
    "checkpoint = ModelCheckpoint(filepath, monitor='loss', verbose=1, save_best_only=True, mode='min')\n",
    "reduce_LR = ReduceLROnPlateau(monitor='loss',factor = 0.9, patience=3,cooldown=2, min_lr = 0.00001)\n",
    "callbacks_list = [checkpoint,reduce_LR]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "4864/4908 [============================>.] - ETA: 0s - loss: 6.0844Epoch 00000: loss improved from inf to 6.07971, saving model to checkpoints/Word-gen-text-weights-00-6.0797.hdf5\n",
      "4908/4908 [==============================] - 1s - loss: 6.0797     \n",
      "Epoch 2/10\n",
      "4736/4908 [===========================>..] - ETA: 0s - loss: 6.0585Epoch 00001: loss improved from 6.07971 to 6.05442, saving model to checkpoints/Word-gen-text-weights-01-6.0544.hdf5\n",
      "4908/4908 [==============================] - 1s - loss: 6.0544     \n",
      "Epoch 3/10\n",
      "4864/4908 [============================>.] - ETA: 0s - loss: 6.0226Epoch 00002: loss improved from 6.05442 to 6.02307, saving model to checkpoints/Word-gen-text-weights-02-6.0231.hdf5\n",
      "4908/4908 [==============================] - 1s - loss: 6.0231     \n",
      "Epoch 4/10\n",
      "4800/4908 [============================>.] - ETA: 0s - loss: 5.9860Epoch 00003: loss improved from 6.02307 to 5.99032, saving model to checkpoints/Word-gen-text-weights-03-5.9903.hdf5\n",
      "4908/4908 [==============================] - 1s - loss: 5.9903     \n",
      "Epoch 5/10\n",
      "4864/4908 [============================>.] - ETA: 0s - loss: 5.9533Epoch 00004: loss improved from 5.99032 to 5.95189, saving model to checkpoints/Word-gen-text-weights-04-5.9519.hdf5\n",
      "4908/4908 [==============================] - 1s - loss: 5.9519     \n",
      "Epoch 6/10\n",
      "4800/4908 [============================>.] - ETA: 0s - loss: 5.9097Epoch 00005: loss improved from 5.95189 to 5.90752, saving model to checkpoints/Word-gen-text-weights-05-5.9075.hdf5\n",
      "4908/4908 [==============================] - 1s - loss: 5.9075     \n",
      "Epoch 7/10\n",
      "4864/4908 [============================>.] - ETA: 0s - loss: 5.8636Epoch 00006: loss improved from 5.90752 to 5.86493, saving model to checkpoints/Word-gen-text-weights-06-5.8649.hdf5\n",
      "4908/4908 [==============================] - 1s - loss: 5.8649     \n",
      "Epoch 8/10\n",
      "4864/4908 [============================>.] - ETA: 0s - loss: 5.8151Epoch 00007: loss improved from 5.86493 to 5.81848, saving model to checkpoints/Word-gen-text-weights-07-5.8185.hdf5\n",
      "4908/4908 [==============================] - 1s - loss: 5.8185     \n",
      "Epoch 9/10\n",
      "4736/4908 [===========================>..] - ETA: 0s - loss: 5.7619Epoch 00008: loss improved from 5.81848 to 5.77354, saving model to checkpoints/Word-gen-text-weights-08-5.7735.hdf5\n",
      "4908/4908 [==============================] - 1s - loss: 5.7735     \n",
      "Epoch 10/10\n",
      "4864/4908 [============================>.] - ETA: 0s - loss: 5.7198Epoch 00009: loss improved from 5.77354 to 5.71841, saving model to checkpoints/Word-gen-text-weights-09-5.7184.hdf5\n",
      "4908/4908 [==============================] - 1s - loss: 5.7184     \n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x7f924c10f898>"
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "generative_model.fit(X, y, epochs=10, batch_size=64, callbacks=callbacks_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "generative_model.save('Text_gen_01_words_no_embedding')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "generative_model = load_model('Text_gen_01_words_no_embedding')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/500\n",
      "4800/4908 [============================>.] - ETA: 0s - loss: 2.0954Epoch 00000: loss improved from 2.11435 to 2.09547, saving model to checkpoints/Word-gen-text-weights-00-2.0955.hdf5\n",
      "4908/4908 [==============================] - 1s - loss: 2.0955     \n",
      "Epoch 2/500\n",
      "4864/4908 [============================>.] - ETA: 0s - loss: 2.0940Epoch 00001: loss improved from 2.09547 to 2.09532, saving model to checkpoints/Word-gen-text-weights-01-2.0953.hdf5\n",
      "4908/4908 [==============================] - 1s - loss: 2.0953     \n",
      "Epoch 3/500\n",
      "4864/4908 [============================>.] - ETA: 0s - loss: 2.0555Epoch 00002: loss improved from 2.09532 to 2.05287, saving model to checkpoints/Word-gen-text-weights-02-2.0529.hdf5\n",
      "4908/4908 [==============================] - 1s - loss: 2.0529     \n",
      "Epoch 4/500\n",
      "4864/4908 [============================>.] - ETA: 0s - loss: 2.0706Epoch 00003: loss did not improve\n",
      "4908/4908 [==============================] - 1s - loss: 2.0693     \n",
      "Epoch 5/500\n",
      "4864/4908 [============================>.] - ETA: 0s - loss: 2.0178Epoch 00004: loss improved from 2.05287 to 2.02024, saving model to checkpoints/Word-gen-text-weights-04-2.0202.hdf5\n",
      "4908/4908 [==============================] - 1s - loss: 2.0202     \n",
      "Epoch 6/500\n",
      "4864/4908 [============================>.] - ETA: 0s - loss: 2.0147Epoch 00005: loss improved from 2.02024 to 2.01583, saving model to checkpoints/Word-gen-text-weights-05-2.0158.hdf5\n",
      "4908/4908 [==============================] - 1s - loss: 2.0158     \n",
      "Epoch 7/500\n",
      "4736/4908 [===========================>..] - ETA: 0s - loss: 1.9927Epoch 00006: loss improved from 2.01583 to 1.99338, saving model to checkpoints/Word-gen-text-weights-06-1.9934.hdf5\n",
      "4908/4908 [==============================] - 1s - loss: 1.9934     \n",
      "Epoch 8/500\n",
      "4864/4908 [============================>.] - ETA: 0s - loss: 1.9869Epoch 00007: loss improved from 1.99338 to 1.99091, saving model to checkpoints/Word-gen-text-weights-07-1.9909.hdf5\n",
      "4908/4908 [==============================] - 1s - loss: 1.9909     \n",
      "Epoch 9/500\n",
      "4864/4908 [============================>.] - ETA: 0s - loss: 1.9749Epoch 00008: loss improved from 1.99091 to 1.97671, saving model to checkpoints/Word-gen-text-weights-08-1.9767.hdf5\n",
      "4908/4908 [==============================] - 1s - loss: 1.9767     \n",
      "Epoch 10/500\n",
      "4736/4908 [===========================>..] - ETA: 0s - loss: 1.9617Epoch 00009: loss improved from 1.97671 to 1.96330, saving model to checkpoints/Word-gen-text-weights-09-1.9633.hdf5\n",
      "4908/4908 [==============================] - 1s - loss: 1.9633     \n",
      "Epoch 11/500\n",
      "4864/4908 [============================>.] - ETA: 0s - loss: 1.9680Epoch 00010: loss did not improve\n",
      "4908/4908 [==============================] - 1s - loss: 1.9652     \n",
      "Epoch 12/500\n",
      "4736/4908 [===========================>..] - ETA: 0s - loss: 1.9393Epoch 00011: loss improved from 1.96330 to 1.95205, saving model to checkpoints/Word-gen-text-weights-11-1.9521.hdf5\n",
      "4908/4908 [==============================] - 1s - loss: 1.9521     \n",
      "Epoch 13/500\n",
      "4736/4908 [===========================>..] - ETA: 0s - loss: 1.9391Epoch 00012: loss improved from 1.95205 to 1.93612, saving model to checkpoints/Word-gen-text-weights-12-1.9361.hdf5\n",
      "4908/4908 [==============================] - 1s - loss: 1.9361     \n",
      "Epoch 14/500\n",
      "4736/4908 [===========================>..] - ETA: 0s - loss: 1.9109Epoch 00013: loss improved from 1.93612 to 1.91431, saving model to checkpoints/Word-gen-text-weights-13-1.9143.hdf5\n",
      "4908/4908 [==============================] - 1s - loss: 1.9143     \n",
      "Epoch 15/500\n",
      "4800/4908 [============================>.] - ETA: 0s - loss: 1.8937Epoch 00014: loss improved from 1.91431 to 1.89988, saving model to checkpoints/Word-gen-text-weights-14-1.8999.hdf5\n",
      "4908/4908 [==============================] - 1s - loss: 1.8999     \n",
      "Epoch 16/500\n",
      "4800/4908 [============================>.] - ETA: 0s - loss: 1.9059Epoch 00015: loss did not improve\n",
      "4908/4908 [==============================] - 1s - loss: 1.9045     \n",
      "Epoch 17/500\n",
      "4800/4908 [============================>.] - ETA: 0s - loss: 1.8851Epoch 00016: loss improved from 1.89988 to 1.88898, saving model to checkpoints/Word-gen-text-weights-16-1.8890.hdf5\n",
      "4908/4908 [==============================] - 1s - loss: 1.8890     \n",
      "Epoch 18/500\n",
      "4736/4908 [===========================>..] - ETA: 0s - loss: 1.8638Epoch 00017: loss improved from 1.88898 to 1.86021, saving model to checkpoints/Word-gen-text-weights-17-1.8602.hdf5\n",
      "4908/4908 [==============================] - 1s - loss: 1.8602     \n",
      "Epoch 19/500\n",
      "4864/4908 [============================>.] - ETA: 0s - loss: 1.8702Epoch 00018: loss did not improve\n",
      "4908/4908 [==============================] - 1s - loss: 1.8688     \n",
      "Epoch 20/500\n",
      "4800/4908 [============================>.] - ETA: 0s - loss: 1.8254Epoch 00019: loss improved from 1.86021 to 1.83300, saving model to checkpoints/Word-gen-text-weights-19-1.8330.hdf5\n",
      "4908/4908 [==============================] - 1s - loss: 1.8330     \n",
      "Epoch 21/500\n",
      "4736/4908 [===========================>..] - ETA: 0s - loss: 1.8394Epoch 00020: loss did not improve\n",
      "4908/4908 [==============================] - 1s - loss: 1.8402     \n",
      "Epoch 22/500\n",
      "4864/4908 [============================>.] - ETA: 0s - loss: 1.8289Epoch 00021: loss improved from 1.83300 to 1.83226, saving model to checkpoints/Word-gen-text-weights-21-1.8323.hdf5\n",
      "4908/4908 [==============================] - 1s - loss: 1.8323     \n",
      "Epoch 23/500\n",
      "4736/4908 [===========================>..] - ETA: 0s - loss: 1.8030Epoch 00022: loss improved from 1.83226 to 1.80694, saving model to checkpoints/Word-gen-text-weights-22-1.8069.hdf5\n",
      "4908/4908 [==============================] - 1s - loss: 1.8069     \n",
      "Epoch 24/500\n",
      "4864/4908 [============================>.] - ETA: 0s - loss: 1.7816Epoch 00023: loss improved from 1.80694 to 1.78424, saving model to checkpoints/Word-gen-text-weights-23-1.7842.hdf5\n",
      "4908/4908 [==============================] - 1s - loss: 1.7842     \n",
      "Epoch 25/500\n",
      "4800/4908 [============================>.] - ETA: 0s - loss: 1.7707Epoch 00024: loss improved from 1.78424 to 1.77205, saving model to checkpoints/Word-gen-text-weights-24-1.7721.hdf5\n",
      "4908/4908 [==============================] - 1s - loss: 1.7721     \n",
      "Epoch 26/500\n",
      "4800/4908 [============================>.] - ETA: 0s - loss: 1.7634Epoch 00025: loss improved from 1.77205 to 1.76520, saving model to checkpoints/Word-gen-text-weights-25-1.7652.hdf5\n",
      "4908/4908 [==============================] - 1s - loss: 1.7652     \n",
      "Epoch 27/500\n",
      "4864/4908 [============================>.] - ETA: 0s - loss: 1.7752Epoch 00026: loss did not improve\n",
      "4908/4908 [==============================] - 1s - loss: 1.7787     \n",
      "Epoch 28/500\n",
      "4800/4908 [============================>.] - ETA: 0s - loss: 1.7612Epoch 00027: loss did not improve\n",
      "4908/4908 [==============================] - 1s - loss: 1.7654     \n",
      "Epoch 29/500\n",
      "4864/4908 [============================>.] - ETA: 0s - loss: 1.7589Epoch 00028: loss improved from 1.76520 to 1.76139, saving model to checkpoints/Word-gen-text-weights-28-1.7614.hdf5\n",
      "4908/4908 [==============================] - 1s - loss: 1.7614     \n",
      "Epoch 30/500\n",
      "4736/4908 [===========================>..] - ETA: 0s - loss: 1.7235Epoch 00029: loss improved from 1.76139 to 1.72805, saving model to checkpoints/Word-gen-text-weights-29-1.7280.hdf5\n",
      "4908/4908 [==============================] - 1s - loss: 1.7280     \n",
      "Epoch 31/500\n",
      "4864/4908 [============================>.] - ETA: 0s - loss: 1.7100Epoch 00030: loss improved from 1.72805 to 1.70867, saving model to checkpoints/Word-gen-text-weights-30-1.7087.hdf5\n",
      "4908/4908 [==============================] - 1s - loss: 1.7087     \n",
      "Epoch 32/500\n",
      "4736/4908 [===========================>..] - ETA: 0s - loss: 1.6997Epoch 00031: loss improved from 1.70867 to 1.70471, saving model to checkpoints/Word-gen-text-weights-31-1.7047.hdf5\n",
      "4908/4908 [==============================] - 1s - loss: 1.7047     \n",
      "Epoch 33/500\n",
      "4800/4908 [============================>.] - ETA: 0s - loss: 1.6822Epoch 00032: loss improved from 1.70471 to 1.68592, saving model to checkpoints/Word-gen-text-weights-32-1.6859.hdf5\n",
      "4908/4908 [==============================] - 1s - loss: 1.6859     \n",
      "Epoch 34/500\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4800/4908 [============================>.] - ETA: 0s - loss: 1.6812Epoch 00033: loss did not improve\n",
      "4908/4908 [==============================] - 1s - loss: 1.6863     \n",
      "Epoch 35/500\n",
      "4864/4908 [============================>.] - ETA: 0s - loss: 1.6924Epoch 00034: loss did not improve\n",
      "4908/4908 [==============================] - 1s - loss: 1.6905     \n",
      "Epoch 36/500\n",
      "4736/4908 [===========================>..] - ETA: 0s - loss: 1.6694Epoch 00035: loss improved from 1.68592 to 1.67381, saving model to checkpoints/Word-gen-text-weights-35-1.6738.hdf5\n",
      "4908/4908 [==============================] - 1s - loss: 1.6738     \n",
      "Epoch 37/500\n",
      "4864/4908 [============================>.] - ETA: 0s - loss: 1.6492Epoch 00036: loss improved from 1.67381 to 1.64979, saving model to checkpoints/Word-gen-text-weights-36-1.6498.hdf5\n",
      "4908/4908 [==============================] - 1s - loss: 1.6498     \n",
      "Epoch 38/500\n",
      "4736/4908 [===========================>..] - ETA: 0s - loss: 1.6592Epoch 00037: loss did not improve\n",
      "4908/4908 [==============================] - 1s - loss: 1.6540     \n",
      "Epoch 39/500\n",
      "4736/4908 [===========================>..] - ETA: 0s - loss: 1.6351Epoch 00038: loss improved from 1.64979 to 1.63925, saving model to checkpoints/Word-gen-text-weights-38-1.6393.hdf5\n",
      "4908/4908 [==============================] - 1s - loss: 1.6393     \n",
      "Epoch 40/500\n",
      "4736/4908 [===========================>..] - ETA: 0s - loss: 1.6217Epoch 00039: loss improved from 1.63925 to 1.62250, saving model to checkpoints/Word-gen-text-weights-39-1.6225.hdf5\n",
      "4908/4908 [==============================] - 1s - loss: 1.6225     \n",
      "Epoch 41/500\n",
      "4736/4908 [===========================>..] - ETA: 0s - loss: 1.6235Epoch 00040: loss did not improve\n",
      "4908/4908 [==============================] - 1s - loss: 1.6275     \n",
      "Epoch 42/500\n",
      "4736/4908 [===========================>..] - ETA: 0s - loss: 1.6220Epoch 00041: loss improved from 1.62250 to 1.61955, saving model to checkpoints/Word-gen-text-weights-41-1.6195.hdf5\n",
      "4908/4908 [==============================] - 1s - loss: 1.6195     \n",
      "Epoch 43/500\n",
      "4864/4908 [============================>.] - ETA: 0s - loss: 1.6108Epoch 00042: loss improved from 1.61955 to 1.61405, saving model to checkpoints/Word-gen-text-weights-42-1.6140.hdf5\n",
      "4908/4908 [==============================] - 1s - loss: 1.6140     \n",
      "Epoch 44/500\n",
      "4800/4908 [============================>.] - ETA: 0s - loss: 1.6238Epoch 00043: loss did not improve\n",
      "4908/4908 [==============================] - 1s - loss: 1.6248     \n",
      "Epoch 45/500\n",
      "4800/4908 [============================>.] - ETA: 0s - loss: 1.5937Epoch 00044: loss improved from 1.61405 to 1.59514, saving model to checkpoints/Word-gen-text-weights-44-1.5951.hdf5\n",
      "4908/4908 [==============================] - 1s - loss: 1.5951     \n",
      "Epoch 46/500\n",
      "4736/4908 [===========================>..] - ETA: 0s - loss: 1.5720Epoch 00045: loss improved from 1.59514 to 1.57433, saving model to checkpoints/Word-gen-text-weights-45-1.5743.hdf5\n",
      "4908/4908 [==============================] - 1s - loss: 1.5743     \n",
      "Epoch 47/500\n",
      "4864/4908 [============================>.] - ETA: 0s - loss: 1.5649Epoch 00046: loss improved from 1.57433 to 1.56479, saving model to checkpoints/Word-gen-text-weights-46-1.5648.hdf5\n",
      "4908/4908 [==============================] - 1s - loss: 1.5648     \n",
      "Epoch 48/500\n",
      "4736/4908 [===========================>..] - ETA: 0s - loss: 1.5661Epoch 00047: loss improved from 1.56479 to 1.56472, saving model to checkpoints/Word-gen-text-weights-47-1.5647.hdf5\n",
      "4908/4908 [==============================] - 1s - loss: 1.5647     \n",
      "Epoch 49/500\n",
      "4864/4908 [============================>.] - ETA: 0s - loss: 1.5535Epoch 00048: loss improved from 1.56472 to 1.55302, saving model to checkpoints/Word-gen-text-weights-48-1.5530.hdf5\n",
      "4908/4908 [==============================] - 1s - loss: 1.5530     \n",
      "Epoch 50/500\n",
      "4864/4908 [============================>.] - ETA: 0s - loss: 1.5547Epoch 00049: loss did not improve\n",
      "4908/4908 [==============================] - 1s - loss: 1.5566     \n",
      "Epoch 51/500\n",
      "4864/4908 [============================>.] - ETA: 0s - loss: 1.5429Epoch 00050: loss improved from 1.55302 to 1.54399, saving model to checkpoints/Word-gen-text-weights-50-1.5440.hdf5\n",
      "4908/4908 [==============================] - 1s - loss: 1.5440     \n",
      "Epoch 52/500\n",
      "4736/4908 [===========================>..] - ETA: 0s - loss: 1.5442Epoch 00051: loss did not improve\n",
      "4908/4908 [==============================] - 1s - loss: 1.5477     \n",
      "Epoch 53/500\n",
      "4864/4908 [============================>.] - ETA: 0s - loss: 1.5244Epoch 00052: loss improved from 1.54399 to 1.52209, saving model to checkpoints/Word-gen-text-weights-52-1.5221.hdf5\n",
      "4908/4908 [==============================] - 1s - loss: 1.5221     \n",
      "Epoch 54/500\n",
      "4864/4908 [============================>.] - ETA: 0s - loss: 1.5201Epoch 00053: loss improved from 1.52209 to 1.51942, saving model to checkpoints/Word-gen-text-weights-53-1.5194.hdf5\n",
      "4908/4908 [==============================] - 1s - loss: 1.5194     \n",
      "Epoch 55/500\n",
      "4800/4908 [============================>.] - ETA: 0s - loss: 1.4984Epoch 00054: loss improved from 1.51942 to 1.49632, saving model to checkpoints/Word-gen-text-weights-54-1.4963.hdf5\n",
      "4908/4908 [==============================] - 1s - loss: 1.4963     \n",
      "Epoch 56/500\n",
      "4864/4908 [============================>.] - ETA: 0s - loss: 1.4822Epoch 00055: loss improved from 1.49632 to 1.48542, saving model to checkpoints/Word-gen-text-weights-55-1.4854.hdf5\n",
      "4908/4908 [==============================] - 1s - loss: 1.4854     \n",
      "Epoch 57/500\n",
      "4864/4908 [============================>.] - ETA: 0s - loss: 1.4848Epoch 00056: loss did not improve\n",
      "4908/4908 [==============================] - 1s - loss: 1.4872     \n",
      "Epoch 58/500\n",
      "4800/4908 [============================>.] - ETA: 0s - loss: 1.4813Epoch 00057: loss improved from 1.48542 to 1.48511, saving model to checkpoints/Word-gen-text-weights-57-1.4851.hdf5\n",
      "4908/4908 [==============================] - 1s - loss: 1.4851     \n",
      "Epoch 59/500\n",
      "4800/4908 [============================>.] - ETA: 0s - loss: 1.4876Epoch 00058: loss did not improve\n",
      "4908/4908 [==============================] - 1s - loss: 1.4859     \n",
      "Epoch 60/500\n",
      "4864/4908 [============================>.] - ETA: 0s - loss: 1.4722Epoch 00059: loss improved from 1.48511 to 1.47160, saving model to checkpoints/Word-gen-text-weights-59-1.4716.hdf5\n",
      "4908/4908 [==============================] - 1s - loss: 1.4716     \n",
      "Epoch 61/500\n",
      "4800/4908 [============================>.] - ETA: 0s - loss: 1.4676Epoch 00060: loss did not improve\n",
      "4908/4908 [==============================] - 1s - loss: 1.4756     \n",
      "Epoch 62/500\n",
      "4864/4908 [============================>.] - ETA: 0s - loss: 1.4654Epoch 00061: loss improved from 1.47160 to 1.46318, saving model to checkpoints/Word-gen-text-weights-61-1.4632.hdf5\n",
      "4908/4908 [==============================] - 1s - loss: 1.4632     \n",
      "Epoch 63/500\n",
      "4864/4908 [============================>.] - ETA: 0s - loss: 1.4308Epoch 00062: loss improved from 1.46318 to 1.43151, saving model to checkpoints/Word-gen-text-weights-62-1.4315.hdf5\n",
      "4908/4908 [==============================] - 1s - loss: 1.4315     \n",
      "Epoch 64/500\n",
      "4864/4908 [============================>.] - ETA: 0s - loss: 1.4049Epoch 00063: loss improved from 1.43151 to 1.40821, saving model to checkpoints/Word-gen-text-weights-63-1.4082.hdf5\n",
      "4908/4908 [==============================] - 1s - loss: 1.4082     \n",
      "Epoch 65/500\n",
      "4736/4908 [===========================>..] - ETA: 0s - loss: 1.4136Epoch 00064: loss did not improve\n",
      "4908/4908 [==============================] - 1s - loss: 1.4124     \n",
      "Epoch 66/500\n",
      "4800/4908 [============================>.] - ETA: 0s - loss: 1.4140Epoch 00065: loss did not improve\n",
      "4908/4908 [==============================] - 1s - loss: 1.4158     \n",
      "Epoch 67/500\n",
      "4864/4908 [============================>.] - ETA: 0s - loss: 1.3948Epoch 00066: loss improved from 1.40821 to 1.39481, saving model to checkpoints/Word-gen-text-weights-66-1.3948.hdf5\n",
      "4908/4908 [==============================] - 1s - loss: 1.3948     \n",
      "Epoch 68/500\n",
      "4800/4908 [============================>.] - ETA: 0s - loss: 1.3971Epoch 00067: loss did not improve\n",
      "4908/4908 [==============================] - 1s - loss: 1.3985     \n",
      "Epoch 69/500\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4864/4908 [============================>.] - ETA: 0s - loss: 1.3816Epoch 00068: loss improved from 1.39481 to 1.38431, saving model to checkpoints/Word-gen-text-weights-68-1.3843.hdf5\n",
      "4908/4908 [==============================] - 1s - loss: 1.3843     \n",
      "Epoch 70/500\n",
      "4864/4908 [============================>.] - ETA: 0s - loss: 1.4042Epoch 00069: loss did not improve\n",
      "4908/4908 [==============================] - 1s - loss: 1.4024     \n",
      "Epoch 71/500\n",
      "4864/4908 [============================>.] - ETA: 0s - loss: 1.3824Epoch 00070: loss did not improve\n",
      "4908/4908 [==============================] - 1s - loss: 1.3873     \n",
      "Epoch 72/500\n",
      "4736/4908 [===========================>..] - ETA: 0s - loss: 1.3595Epoch 00071: loss improved from 1.38431 to 1.36705, saving model to checkpoints/Word-gen-text-weights-71-1.3671.hdf5\n",
      "4908/4908 [==============================] - 1s - loss: 1.3671     \n",
      "Epoch 73/500\n",
      "4736/4908 [===========================>..] - ETA: 0s - loss: 1.3627Epoch 00072: loss improved from 1.36705 to 1.36527, saving model to checkpoints/Word-gen-text-weights-72-1.3653.hdf5\n",
      "4908/4908 [==============================] - 1s - loss: 1.3653     \n",
      "Epoch 74/500\n",
      "4800/4908 [============================>.] - ETA: 0s - loss: 1.3469Epoch 00073: loss improved from 1.36527 to 1.34985, saving model to checkpoints/Word-gen-text-weights-73-1.3499.hdf5\n",
      "4908/4908 [==============================] - 1s - loss: 1.3499     \n",
      "Epoch 75/500\n",
      "4800/4908 [============================>.] - ETA: 0s - loss: 1.3771- ETA: 0sEpoch 00074: loss did not improve\n",
      "4908/4908 [==============================] - 1s - loss: 1.3759     \n",
      "Epoch 76/500\n",
      "4736/4908 [===========================>..] - ETA: 0s - loss: 1.3240Epoch 00075: loss improved from 1.34985 to 1.33206, saving model to checkpoints/Word-gen-text-weights-75-1.3321.hdf5\n",
      "4908/4908 [==============================] - 1s - loss: 1.3321     \n",
      "Epoch 77/500\n",
      "4736/4908 [===========================>..] - ETA: 0s - loss: 1.3134Epoch 00076: loss improved from 1.33206 to 1.32018, saving model to checkpoints/Word-gen-text-weights-76-1.3202.hdf5\n",
      "4908/4908 [==============================] - 1s - loss: 1.3202     \n",
      "Epoch 78/500\n",
      "4736/4908 [===========================>..] - ETA: 0s - loss: 1.3239Epoch 00077: loss did not improve\n",
      "4908/4908 [==============================] - 1s - loss: 1.3270     \n",
      "Epoch 79/500\n",
      "4736/4908 [===========================>..] - ETA: 0s - loss: 1.3064Epoch 00078: loss improved from 1.32018 to 1.30635, saving model to checkpoints/Word-gen-text-weights-78-1.3063.hdf5\n",
      "4908/4908 [==============================] - 1s - loss: 1.3063     \n",
      "Epoch 80/500\n",
      "4864/4908 [============================>.] - ETA: 0s - loss: 1.3391Epoch 00079: loss did not improve\n",
      "4908/4908 [==============================] - 1s - loss: 1.3363     \n",
      "Epoch 81/500\n",
      "4864/4908 [============================>.] - ETA: 0s - loss: 1.3611Epoch 00080: loss did not improve\n",
      "4908/4908 [==============================] - 1s - loss: 1.3625     \n",
      "Epoch 82/500\n",
      "4800/4908 [============================>.] - ETA: 0s - loss: 1.2868Epoch 00081: loss improved from 1.30635 to 1.28573, saving model to checkpoints/Word-gen-text-weights-81-1.2857.hdf5\n",
      "4908/4908 [==============================] - 1s - loss: 1.2857     \n",
      "Epoch 83/500\n",
      "4864/4908 [============================>.] - ETA: 0s - loss: 1.2885Epoch 00082: loss did not improve\n",
      "4908/4908 [==============================] - 1s - loss: 1.2870     \n",
      "Epoch 84/500\n",
      "4800/4908 [============================>.] - ETA: 0s - loss: 1.2934Epoch 00083: loss did not improve\n",
      "4908/4908 [==============================] - 1s - loss: 1.2934     \n",
      "Epoch 85/500\n",
      "4800/4908 [============================>.] - ETA: 0s - loss: 1.2704Epoch 00084: loss improved from 1.28573 to 1.27196, saving model to checkpoints/Word-gen-text-weights-84-1.2720.hdf5\n",
      "4908/4908 [==============================] - 1s - loss: 1.2720     \n",
      "Epoch 86/500\n",
      "4800/4908 [============================>.] - ETA: 0s - loss: 1.2597Epoch 00085: loss improved from 1.27196 to 1.25734, saving model to checkpoints/Word-gen-text-weights-85-1.2573.hdf5\n",
      "4908/4908 [==============================] - 1s - loss: 1.2573     \n",
      "Epoch 87/500\n",
      "4800/4908 [============================>.] - ETA: 0s - loss: 1.2542Epoch 00086: loss improved from 1.25734 to 1.25322, saving model to checkpoints/Word-gen-text-weights-86-1.2532.hdf5\n",
      "4908/4908 [==============================] - 1s - loss: 1.2532     \n",
      "Epoch 88/500\n",
      "4800/4908 [============================>.] - ETA: 0s - loss: 1.2540- ETA: 0s - loss: 1.251Epoch 00087: loss did not improve\n",
      "4908/4908 [==============================] - 1s - loss: 1.2578     \n",
      "Epoch 89/500\n",
      "4736/4908 [===========================>..] - ETA: 0s - loss: 1.2669Epoch 00088: loss did not improve\n",
      "4908/4908 [==============================] - 1s - loss: 1.2662     \n",
      "Epoch 90/500\n",
      "4864/4908 [============================>.] - ETA: 0s - loss: 1.2275Epoch 00089: loss improved from 1.25322 to 1.22594, saving model to checkpoints/Word-gen-text-weights-89-1.2259.hdf5\n",
      "4908/4908 [==============================] - 1s - loss: 1.2259     \n",
      "Epoch 91/500\n",
      "4800/4908 [============================>.] - ETA: 0s - loss: 1.2327Epoch 00090: loss did not improve\n",
      "4908/4908 [==============================] - 1s - loss: 1.2346     \n",
      "Epoch 92/500\n",
      "4800/4908 [============================>.] - ETA: 0s - loss: 1.2134Epoch 00091: loss improved from 1.22594 to 1.21273, saving model to checkpoints/Word-gen-text-weights-91-1.2127.hdf5\n",
      "4908/4908 [==============================] - 1s - loss: 1.2127     \n",
      "Epoch 93/500\n",
      "4736/4908 [===========================>..] - ETA: 0s - loss: 1.2318Epoch 00092: loss did not improve\n",
      "4908/4908 [==============================] - 1s - loss: 1.2380     \n",
      "Epoch 94/500\n",
      "4864/4908 [============================>.] - ETA: 0s - loss: 1.2105Epoch 00093: loss improved from 1.21273 to 1.21218, saving model to checkpoints/Word-gen-text-weights-93-1.2122.hdf5\n",
      "4908/4908 [==============================] - 1s - loss: 1.2122     \n",
      "Epoch 95/500\n",
      "4864/4908 [============================>.] - ETA: 0s - loss: 1.2102Epoch 00094: loss improved from 1.21218 to 1.20581, saving model to checkpoints/Word-gen-text-weights-94-1.2058.hdf5\n",
      "4908/4908 [==============================] - 1s - loss: 1.2058     \n",
      "Epoch 96/500\n",
      "4864/4908 [============================>.] - ETA: 0s - loss: 1.1992Epoch 00095: loss improved from 1.20581 to 1.19978, saving model to checkpoints/Word-gen-text-weights-95-1.1998.hdf5\n",
      "4908/4908 [==============================] - 1s - loss: 1.1998     \n",
      "Epoch 97/500\n",
      "4736/4908 [===========================>..] - ETA: 0s - loss: 1.2087Epoch 00096: loss did not improve\n",
      "4908/4908 [==============================] - 1s - loss: 1.2114     \n",
      "Epoch 98/500\n",
      "4864/4908 [============================>.] - ETA: 0s - loss: 1.2005Epoch 00097: loss improved from 1.19978 to 1.19846, saving model to checkpoints/Word-gen-text-weights-97-1.1985.hdf5\n",
      "4908/4908 [==============================] - 1s - loss: 1.1985     \n",
      "Epoch 99/500\n",
      "4736/4908 [===========================>..] - ETA: 0s - loss: 1.1636Epoch 00098: loss improved from 1.19846 to 1.16808, saving model to checkpoints/Word-gen-text-weights-98-1.1681.hdf5\n",
      "4908/4908 [==============================] - 1s - loss: 1.1681     \n",
      "Epoch 100/500\n",
      "4864/4908 [============================>.] - ETA: 0s - loss: 1.1673Epoch 00099: loss did not improve\n",
      "4908/4908 [==============================] - 1s - loss: 1.1686     \n",
      "Epoch 101/500\n",
      "4864/4908 [============================>.] - ETA: 0s - loss: 1.1509Epoch 00100: loss improved from 1.16808 to 1.15120, saving model to checkpoints/Word-gen-text-weights-100-1.1512.hdf5\n",
      "4908/4908 [==============================] - 1s - loss: 1.1512     \n",
      "Epoch 102/500\n",
      "4736/4908 [===========================>..] - ETA: 0s - loss: 1.1604Epoch 00101: loss did not improve\n",
      "4908/4908 [==============================] - 1s - loss: 1.1633     \n",
      "Epoch 103/500\n",
      "4736/4908 [===========================>..] - ETA: 0s - loss: 1.1506Epoch 00102: loss did not improve\n",
      "4908/4908 [==============================] - 1s - loss: 1.1527     \n",
      "Epoch 104/500\n",
      "4800/4908 [============================>.] - ETA: 0s - loss: 1.1704Epoch 00103: loss did not improve\n",
      "4908/4908 [==============================] - 1s - loss: 1.1691     \n",
      "Epoch 105/500\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4736/4908 [===========================>..] - ETA: 0s - loss: 1.1893Epoch 00104: loss did not improve\n",
      "4908/4908 [==============================] - 1s - loss: 1.1894     \n",
      "Epoch 106/500\n",
      "4800/4908 [============================>.] - ETA: 0s - loss: 1.1262Epoch 00105: loss improved from 1.15120 to 1.13054, saving model to checkpoints/Word-gen-text-weights-105-1.1305.hdf5\n",
      "4908/4908 [==============================] - 1s - loss: 1.1305     \n",
      "Epoch 107/500\n",
      "4864/4908 [============================>.] - ETA: 0s - loss: 1.0995Epoch 00106: loss improved from 1.13054 to 1.10093, saving model to checkpoints/Word-gen-text-weights-106-1.1009.hdf5\n",
      "4908/4908 [==============================] - 1s - loss: 1.1009     \n",
      "Epoch 108/500\n",
      "4800/4908 [============================>.] - ETA: 0s - loss: 1.0936Epoch 00107: loss improved from 1.10093 to 1.09355, saving model to checkpoints/Word-gen-text-weights-107-1.0936.hdf5\n",
      "4908/4908 [==============================] - 1s - loss: 1.0936     \n",
      "Epoch 109/500\n",
      "4736/4908 [===========================>..] - ETA: 0s - loss: 1.1034Epoch 00108: loss did not improve\n",
      "4908/4908 [==============================] - 1s - loss: 1.1002     \n",
      "Epoch 110/500\n",
      "4736/4908 [===========================>..] - ETA: 0s - loss: 1.0833Epoch 00109: loss improved from 1.09355 to 1.07871, saving model to checkpoints/Word-gen-text-weights-109-1.0787.hdf5\n",
      "4908/4908 [==============================] - 1s - loss: 1.0787     \n",
      "Epoch 111/500\n",
      "4736/4908 [===========================>..] - ETA: 0s - loss: 1.0915Epoch 00110: loss did not improve\n",
      "4908/4908 [==============================] - 1s - loss: 1.0880     \n",
      "Epoch 112/500\n",
      "4800/4908 [============================>.] - ETA: 0s - loss: 1.1018- ETA: 0s Epoch 00111: loss did not improve\n",
      "4908/4908 [==============================] - 1s - loss: 1.1035     \n",
      "Epoch 113/500\n",
      "4864/4908 [============================>.] - ETA: 0s - loss: 1.0649Epoch 00112: loss improved from 1.07871 to 1.06534, saving model to checkpoints/Word-gen-text-weights-112-1.0653.hdf5\n",
      "4908/4908 [==============================] - 1s - loss: 1.0653     \n",
      "Epoch 114/500\n",
      "4736/4908 [===========================>..] - ETA: 0s - loss: 1.1031Epoch 00113: loss did not improve\n",
      "4908/4908 [==============================] - 1s - loss: 1.1062     \n",
      "Epoch 115/500\n",
      "4864/4908 [============================>.] - ETA: 0s - loss: 1.0820Epoch 00114: loss did not improve\n",
      "4908/4908 [==============================] - 1s - loss: 1.0817     \n",
      "Epoch 116/500\n",
      "4800/4908 [============================>.] - ETA: 0s - loss: 1.0570Epoch 00115: loss improved from 1.06534 to 1.05500, saving model to checkpoints/Word-gen-text-weights-115-1.0550.hdf5\n",
      "4908/4908 [==============================] - 1s - loss: 1.0550     \n",
      "Epoch 117/500\n",
      "4800/4908 [============================>.] - ETA: 0s - loss: 1.0799Epoch 00116: loss did not improve\n",
      "4908/4908 [==============================] - 1s - loss: 1.0796     \n",
      "Epoch 118/500\n",
      "4800/4908 [============================>.] - ETA: 0s - loss: 1.0658Epoch 00117: loss did not improve\n",
      "4908/4908 [==============================] - 1s - loss: 1.0688     \n",
      "Epoch 119/500\n",
      "4864/4908 [============================>.] - ETA: 0s - loss: 1.0664Epoch 00118: loss did not improve\n",
      "4908/4908 [==============================] - 1s - loss: 1.0682     \n",
      "Epoch 120/500\n",
      "4800/4908 [============================>.] - ETA: 0s - loss: 1.0371Epoch 00119: loss improved from 1.05500 to 1.03802, saving model to checkpoints/Word-gen-text-weights-119-1.0380.hdf5\n",
      "4908/4908 [==============================] - 1s - loss: 1.0380     \n",
      "Epoch 121/500\n",
      "4736/4908 [===========================>..] - ETA: 0s - loss: 1.0445Epoch 00120: loss did not improve\n",
      "4908/4908 [==============================] - 1s - loss: 1.0420     \n",
      "Epoch 122/500\n",
      "4736/4908 [===========================>..] - ETA: 0s - loss: 1.0408Epoch 00121: loss did not improve\n",
      "4908/4908 [==============================] - 1s - loss: 1.0441     \n",
      "Epoch 123/500\n",
      "4864/4908 [============================>.] - ETA: 0s - loss: 1.0174Epoch 00122: loss improved from 1.03802 to 1.01623, saving model to checkpoints/Word-gen-text-weights-122-1.0162.hdf5\n",
      "4908/4908 [==============================] - 1s - loss: 1.0162     \n",
      "Epoch 124/500\n",
      "4736/4908 [===========================>..] - ETA: 0s - loss: 1.0098Epoch 00123: loss did not improve\n",
      "4908/4908 [==============================] - 1s - loss: 1.0166     \n",
      "Epoch 125/500\n",
      "4800/4908 [============================>.] - ETA: 0s - loss: 1.0327Epoch 00124: loss did not improve\n",
      "4908/4908 [==============================] - 1s - loss: 1.0315     \n",
      "Epoch 126/500\n",
      "4736/4908 [===========================>..] - ETA: 0s - loss: 1.0214Epoch 00125: loss did not improve\n",
      "4908/4908 [==============================] - 1s - loss: 1.0201     \n",
      "Epoch 127/500\n",
      "4864/4908 [============================>.] - ETA: 0s - loss: 1.0127Epoch 00126: loss improved from 1.01623 to 1.01128, saving model to checkpoints/Word-gen-text-weights-126-1.0113.hdf5\n",
      "4908/4908 [==============================] - 1s - loss: 1.0113     \n",
      "Epoch 128/500\n",
      "4736/4908 [===========================>..] - ETA: 0s - loss: 1.0087Epoch 00127: loss improved from 1.01128 to 1.00784, saving model to checkpoints/Word-gen-text-weights-127-1.0078.hdf5\n",
      "4908/4908 [==============================] - 1s - loss: 1.0078     \n",
      "Epoch 129/500\n",
      "4736/4908 [===========================>..] - ETA: 0s - loss: 0.9973Epoch 00128: loss improved from 1.00784 to 0.99997, saving model to checkpoints/Word-gen-text-weights-128-1.0000.hdf5\n",
      "4908/4908 [==============================] - 1s - loss: 1.0000     \n",
      "Epoch 130/500\n",
      "4800/4908 [============================>.] - ETA: 0s - loss: 1.0062Epoch 00129: loss did not improve\n",
      "4908/4908 [==============================] - 1s - loss: 1.0084     \n",
      "Epoch 131/500\n",
      "4800/4908 [============================>.] - ETA: 0s - loss: 0.9706Epoch 00130: loss improved from 0.99997 to 0.97386, saving model to checkpoints/Word-gen-text-weights-130-0.9739.hdf5\n",
      "4908/4908 [==============================] - 1s - loss: 0.9739     \n",
      "Epoch 132/500\n",
      "4800/4908 [============================>.] - ETA: 0s - loss: 0.9910Epoch 00131: loss did not improve\n",
      "4908/4908 [==============================] - 1s - loss: 0.9930     \n",
      "Epoch 133/500\n",
      "4800/4908 [============================>.] - ETA: 0s - loss: 0.9860Epoch 00132: loss did not improve\n",
      "4908/4908 [==============================] - 1s - loss: 0.9854     \n",
      "Epoch 134/500\n",
      "4736/4908 [===========================>..] - ETA: 0s - loss: 0.9902Epoch 00133: loss did not improve\n",
      "4908/4908 [==============================] - 1s - loss: 0.9925     \n",
      "Epoch 135/500\n",
      "4864/4908 [============================>.] - ETA: 0s - loss: 0.9828Epoch 00134: loss did not improve\n",
      "4908/4908 [==============================] - 1s - loss: 0.9796     \n",
      "Epoch 136/500\n",
      "4864/4908 [============================>.] - ETA: 0s - loss: 0.9800Epoch 00135: loss did not improve\n",
      "4908/4908 [==============================] - 1s - loss: 0.9788     \n",
      "Epoch 137/500\n",
      "4800/4908 [============================>.] - ETA: 0s - loss: 0.9398Epoch 00136: loss improved from 0.97386 to 0.93744, saving model to checkpoints/Word-gen-text-weights-136-0.9374.hdf5\n",
      "4908/4908 [==============================] - 1s - loss: 0.9374     \n",
      "Epoch 138/500\n",
      "4736/4908 [===========================>..] - ETA: 0s - loss: 0.9396Epoch 00137: loss did not improve\n",
      "4908/4908 [==============================] - 1s - loss: 0.9415     \n",
      "Epoch 139/500\n",
      "4736/4908 [===========================>..] - ETA: 0s - loss: 0.9327Epoch 00138: loss improved from 0.93744 to 0.93739, saving model to checkpoints/Word-gen-text-weights-138-0.9374.hdf5\n",
      "4908/4908 [==============================] - 1s - loss: 0.9374     \n",
      "Epoch 140/500\n",
      "4864/4908 [============================>.] - ETA: 0s - loss: 0.9230Epoch 00139: loss improved from 0.93739 to 0.92399, saving model to checkpoints/Word-gen-text-weights-139-0.9240.hdf5\n",
      "4908/4908 [==============================] - 1s - loss: 0.9240     \n",
      "Epoch 141/500\n",
      "4736/4908 [===========================>..] - ETA: 0s - loss: 0.9416Epoch 00140: loss did not improve\n",
      "4908/4908 [==============================] - 1s - loss: 0.9459     \n",
      "Epoch 142/500\n",
      "4800/4908 [============================>.] - ETA: 0s - loss: 0.9185Epoch 00141: loss improved from 0.92399 to 0.91496, saving model to checkpoints/Word-gen-text-weights-141-0.9150.hdf5\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4908/4908 [==============================] - 1s - loss: 0.9150     \n",
      "Epoch 143/500\n",
      "4800/4908 [============================>.] - ETA: 0s - loss: 0.9095Epoch 00142: loss improved from 0.91496 to 0.90618, saving model to checkpoints/Word-gen-text-weights-142-0.9062.hdf5\n",
      "4908/4908 [==============================] - 1s - loss: 0.9062     \n",
      "Epoch 144/500\n",
      "4800/4908 [============================>.] - ETA: 0s - loss: 0.9193Epoch 00143: loss did not improve\n",
      "4908/4908 [==============================] - 1s - loss: 0.9174     \n",
      "Epoch 145/500\n",
      "4864/4908 [============================>.] - ETA: 0s - loss: 0.9104Epoch 00144: loss did not improve\n",
      "4908/4908 [==============================] - 1s - loss: 0.9120     \n",
      "Epoch 146/500\n",
      "4736/4908 [===========================>..] - ETA: 0s - loss: 0.8822Epoch 00145: loss improved from 0.90618 to 0.88377, saving model to checkpoints/Word-gen-text-weights-145-0.8838.hdf5\n",
      "4908/4908 [==============================] - 1s - loss: 0.8838     \n",
      "Epoch 147/500\n",
      "4736/4908 [===========================>..] - ETA: 0s - loss: 0.8979- ETA: 1sEpoch 00146: loss did not improve\n",
      "4908/4908 [==============================] - 1s - loss: 0.8974     \n",
      "Epoch 148/500\n",
      "4800/4908 [============================>.] - ETA: 0s - loss: 0.9014Epoch 00147: loss did not improve\n",
      "4908/4908 [==============================] - 1s - loss: 0.9007     \n",
      "Epoch 149/500\n",
      "4736/4908 [===========================>..] - ETA: 0s - loss: 0.9042Epoch 00148: loss did not improve\n",
      "4908/4908 [==============================] - 1s - loss: 0.9073     \n",
      "Epoch 150/500\n",
      "4736/4908 [===========================>..] - ETA: 0s - loss: 0.9067Epoch 00149: loss did not improve\n",
      "4908/4908 [==============================] - 1s - loss: 0.9138     \n",
      "Epoch 151/500\n",
      "4800/4908 [============================>.] - ETA: 0s - loss: 0.8883Epoch 00150: loss did not improve\n",
      "4908/4908 [==============================] - 1s - loss: 0.8909     \n",
      "Epoch 152/500\n",
      "4864/4908 [============================>.] - ETA: 0s - loss: 0.8791Epoch 00151: loss improved from 0.88377 to 0.87738, saving model to checkpoints/Word-gen-text-weights-151-0.8774.hdf5\n",
      "4908/4908 [==============================] - 1s - loss: 0.8774     \n",
      "Epoch 153/500\n",
      "4864/4908 [============================>.] - ETA: 0s - loss: 0.8643Epoch 00152: loss improved from 0.87738 to 0.86269, saving model to checkpoints/Word-gen-text-weights-152-0.8627.hdf5\n",
      "4908/4908 [==============================] - 1s - loss: 0.8627     \n",
      "Epoch 154/500\n",
      "4736/4908 [===========================>..] - ETA: 0s - loss: 0.8527Epoch 00153: loss improved from 0.86269 to 0.85790, saving model to checkpoints/Word-gen-text-weights-153-0.8579.hdf5\n",
      "4908/4908 [==============================] - 1s - loss: 0.8579     \n",
      "Epoch 155/500\n",
      "4800/4908 [============================>.] - ETA: 0s - loss: 0.8683Epoch 00154: loss did not improve\n",
      "4908/4908 [==============================] - 1s - loss: 0.8696     \n",
      "Epoch 156/500\n",
      "4736/4908 [===========================>..] - ETA: 0s - loss: 0.8513Epoch 00155: loss improved from 0.85790 to 0.85193, saving model to checkpoints/Word-gen-text-weights-155-0.8519.hdf5\n",
      "4908/4908 [==============================] - 1s - loss: 0.8519     \n",
      "Epoch 157/500\n",
      "4736/4908 [===========================>..] - ETA: 0s - loss: 0.8400Epoch 00156: loss improved from 0.85193 to 0.83998, saving model to checkpoints/Word-gen-text-weights-156-0.8400.hdf5\n",
      "4908/4908 [==============================] - 1s - loss: 0.8400     \n",
      "Epoch 158/500\n",
      "4736/4908 [===========================>..] - ETA: 0s - loss: 0.8376Epoch 00157: loss improved from 0.83998 to 0.83674, saving model to checkpoints/Word-gen-text-weights-157-0.8367.hdf5\n",
      "4908/4908 [==============================] - 1s - loss: 0.8367     \n",
      "Epoch 159/500\n",
      "4864/4908 [============================>.] - ETA: 0s - loss: 0.8551Epoch 00158: loss did not improve\n",
      "4908/4908 [==============================] - 1s - loss: 0.8568     \n",
      "Epoch 160/500\n",
      "4864/4908 [============================>.] - ETA: 0s - loss: 0.8379Epoch 00159: loss did not improve\n",
      "4908/4908 [==============================] - 1s - loss: 0.8379     \n",
      "Epoch 161/500\n",
      "4864/4908 [============================>.] - ETA: 0s - loss: 0.8175Epoch 00160: loss improved from 0.83674 to 0.82139, saving model to checkpoints/Word-gen-text-weights-160-0.8214.hdf5\n",
      "4908/4908 [==============================] - 1s - loss: 0.8214     \n",
      "Epoch 162/500\n",
      "4864/4908 [============================>.] - ETA: 0s - loss: 0.8383Epoch 00161: loss did not improve\n",
      "4908/4908 [==============================] - 1s - loss: 0.8399     \n",
      "Epoch 163/500\n",
      "4864/4908 [============================>.] - ETA: 0s - loss: 0.8515Epoch 00162: loss did not improve\n",
      "4908/4908 [==============================] - 1s - loss: 0.8488     \n",
      "Epoch 164/500\n",
      "4736/4908 [===========================>..] - ETA: 0s - loss: 0.8224Epoch 00163: loss did not improve\n",
      "4908/4908 [==============================] - 1s - loss: 0.8263     \n",
      "Epoch 165/500\n",
      "4864/4908 [============================>.] - ETA: 0s - loss: 0.8251Epoch 00164: loss did not improve\n",
      "4908/4908 [==============================] - 1s - loss: 0.8256     \n",
      "Epoch 166/500\n",
      "4864/4908 [============================>.] - ETA: 0s - loss: 0.8073Epoch 00165: loss improved from 0.82139 to 0.80775, saving model to checkpoints/Word-gen-text-weights-165-0.8078.hdf5\n",
      "4908/4908 [==============================] - 1s - loss: 0.8078     \n",
      "Epoch 167/500\n",
      "4736/4908 [===========================>..] - ETA: 0s - loss: 0.8145Epoch 00166: loss did not improve\n",
      "4908/4908 [==============================] - 1s - loss: 0.8142     \n",
      "Epoch 168/500\n",
      "4864/4908 [============================>.] - ETA: 0s - loss: 0.8066Epoch 00167: loss improved from 0.80775 to 0.80443, saving model to checkpoints/Word-gen-text-weights-167-0.8044.hdf5\n",
      "4908/4908 [==============================] - 1s - loss: 0.8044     \n",
      "Epoch 169/500\n",
      "4864/4908 [============================>.] - ETA: 0s - loss: 0.8094Epoch 00168: loss did not improve\n",
      "4908/4908 [==============================] - 1s - loss: 0.8112     \n",
      "Epoch 170/500\n",
      "4864/4908 [============================>.] - ETA: 0s - loss: 0.8132Epoch 00169: loss did not improve\n",
      "4908/4908 [==============================] - 1s - loss: 0.8138     \n",
      "Epoch 171/500\n",
      "4736/4908 [===========================>..] - ETA: 0s - loss: 0.7918Epoch 00170: loss improved from 0.80443 to 0.79116, saving model to checkpoints/Word-gen-text-weights-170-0.7912.hdf5\n",
      "4908/4908 [==============================] - 1s - loss: 0.7912     \n",
      "Epoch 172/500\n",
      "4736/4908 [===========================>..] - ETA: 0s - loss: 0.8013Epoch 00171: loss did not improve\n",
      "4908/4908 [==============================] - 1s - loss: 0.8045     \n",
      "Epoch 173/500\n",
      "4800/4908 [============================>.] - ETA: 0s - loss: 0.7715Epoch 00172: loss improved from 0.79116 to 0.77280, saving model to checkpoints/Word-gen-text-weights-172-0.7728.hdf5\n",
      "4908/4908 [==============================] - 1s - loss: 0.7728     \n",
      "Epoch 174/500\n",
      "4800/4908 [============================>.] - ETA: 0s - loss: 0.7979Epoch 00173: loss did not improve\n",
      "4908/4908 [==============================] - 1s - loss: 0.7987     \n",
      "Epoch 175/500\n",
      "4800/4908 [============================>.] - ETA: 0s - loss: 0.7817Epoch 00174: loss did not improve\n",
      "4908/4908 [==============================] - 1s - loss: 0.7810     \n",
      "Epoch 176/500\n",
      "4736/4908 [===========================>..] - ETA: 0s - loss: 0.7850Epoch 00175: loss did not improve\n",
      "4908/4908 [==============================] - 1s - loss: 0.7854     \n",
      "Epoch 177/500\n",
      "4736/4908 [===========================>..] - ETA: 0s - loss: 0.7636Epoch 00176: loss improved from 0.77280 to 0.76720, saving model to checkpoints/Word-gen-text-weights-176-0.7672.hdf5\n",
      "4908/4908 [==============================] - 1s - loss: 0.7672     \n",
      "Epoch 178/500\n",
      "4736/4908 [===========================>..] - ETA: 0s - loss: 0.7787Epoch 00177: loss did not improve\n",
      "4908/4908 [==============================] - 1s - loss: 0.7800     \n",
      "Epoch 179/500\n",
      "4736/4908 [===========================>..] - ETA: 0s - loss: 0.7651Epoch 00178: loss improved from 0.76720 to 0.76475, saving model to checkpoints/Word-gen-text-weights-178-0.7647.hdf5\n",
      "4908/4908 [==============================] - 1s - loss: 0.7647     \n",
      "Epoch 180/500\n",
      "4864/4908 [============================>.] - ETA: 0s - loss: 0.7732Epoch 00179: loss did not improve\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4908/4908 [==============================] - 1s - loss: 0.7755     \n",
      "Epoch 181/500\n",
      "4864/4908 [============================>.] - ETA: 0s - loss: 0.7608Epoch 00180: loss improved from 0.76475 to 0.76079, saving model to checkpoints/Word-gen-text-weights-180-0.7608.hdf5\n",
      "4908/4908 [==============================] - 1s - loss: 0.7608     \n",
      "Epoch 182/500\n",
      "4800/4908 [============================>.] - ETA: 0s - loss: 0.7631Epoch 00181: loss did not improve\n",
      "4908/4908 [==============================] - 1s - loss: 0.7644     \n",
      "Epoch 183/500\n",
      "4800/4908 [============================>.] - ETA: 0s - loss: 0.7470Epoch 00182: loss improved from 0.76079 to 0.75015, saving model to checkpoints/Word-gen-text-weights-182-0.7502.hdf5\n",
      "4908/4908 [==============================] - 1s - loss: 0.7502     \n",
      "Epoch 184/500\n",
      "4736/4908 [===========================>..] - ETA: 0s - loss: 0.7500Epoch 00183: loss did not improve\n",
      "4908/4908 [==============================] - 1s - loss: 0.7535     \n",
      "Epoch 185/500\n",
      "4864/4908 [============================>.] - ETA: 0s - loss: 0.7830Epoch 00184: loss did not improve\n",
      "4908/4908 [==============================] - 1s - loss: 0.7822     \n",
      "Epoch 186/500\n",
      "4864/4908 [============================>.] - ETA: 0s - loss: 0.7449Epoch 00185: loss improved from 0.75015 to 0.74492, saving model to checkpoints/Word-gen-text-weights-185-0.7449.hdf5\n",
      "4908/4908 [==============================] - 1s - loss: 0.7449     \n",
      "Epoch 187/500\n",
      "4864/4908 [============================>.] - ETA: 0s - loss: 0.7489Epoch 00186: loss did not improve\n",
      "4908/4908 [==============================] - 1s - loss: 0.7491     \n",
      "Epoch 188/500\n",
      "4736/4908 [===========================>..] - ETA: 0s - loss: 0.7469Epoch 00187: loss did not improve\n",
      "4908/4908 [==============================] - 1s - loss: 0.7476     \n",
      "Epoch 189/500\n",
      "4864/4908 [============================>.] - ETA: 0s - loss: 0.7636Epoch 00188: loss did not improve\n",
      "4908/4908 [==============================] - 1s - loss: 0.7635     \n",
      "Epoch 190/500\n",
      "4864/4908 [============================>.] - ETA: 0s - loss: 0.7548Epoch 00189: loss did not improve\n",
      "4908/4908 [==============================] - 1s - loss: 0.7549     \n",
      "Epoch 191/500\n",
      "4800/4908 [============================>.] - ETA: 0s - loss: 0.7250Epoch 00190: loss improved from 0.74492 to 0.72335, saving model to checkpoints/Word-gen-text-weights-190-0.7234.hdf5\n",
      "4908/4908 [==============================] - 1s - loss: 0.7234     \n",
      "Epoch 192/500\n",
      "4736/4908 [===========================>..] - ETA: 0s - loss: 0.7131Epoch 00191: loss improved from 0.72335 to 0.71725, saving model to checkpoints/Word-gen-text-weights-191-0.7172.hdf5\n",
      "4908/4908 [==============================] - 1s - loss: 0.7172     \n",
      "Epoch 193/500\n",
      "4864/4908 [============================>.] - ETA: 0s - loss: 0.7177Epoch 00192: loss improved from 0.71725 to 0.71622, saving model to checkpoints/Word-gen-text-weights-192-0.7162.hdf5\n",
      "4908/4908 [==============================] - 1s - loss: 0.7162     \n",
      "Epoch 194/500\n",
      "4800/4908 [============================>.] - ETA: 0s - loss: 0.7162Epoch 00193: loss did not improve\n",
      "4908/4908 [==============================] - 1s - loss: 0.7172     \n",
      "Epoch 195/500\n",
      "4800/4908 [============================>.] - ETA: 0s - loss: 0.6878Epoch 00194: loss improved from 0.71622 to 0.69031, saving model to checkpoints/Word-gen-text-weights-194-0.6903.hdf5\n",
      "4908/4908 [==============================] - 1s - loss: 0.6903     \n",
      "Epoch 196/500\n",
      "4736/4908 [===========================>..] - ETA: 0s - loss: 0.7139Epoch 00195: loss did not improve\n",
      "4908/4908 [==============================] - 1s - loss: 0.7186     \n",
      "Epoch 197/500\n",
      "4864/4908 [============================>.] - ETA: 0s - loss: 0.7135Epoch 00196: loss did not improve\n",
      "4908/4908 [==============================] - 1s - loss: 0.7142     \n",
      "Epoch 198/500\n",
      "4864/4908 [============================>.] - ETA: 0s - loss: 0.7257Epoch 00197: loss did not improve\n",
      "4908/4908 [==============================] - 1s - loss: 0.7237     \n",
      "Epoch 199/500\n",
      "4864/4908 [============================>.] - ETA: 0s - loss: 0.6985Epoch 00198: loss did not improve\n",
      "4908/4908 [==============================] - 1s - loss: 0.6983     \n",
      "Epoch 200/500\n",
      "4736/4908 [===========================>..] - ETA: 0s - loss: 0.6990Epoch 00199: loss did not improve\n",
      "4908/4908 [==============================] - 1s - loss: 0.6977     \n",
      "Epoch 201/500\n",
      "4800/4908 [============================>.] - ETA: 0s - loss: 0.6761Epoch 00200: loss improved from 0.69031 to 0.67654, saving model to checkpoints/Word-gen-text-weights-200-0.6765.hdf5\n",
      "4908/4908 [==============================] - 1s - loss: 0.6765     \n",
      "Epoch 202/500\n",
      "4864/4908 [============================>.] - ETA: 0s - loss: 0.6897Epoch 00201: loss did not improve\n",
      "4908/4908 [==============================] - 1s - loss: 0.6880     \n",
      "Epoch 203/500\n",
      "4736/4908 [===========================>..] - ETA: 0s - loss: 0.6778Epoch 00202: loss did not improve\n",
      "4908/4908 [==============================] - 1s - loss: 0.6849     \n",
      "Epoch 204/500\n",
      "4736/4908 [===========================>..] - ETA: 0s - loss: 0.6758Epoch 00203: loss improved from 0.67654 to 0.67618, saving model to checkpoints/Word-gen-text-weights-203-0.6762.hdf5\n",
      "4908/4908 [==============================] - 1s - loss: 0.6762     \n",
      "Epoch 205/500\n",
      "4864/4908 [============================>.] - ETA: 0s - loss: 0.6824Epoch 00204: loss did not improve\n",
      "4908/4908 [==============================] - 1s - loss: 0.6810     \n",
      "Epoch 206/500\n",
      "4800/4908 [============================>.] - ETA: 0s - loss: 0.6618Epoch 00205: loss improved from 0.67618 to 0.66406, saving model to checkpoints/Word-gen-text-weights-205-0.6641.hdf5\n",
      "4908/4908 [==============================] - 1s - loss: 0.6641     \n",
      "Epoch 207/500\n",
      "4736/4908 [===========================>..] - ETA: 0s - loss: 0.6905Epoch 00206: loss did not improve\n",
      "4908/4908 [==============================] - 1s - loss: 0.6906     \n",
      "Epoch 208/500\n",
      "4864/4908 [============================>.] - ETA: 0s - loss: 0.6734Epoch 00207: loss did not improve\n",
      "4908/4908 [==============================] - 1s - loss: 0.6737     \n",
      "Epoch 209/500\n",
      "4800/4908 [============================>.] - ETA: 0s - loss: 0.6632Epoch 00208: loss improved from 0.66406 to 0.66006, saving model to checkpoints/Word-gen-text-weights-208-0.6601.hdf5\n",
      "4908/4908 [==============================] - 1s - loss: 0.6601     \n",
      "Epoch 210/500\n",
      "4736/4908 [===========================>..] - ETA: 0s - loss: 0.6595Epoch 00209: loss improved from 0.66006 to 0.65986, saving model to checkpoints/Word-gen-text-weights-209-0.6599.hdf5\n",
      "4908/4908 [==============================] - 1s - loss: 0.6599     \n",
      "Epoch 211/500\n",
      "4800/4908 [============================>.] - ETA: 0s - loss: 0.6683Epoch 00210: loss did not improve\n",
      "4908/4908 [==============================] - 1s - loss: 0.6630     \n",
      "Epoch 212/500\n",
      "4736/4908 [===========================>..] - ETA: 0s - loss: 0.6589Epoch 00211: loss did not improve\n",
      "4908/4908 [==============================] - 1s - loss: 0.6603     \n",
      "Epoch 213/500\n",
      "4800/4908 [============================>.] - ETA: 0s - loss: 0.6606Epoch 00212: loss improved from 0.65986 to 0.65951, saving model to checkpoints/Word-gen-text-weights-212-0.6595.hdf5\n",
      "4908/4908 [==============================] - 1s - loss: 0.6595     \n",
      "Epoch 214/500\n",
      "4736/4908 [===========================>..] - ETA: 0s - loss: 0.6508Epoch 00213: loss improved from 0.65951 to 0.64693, saving model to checkpoints/Word-gen-text-weights-213-0.6469.hdf5\n",
      "4908/4908 [==============================] - 1s - loss: 0.6469     \n",
      "Epoch 215/500\n",
      "4864/4908 [============================>.] - ETA: 0s - loss: 0.6497Epoch 00214: loss did not improve\n",
      "4908/4908 [==============================] - 1s - loss: 0.6522     \n",
      "Epoch 216/500\n",
      "4800/4908 [============================>.] - ETA: 0s - loss: 0.6539Epoch 00215: loss did not improve\n",
      "4908/4908 [==============================] - 1s - loss: 0.6532     \n",
      "Epoch 217/500\n",
      "4736/4908 [===========================>..] - ETA: 0s - loss: 0.6413Epoch 00216: loss improved from 0.64693 to 0.64320, saving model to checkpoints/Word-gen-text-weights-216-0.6432.hdf5\n",
      "4908/4908 [==============================] - 1s - loss: 0.6432     \n",
      "Epoch 218/500\n",
      "4800/4908 [============================>.] - ETA: 0s - loss: 0.6305Epoch 00217: loss improved from 0.64320 to 0.62979, saving model to checkpoints/Word-gen-text-weights-217-0.6298.hdf5\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4908/4908 [==============================] - 1s - loss: 0.6298     \n",
      "Epoch 219/500\n",
      "4736/4908 [===========================>..] - ETA: 0s - loss: 0.6414Epoch 00218: loss did not improve\n",
      "4908/4908 [==============================] - 1s - loss: 0.6391     \n",
      "Epoch 220/500\n",
      "4864/4908 [============================>.] - ETA: 0s - loss: 0.6355Epoch 00219: loss did not improve\n",
      "4908/4908 [==============================] - 1s - loss: 0.6367     \n",
      "Epoch 221/500\n",
      "4864/4908 [============================>.] - ETA: 0s - loss: 0.6596Epoch 00220: loss did not improve\n",
      "4908/4908 [==============================] - 1s - loss: 0.6594     \n",
      "Epoch 222/500\n",
      "4864/4908 [============================>.] - ETA: 0s - loss: 0.6427- ETA: 0s - lEpoch 00221: loss did not improve\n",
      "4908/4908 [==============================] - 1s - loss: 0.6423     \n",
      "Epoch 223/500\n",
      "4864/4908 [============================>.] - ETA: 0s - loss: 0.6443Epoch 00222: loss did not improve\n",
      "4908/4908 [==============================] - 1s - loss: 0.6441     \n",
      "Epoch 224/500\n",
      "4864/4908 [============================>.] - ETA: 0s - loss: 0.6200Epoch 00223: loss improved from 0.62979 to 0.62134, saving model to checkpoints/Word-gen-text-weights-223-0.6213.hdf5\n",
      "4908/4908 [==============================] - 1s - loss: 0.6213     \n",
      "Epoch 225/500\n",
      "4736/4908 [===========================>..] - ETA: 0s - loss: 0.6298Epoch 00224: loss did not improve\n",
      "4908/4908 [==============================] - 1s - loss: 0.6255     \n",
      "Epoch 226/500\n",
      "4800/4908 [============================>.] - ETA: 0s - loss: 0.6337Epoch 00225: loss did not improve\n",
      "4908/4908 [==============================] - 1s - loss: 0.6339     \n",
      "Epoch 227/500\n",
      "4864/4908 [============================>.] - ETA: 0s - loss: 0.6223Epoch 00226: loss improved from 0.62134 to 0.62109, saving model to checkpoints/Word-gen-text-weights-226-0.6211.hdf5\n",
      "4908/4908 [==============================] - 1s - loss: 0.6211     \n",
      "Epoch 228/500\n",
      "4800/4908 [============================>.] - ETA: 0s - loss: 0.6124Epoch 00227: loss improved from 0.62109 to 0.61347, saving model to checkpoints/Word-gen-text-weights-227-0.6135.hdf5\n",
      "4908/4908 [==============================] - 1s - loss: 0.6135     \n",
      "Epoch 229/500\n",
      "4800/4908 [============================>.] - ETA: 0s - loss: 0.6322Epoch 00228: loss did not improve\n",
      "4908/4908 [==============================] - 1s - loss: 0.6315     \n",
      "Epoch 230/500\n",
      "4800/4908 [============================>.] - ETA: 0s - loss: 0.6280Epoch 00229: loss did not improve\n",
      "4908/4908 [==============================] - 1s - loss: 0.6255     \n",
      "Epoch 231/500\n",
      "4800/4908 [============================>.] - ETA: 0s - loss: 0.6022Epoch 00230: loss improved from 0.61347 to 0.60059, saving model to checkpoints/Word-gen-text-weights-230-0.6006.hdf5\n",
      "4908/4908 [==============================] - 1s - loss: 0.6006     \n",
      "Epoch 232/500\n",
      "4864/4908 [============================>.] - ETA: 0s - loss: 0.6063Epoch 00231: loss did not improve\n",
      "4908/4908 [==============================] - 1s - loss: 0.6075     \n",
      "Epoch 233/500\n",
      "4800/4908 [============================>.] - ETA: 0s - loss: 0.6118Epoch 00232: loss did not improve\n",
      "4908/4908 [==============================] - 1s - loss: 0.6110     \n",
      "Epoch 234/500\n",
      "4800/4908 [============================>.] - ETA: 0s - loss: 0.5776Epoch 00233: loss improved from 0.60059 to 0.57733, saving model to checkpoints/Word-gen-text-weights-233-0.5773.hdf5\n",
      "4908/4908 [==============================] - 1s - loss: 0.5773     \n",
      "Epoch 235/500\n",
      "4800/4908 [============================>.] - ETA: 0s - loss: 0.6016Epoch 00234: loss did not improve\n",
      "4908/4908 [==============================] - 1s - loss: 0.5979     \n",
      "Epoch 236/500\n",
      "4864/4908 [============================>.] - ETA: 0s - loss: 0.6187Epoch 00235: loss did not improve\n",
      "4908/4908 [==============================] - 1s - loss: 0.6172     \n",
      "Epoch 237/500\n",
      "4864/4908 [============================>.] - ETA: 0s - loss: 0.6170Epoch 00236: loss did not improve\n",
      "4908/4908 [==============================] - 1s - loss: 0.6175     \n",
      "Epoch 238/500\n",
      "4864/4908 [============================>.] - ETA: 0s - loss: 0.6232Epoch 00237: loss did not improve\n",
      "4908/4908 [==============================] - 1s - loss: 0.6250     \n",
      "Epoch 239/500\n",
      "4864/4908 [============================>.] - ETA: 0s - loss: 0.6280Epoch 00238: loss did not improve\n",
      "4908/4908 [==============================] - 1s - loss: 0.6298     \n",
      "Epoch 240/500\n",
      "4864/4908 [============================>.] - ETA: 0s - loss: 0.5972Epoch 00239: loss did not improve\n",
      "4908/4908 [==============================] - 1s - loss: 0.5963     \n",
      "Epoch 241/500\n",
      "4864/4908 [============================>.] - ETA: 0s - loss: 0.5816Epoch 00240: loss did not improve\n",
      "4908/4908 [==============================] - 1s - loss: 0.5811     \n",
      "Epoch 242/500\n",
      "4864/4908 [============================>.] - ETA: 0s - loss: 0.5517Epoch 00241: loss improved from 0.57733 to 0.55236, saving model to checkpoints/Word-gen-text-weights-241-0.5524.hdf5\n",
      "4908/4908 [==============================] - 1s - loss: 0.5524     \n",
      "Epoch 243/500\n",
      "4736/4908 [===========================>..] - ETA: 0s - loss: 0.5875Epoch 00242: loss did not improve\n",
      "4908/4908 [==============================] - 1s - loss: 0.5864     \n",
      "Epoch 244/500\n",
      "4800/4908 [============================>.] - ETA: 0s - loss: 0.5825Epoch 00243: loss did not improve\n",
      "4908/4908 [==============================] - 1s - loss: 0.5839     \n",
      "Epoch 245/500\n",
      "4864/4908 [============================>.] - ETA: 0s - loss: 0.5862Epoch 00244: loss did not improve\n",
      "4908/4908 [==============================] - 1s - loss: 0.5872     \n",
      "Epoch 246/500\n",
      "4800/4908 [============================>.] - ETA: 0s - loss: 0.5672Epoch 00245: loss did not improve\n",
      "4908/4908 [==============================] - 1s - loss: 0.5693     \n",
      "Epoch 247/500\n",
      "4736/4908 [===========================>..] - ETA: 0s - loss: 0.5574Epoch 00246: loss did not improve\n",
      "4908/4908 [==============================] - 1s - loss: 0.5566     \n",
      "Epoch 248/500\n",
      "4864/4908 [============================>.] - ETA: 0s - loss: 0.5719Epoch 00247: loss did not improve\n",
      "4908/4908 [==============================] - 1s - loss: 0.5729     \n",
      "Epoch 249/500\n",
      "4864/4908 [============================>.] - ETA: 0s - loss: 0.5434Epoch 00248: loss improved from 0.55236 to 0.54163, saving model to checkpoints/Word-gen-text-weights-248-0.5416.hdf5\n",
      "4908/4908 [==============================] - 1s - loss: 0.5416     \n",
      "Epoch 250/500\n",
      "4800/4908 [============================>.] - ETA: 0s - loss: 0.5645Epoch 00249: loss did not improve\n",
      "4908/4908 [==============================] - 1s - loss: 0.5633     \n",
      "Epoch 251/500\n",
      "4736/4908 [===========================>..] - ETA: 0s - loss: 0.5667Epoch 00250: loss did not improve\n",
      "4908/4908 [==============================] - 1s - loss: 0.5642     \n",
      "Epoch 252/500\n",
      "4736/4908 [===========================>..] - ETA: 0s - loss: 0.5752Epoch 00251: loss did not improve\n",
      "4908/4908 [==============================] - 1s - loss: 0.5770     \n",
      "Epoch 253/500\n",
      "4864/4908 [============================>.] - ETA: 0s - loss: 0.5557Epoch 00252: loss did not improve\n",
      "4908/4908 [==============================] - 1s - loss: 0.5568     \n",
      "Epoch 254/500\n",
      "4800/4908 [============================>.] - ETA: 0s - loss: 0.5397Epoch 00253: loss improved from 0.54163 to 0.54070, saving model to checkpoints/Word-gen-text-weights-253-0.5407.hdf5\n",
      "4908/4908 [==============================] - 1s - loss: 0.5407     \n",
      "Epoch 255/500\n",
      "4800/4908 [============================>.] - ETA: 0s - loss: 0.5464Epoch 00254: loss did not improve\n",
      "4908/4908 [==============================] - 1s - loss: 0.5446     \n",
      "Epoch 256/500\n",
      "4736/4908 [===========================>..] - ETA: 0s - loss: 0.5384Epoch 00255: loss improved from 0.54070 to 0.53897, saving model to checkpoints/Word-gen-text-weights-255-0.5390.hdf5\n",
      "4908/4908 [==============================] - 1s - loss: 0.5390     \n",
      "Epoch 257/500\n",
      "4800/4908 [============================>.] - ETA: 0s - loss: 0.5504Epoch 00256: loss did not improve\n",
      "4908/4908 [==============================] - 1s - loss: 0.5491     \n",
      "Epoch 258/500\n",
      "4800/4908 [============================>.] - ETA: 0s - loss: 0.5552Epoch 00257: loss did not improve\n",
      "4908/4908 [==============================] - 1s - loss: 0.5541     \n",
      "Epoch 259/500\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4736/4908 [===========================>..] - ETA: 0s - loss: 0.5476Epoch 00258: loss did not improve\n",
      "4908/4908 [==============================] - 1s - loss: 0.5468     \n",
      "Epoch 260/500\n",
      "4736/4908 [===========================>..] - ETA: 0s - loss: 0.5388Epoch 00259: loss did not improve\n",
      "4908/4908 [==============================] - 1s - loss: 0.5396     \n",
      "Epoch 261/500\n",
      "4800/4908 [============================>.] - ETA: 0s - loss: 0.5266Epoch 00260: loss improved from 0.53897 to 0.52449, saving model to checkpoints/Word-gen-text-weights-260-0.5245.hdf5\n",
      "4908/4908 [==============================] - 1s - loss: 0.5245     \n",
      "Epoch 262/500\n",
      "4864/4908 [============================>.] - ETA: 0s - loss: 0.5255Epoch 00261: loss did not improve\n",
      "4908/4908 [==============================] - 1s - loss: 0.5266     \n",
      "Epoch 263/500\n",
      "4800/4908 [============================>.] - ETA: 0s - loss: 0.5211Epoch 00262: loss improved from 0.52449 to 0.52425, saving model to checkpoints/Word-gen-text-weights-262-0.5242.hdf5\n",
      "4908/4908 [==============================] - 1s - loss: 0.5242     \n",
      "Epoch 264/500\n",
      "4864/4908 [============================>.] - ETA: 0s - loss: 0.5317Epoch 00263: loss did not improve\n",
      "4908/4908 [==============================] - 1s - loss: 0.5321     \n",
      "Epoch 265/500\n",
      "4864/4908 [============================>.] - ETA: 0s - loss: 0.5167Epoch 00264: loss improved from 0.52425 to 0.51733, saving model to checkpoints/Word-gen-text-weights-264-0.5173.hdf5\n",
      "4908/4908 [==============================] - 1s - loss: 0.5173     \n",
      "Epoch 266/500\n",
      "4800/4908 [============================>.] - ETA: 0s - loss: 0.5154Epoch 00265: loss improved from 0.51733 to 0.51494, saving model to checkpoints/Word-gen-text-weights-265-0.5149.hdf5\n",
      "4908/4908 [==============================] - 1s - loss: 0.5149     \n",
      "Epoch 267/500\n",
      "4864/4908 [============================>.] - ETA: 0s - loss: 0.5170Epoch 00266: loss did not improve\n",
      "4908/4908 [==============================] - 1s - loss: 0.5175     \n",
      "Epoch 268/500\n",
      "4800/4908 [============================>.] - ETA: 0s - loss: 0.5334Epoch 00267: loss did not improve\n",
      "4908/4908 [==============================] - 1s - loss: 0.5355     \n",
      "Epoch 269/500\n",
      "4864/4908 [============================>.] - ETA: 0s - loss: 0.5436Epoch 00268: loss did not improve\n",
      "4908/4908 [==============================] - 1s - loss: 0.5437     \n",
      "Epoch 270/500\n",
      "4800/4908 [============================>.] - ETA: 0s - loss: 0.5176Epoch 00269: loss did not improve\n",
      "4908/4908 [==============================] - 1s - loss: 0.5191     \n",
      "Epoch 271/500\n",
      "4864/4908 [============================>.] - ETA: 0s - loss: 0.5278Epoch 00270: loss did not improve\n",
      "4908/4908 [==============================] - 1s - loss: 0.5274     \n",
      "Epoch 272/500\n",
      "4864/4908 [============================>.] - ETA: 0s - loss: 0.5065Epoch 00271: loss improved from 0.51494 to 0.50759, saving model to checkpoints/Word-gen-text-weights-271-0.5076.hdf5\n",
      "4908/4908 [==============================] - 1s - loss: 0.5076     \n",
      "Epoch 273/500\n",
      "4864/4908 [============================>.] - ETA: 0s - loss: 0.5124Epoch 00272: loss did not improve\n",
      "4908/4908 [==============================] - 1s - loss: 0.5115     \n",
      "Epoch 274/500\n",
      "4864/4908 [============================>.] - ETA: 0s - loss: 0.5088Epoch 00273: loss did not improve\n",
      "4908/4908 [==============================] - 1s - loss: 0.5087     \n",
      "Epoch 275/500\n",
      "4800/4908 [============================>.] - ETA: 0s - loss: 0.5197Epoch 00274: loss did not improve\n",
      "4908/4908 [==============================] - 1s - loss: 0.5159     \n",
      "Epoch 276/500\n",
      "4864/4908 [============================>.] - ETA: 0s - loss: 0.5008Epoch 00275: loss improved from 0.50759 to 0.50073, saving model to checkpoints/Word-gen-text-weights-275-0.5007.hdf5\n",
      "4908/4908 [==============================] - 1s - loss: 0.5007     \n",
      "Epoch 277/500\n",
      "4864/4908 [============================>.] - ETA: 0s - loss: 0.5051Epoch 00276: loss did not improve\n",
      "4908/4908 [==============================] - 1s - loss: 0.5055     \n",
      "Epoch 278/500\n",
      "4864/4908 [============================>.] - ETA: 0s - loss: 0.5050Epoch 00277: loss did not improve\n",
      "4908/4908 [==============================] - 1s - loss: 0.5048     \n",
      "Epoch 279/500\n",
      "4864/4908 [============================>.] - ETA: 0s - loss: 0.4977Epoch 00278: loss improved from 0.50073 to 0.49672, saving model to checkpoints/Word-gen-text-weights-278-0.4967.hdf5\n",
      "4908/4908 [==============================] - 1s - loss: 0.4967     \n",
      "Epoch 280/500\n",
      "4800/4908 [============================>.] - ETA: 0s - loss: 0.4974Epoch 00279: loss did not improve\n",
      "4908/4908 [==============================] - 1s - loss: 0.4980     \n",
      "Epoch 281/500\n",
      "4800/4908 [============================>.] - ETA: 0s - loss: 0.5220Epoch 00280: loss did not improve\n",
      "4908/4908 [==============================] - 1s - loss: 0.5192     \n",
      "Epoch 282/500\n",
      "4800/4908 [============================>.] - ETA: 0s - loss: 0.5026Epoch 00281: loss did not improve\n",
      "4908/4908 [==============================] - 1s - loss: 0.5033     \n",
      "Epoch 283/500\n",
      "4800/4908 [============================>.] - ETA: 0s - loss: 0.4940Epoch 00282: loss improved from 0.49672 to 0.49304, saving model to checkpoints/Word-gen-text-weights-282-0.4930.hdf5\n",
      "4908/4908 [==============================] - 1s - loss: 0.4930     \n",
      "Epoch 284/500\n",
      "4864/4908 [============================>.] - ETA: 0s - loss: 0.4992Epoch 00283: loss did not improve\n",
      "4908/4908 [==============================] - 1s - loss: 0.4981     \n",
      "Epoch 285/500\n",
      "4800/4908 [============================>.] - ETA: 0s - loss: 0.4978Epoch 00284: loss did not improve\n",
      "4908/4908 [==============================] - 1s - loss: 0.4949     \n",
      "Epoch 286/500\n",
      "4800/4908 [============================>.] - ETA: 0s - loss: 0.5097Epoch 00285: loss did not improve\n",
      "4908/4908 [==============================] - 1s - loss: 0.5098     \n",
      "Epoch 287/500\n",
      "4736/4908 [===========================>..] - ETA: 0s - loss: 0.5070Epoch 00286: loss did not improve\n",
      "4908/4908 [==============================] - 1s - loss: 0.5071     \n",
      "Epoch 288/500\n",
      "4800/4908 [============================>.] - ETA: 0s - loss: 0.5109Epoch 00287: loss did not improve\n",
      "4908/4908 [==============================] - 1s - loss: 0.5123     \n",
      "Epoch 289/500\n",
      "4736/4908 [===========================>..] - ETA: 0s - loss: 0.4857Epoch 00288: loss improved from 0.49304 to 0.48929, saving model to checkpoints/Word-gen-text-weights-288-0.4893.hdf5\n",
      "4908/4908 [==============================] - 1s - loss: 0.4893     \n",
      "Epoch 290/500\n",
      "4864/4908 [============================>.] - ETA: 0s - loss: 0.4944Epoch 00289: loss did not improve\n",
      "4908/4908 [==============================] - 1s - loss: 0.4926     \n",
      "Epoch 291/500\n",
      "4800/4908 [============================>.] - ETA: 0s - loss: 0.4979Epoch 00290: loss did not improve\n",
      "4908/4908 [==============================] - 1s - loss: 0.4996     \n",
      "Epoch 292/500\n",
      "4864/4908 [============================>.] - ETA: 0s - loss: 0.4891Epoch 00291: loss improved from 0.48929 to 0.48883, saving model to checkpoints/Word-gen-text-weights-291-0.4888.hdf5\n",
      "4908/4908 [==============================] - 1s - loss: 0.4888     \n",
      "Epoch 293/500\n",
      "4736/4908 [===========================>..] - ETA: 0s - loss: 0.4878Epoch 00292: loss improved from 0.48883 to 0.48636, saving model to checkpoints/Word-gen-text-weights-292-0.4864.hdf5\n",
      "4908/4908 [==============================] - 1s - loss: 0.4864     \n",
      "Epoch 294/500\n",
      "4736/4908 [===========================>..] - ETA: 0s - loss: 0.5009Epoch 00293: loss did not improve\n",
      "4908/4908 [==============================] - 1s - loss: 0.4999     \n",
      "Epoch 295/500\n",
      "4800/4908 [============================>.] - ETA: 0s - loss: 0.4930Epoch 00294: loss did not improve\n",
      "4908/4908 [==============================] - 1s - loss: 0.4969     \n",
      "Epoch 296/500\n",
      "4864/4908 [============================>.] - ETA: 0s - loss: 0.4940Epoch 00295: loss did not improve\n",
      "4908/4908 [==============================] - 1s - loss: 0.4950     \n",
      "Epoch 297/500\n",
      "4800/4908 [============================>.] - ETA: 0s - loss: 0.4852Epoch 00296: loss improved from 0.48636 to 0.48544, saving model to checkpoints/Word-gen-text-weights-296-0.4854.hdf5\n",
      "4908/4908 [==============================] - 1s - loss: 0.4854     \n",
      "Epoch 298/500\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4800/4908 [============================>.] - ETA: 0s - loss: 0.4749Epoch 00297: loss improved from 0.48544 to 0.47588, saving model to checkpoints/Word-gen-text-weights-297-0.4759.hdf5\n",
      "4908/4908 [==============================] - 1s - loss: 0.4759     \n",
      "Epoch 299/500\n",
      "4800/4908 [============================>.] - ETA: 0s - loss: 0.4831Epoch 00298: loss did not improve\n",
      "4908/4908 [==============================] - 1s - loss: 0.4818     \n",
      "Epoch 300/500\n",
      "4800/4908 [============================>.] - ETA: 0s - loss: 0.4807Epoch 00299: loss did not improve\n",
      "4908/4908 [==============================] - 1s - loss: 0.4804     \n",
      "Epoch 301/500\n",
      "4736/4908 [===========================>..] - ETA: 0s - loss: 0.4860Epoch 00300: loss did not improve\n",
      "4908/4908 [==============================] - 1s - loss: 0.4879     \n",
      "Epoch 302/500\n",
      "4864/4908 [============================>.] - ETA: 0s - loss: 0.4886Epoch 00301: loss did not improve\n",
      "4908/4908 [==============================] - 1s - loss: 0.4891     \n",
      "Epoch 303/500\n",
      "4800/4908 [============================>.] - ETA: 0s - loss: 0.4823Epoch 00302: loss did not improve\n",
      "4908/4908 [==============================] - 1s - loss: 0.4801     \n",
      "Epoch 304/500\n",
      "4864/4908 [============================>.] - ETA: 0s - loss: 0.4703Epoch 00303: loss improved from 0.47588 to 0.46947, saving model to checkpoints/Word-gen-text-weights-303-0.4695.hdf5\n",
      "4908/4908 [==============================] - 1s - loss: 0.4695     \n",
      "Epoch 305/500\n",
      "4864/4908 [============================>.] - ETA: 0s - loss: 0.4816Epoch 00304: loss did not improve\n",
      "4908/4908 [==============================] - 1s - loss: 0.4830     \n",
      "Epoch 306/500\n",
      "4864/4908 [============================>.] - ETA: 0s - loss: 0.4685Epoch 00305: loss did not improve\n",
      "4908/4908 [==============================] - 1s - loss: 0.4702     \n",
      "Epoch 307/500\n",
      "4736/4908 [===========================>..] - ETA: 0s - loss: 0.4794Epoch 00306: loss did not improve\n",
      "4908/4908 [==============================] - 1s - loss: 0.4804     \n",
      "Epoch 308/500\n",
      "4864/4908 [============================>.] - ETA: 0s - loss: 0.4701Epoch 00307: loss did not improve\n",
      "4908/4908 [==============================] - 1s - loss: 0.4714     \n",
      "Epoch 309/500\n",
      "4864/4908 [============================>.] - ETA: 0s - loss: 0.4718Epoch 00308: loss did not improve\n",
      "4908/4908 [==============================] - 1s - loss: 0.4711     \n",
      "Epoch 310/500\n",
      "4864/4908 [============================>.] - ETA: 0s - loss: 0.4656Epoch 00309: loss improved from 0.46947 to 0.46636, saving model to checkpoints/Word-gen-text-weights-309-0.4664.hdf5\n",
      "4908/4908 [==============================] - 1s - loss: 0.4664     \n",
      "Epoch 311/500\n",
      "4800/4908 [============================>.] - ETA: 0s - loss: 0.4749Epoch 00310: loss did not improve\n",
      "4908/4908 [==============================] - 1s - loss: 0.4759     \n",
      "Epoch 312/500\n",
      "4736/4908 [===========================>..] - ETA: 0s - loss: 0.4631Epoch 00311: loss improved from 0.46636 to 0.46078, saving model to checkpoints/Word-gen-text-weights-311-0.4608.hdf5\n",
      "4908/4908 [==============================] - 1s - loss: 0.4608     \n",
      "Epoch 313/500\n",
      "4736/4908 [===========================>..] - ETA: 0s - loss: 0.4802Epoch 00312: loss did not improve\n",
      "4908/4908 [==============================] - 1s - loss: 0.4800     \n",
      "Epoch 314/500\n",
      "4736/4908 [===========================>..] - ETA: 0s - loss: 0.4616- ETAEpoch 00313: loss did not improve\n",
      "4908/4908 [==============================] - 1s - loss: 0.4625     \n",
      "Epoch 315/500\n",
      "4864/4908 [============================>.] - ETA: 0s - loss: 0.4492Epoch 00314: loss improved from 0.46078 to 0.45007, saving model to checkpoints/Word-gen-text-weights-314-0.4501.hdf5\n",
      "4908/4908 [==============================] - 1s - loss: 0.4501     \n",
      "Epoch 316/500\n",
      "4800/4908 [============================>.] - ETA: 0s - loss: 0.4598Epoch 00315: loss did not improve\n",
      "4908/4908 [==============================] - 1s - loss: 0.4628     \n",
      "Epoch 317/500\n",
      "4864/4908 [============================>.] - ETA: 0s - loss: 0.4646Epoch 00316: loss did not improve\n",
      "4908/4908 [==============================] - 1s - loss: 0.4650     \n",
      "Epoch 318/500\n",
      "4864/4908 [============================>.] - ETA: 0s - loss: 0.4542Epoch 00317: loss did not improve\n",
      "4908/4908 [==============================] - 1s - loss: 0.4549     \n",
      "Epoch 319/500\n",
      "4800/4908 [============================>.] - ETA: 0s - loss: 0.4620- ETA: 0s - lossEpoch 00318: loss did not improve\n",
      "4908/4908 [==============================] - 1s - loss: 0.4632     \n",
      "Epoch 320/500\n",
      "4864/4908 [============================>.] - ETA: 0s - loss: 0.4594Epoch 00319: loss did not improve\n",
      "4908/4908 [==============================] - 1s - loss: 0.4602     \n",
      "Epoch 321/500\n",
      "4864/4908 [============================>.] - ETA: 0s - loss: 0.4532Epoch 00320: loss did not improve\n",
      "4908/4908 [==============================] - 1s - loss: 0.4521     \n",
      "Epoch 322/500\n",
      "4864/4908 [============================>.] - ETA: 0s - loss: 0.4452Epoch 00321: loss improved from 0.45007 to 0.44676, saving model to checkpoints/Word-gen-text-weights-321-0.4468.hdf5\n",
      "4908/4908 [==============================] - 1s - loss: 0.4468     \n",
      "Epoch 323/500\n",
      "4800/4908 [============================>.] - ETA: 0s - loss: 0.4687Epoch 00322: loss did not improve\n",
      "4908/4908 [==============================] - 1s - loss: 0.4690     \n",
      "Epoch 324/500\n",
      "4800/4908 [============================>.] - ETA: 0s - loss: 0.4419Epoch 00323: loss improved from 0.44676 to 0.44454, saving model to checkpoints/Word-gen-text-weights-323-0.4445.hdf5\n",
      "4908/4908 [==============================] - 1s - loss: 0.4445     \n",
      "Epoch 325/500\n",
      "4864/4908 [============================>.] - ETA: 0s - loss: 0.4419Epoch 00324: loss improved from 0.44454 to 0.44154, saving model to checkpoints/Word-gen-text-weights-324-0.4415.hdf5\n",
      "4908/4908 [==============================] - 1s - loss: 0.4415     \n",
      "Epoch 326/500\n",
      "4800/4908 [============================>.] - ETA: 0s - loss: 0.4469Epoch 00325: loss did not improve\n",
      "4908/4908 [==============================] - 1s - loss: 0.4495     \n",
      "Epoch 327/500\n",
      "4736/4908 [===========================>..] - ETA: 0s - loss: 0.4497Epoch 00326: loss did not improve\n",
      "4908/4908 [==============================] - 1s - loss: 0.4519     \n",
      "Epoch 328/500\n",
      "4736/4908 [===========================>..] - ETA: 0s - loss: 0.4489Epoch 00327: loss did not improve\n",
      "4908/4908 [==============================] - 1s - loss: 0.4491     \n",
      "Epoch 329/500\n",
      "4800/4908 [============================>.] - ETA: 0s - loss: 0.4488Epoch 00328: loss did not improve\n",
      "4908/4908 [==============================] - 1s - loss: 0.4487     \n",
      "Epoch 330/500\n",
      "4736/4908 [===========================>..] - ETA: 0s - loss: 0.4441Epoch 00329: loss did not improve\n",
      "4908/4908 [==============================] - 1s - loss: 0.4461     \n",
      "Epoch 331/500\n",
      "4864/4908 [============================>.] - ETA: 0s - loss: 0.4276Epoch 00330: loss improved from 0.44154 to 0.42774, saving model to checkpoints/Word-gen-text-weights-330-0.4277.hdf5\n",
      "4908/4908 [==============================] - 1s - loss: 0.4277     \n",
      "Epoch 332/500\n",
      "4864/4908 [============================>.] - ETA: 0s - loss: 0.4570Epoch 00331: loss did not improve\n",
      "4908/4908 [==============================] - 1s - loss: 0.4568     \n",
      "Epoch 333/500\n",
      "4800/4908 [============================>.] - ETA: 0s - loss: 0.4395Epoch 00332: loss did not improve\n",
      "4908/4908 [==============================] - 1s - loss: 0.4409     \n",
      "Epoch 334/500\n",
      "4864/4908 [============================>.] - ETA: 0s - loss: 0.4531Epoch 00333: loss did not improve\n",
      "4908/4908 [==============================] - 1s - loss: 0.4538     \n",
      "Epoch 335/500\n",
      "4736/4908 [===========================>..] - ETA: 0s - loss: 0.4354Epoch 00334: loss did not improve\n",
      "4908/4908 [==============================] - 1s - loss: 0.4340     \n",
      "Epoch 336/500\n",
      "4800/4908 [============================>.] - ETA: 0s - loss: 0.4467Epoch 00335: loss did not improve\n",
      "4908/4908 [==============================] - 1s - loss: 0.4462     \n",
      "Epoch 337/500\n",
      "4800/4908 [============================>.] - ETA: 0s - loss: 0.4282Epoch 00336: loss did not improve\n",
      "4908/4908 [==============================] - 1s - loss: 0.4292     \n",
      "Epoch 338/500\n",
      "4800/4908 [============================>.] - ETA: 0s - loss: 0.4437Epoch 00337: loss did not improve\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4908/4908 [==============================] - 1s - loss: 0.4416     \n",
      "Epoch 339/500\n",
      "4736/4908 [===========================>..] - ETA: 0s - loss: 0.4394Epoch 00338: loss did not improve\n",
      "4908/4908 [==============================] - 1s - loss: 0.4376     \n",
      "Epoch 340/500\n",
      "4864/4908 [============================>.] - ETA: 0s - loss: 0.4377Epoch 00339: loss did not improve\n",
      "4908/4908 [==============================] - 1s - loss: 0.4373     \n",
      "Epoch 341/500\n",
      "4864/4908 [============================>.] - ETA: 0s - loss: 0.4389Epoch 00340: loss did not improve\n",
      "4908/4908 [==============================] - 1s - loss: 0.4396     \n",
      "Epoch 342/500\n",
      "4864/4908 [============================>.] - ETA: 0s - loss: 0.4229- ETA: 1sEpoch 00341: loss improved from 0.42774 to 0.42316, saving model to checkpoints/Word-gen-text-weights-341-0.4232.hdf5\n",
      "4908/4908 [==============================] - 1s - loss: 0.4232     \n",
      "Epoch 343/500\n",
      "4736/4908 [===========================>..] - ETA: 0s - loss: 0.4299Epoch 00342: loss did not improve\n",
      "4908/4908 [==============================] - 1s - loss: 0.4286     \n",
      "Epoch 344/500\n",
      "4800/4908 [============================>.] - ETA: 0s - loss: 0.4413Epoch 00343: loss did not improve\n",
      "4908/4908 [==============================] - 1s - loss: 0.4414     \n",
      "Epoch 345/500\n",
      "4864/4908 [============================>.] - ETA: 0s - loss: 0.4326Epoch 00344: loss did not improve\n",
      "4908/4908 [==============================] - 1s - loss: 0.4325     \n",
      "Epoch 346/500\n",
      "4864/4908 [============================>.] - ETA: 0s - loss: 0.4379- ETA:Epoch 00345: loss did not improve\n",
      "4908/4908 [==============================] - 1s - loss: 0.4363     \n",
      "Epoch 347/500\n",
      "4864/4908 [============================>.] - ETA: 0s - loss: 0.4248Epoch 00346: loss did not improve\n",
      "4908/4908 [==============================] - 1s - loss: 0.4258     \n",
      "Epoch 348/500\n",
      "4864/4908 [============================>.] - ETA: 0s - loss: 0.4312Epoch 00347: loss did not improve\n",
      "4908/4908 [==============================] - 1s - loss: 0.4299     \n",
      "Epoch 349/500\n",
      "4864/4908 [============================>.] - ETA: 0s - loss: 0.4280Epoch 00348: loss did not improve\n",
      "4908/4908 [==============================] - 1s - loss: 0.4275     \n",
      "Epoch 350/500\n",
      "4864/4908 [============================>.] - ETA: 0s - loss: 0.4379Epoch 00349: loss did not improve\n",
      "4908/4908 [==============================] - 1s - loss: 0.4379     \n",
      "Epoch 351/500\n",
      "4864/4908 [============================>.] - ETA: 0s - loss: 0.4329Epoch 00350: loss did not improve\n",
      "4908/4908 [==============================] - 1s - loss: 0.4336     \n",
      "Epoch 352/500\n",
      "4864/4908 [============================>.] - ETA: 0s - loss: 0.4308Epoch 00351: loss did not improve\n",
      "4908/4908 [==============================] - 1s - loss: 0.4323     \n",
      "Epoch 353/500\n",
      "4864/4908 [============================>.] - ETA: 0s - loss: 0.4203Epoch 00352: loss improved from 0.42316 to 0.42013, saving model to checkpoints/Word-gen-text-weights-352-0.4201.hdf5\n",
      "4908/4908 [==============================] - 1s - loss: 0.4201     \n",
      "Epoch 354/500\n",
      "4864/4908 [============================>.] - ETA: 0s - loss: 0.4341Epoch 00353: loss did not improve\n",
      "4908/4908 [==============================] - 1s - loss: 0.4331     \n",
      "Epoch 355/500\n",
      "4800/4908 [============================>.] - ETA: 0s - loss: 0.4084Epoch 00354: loss improved from 0.42013 to 0.40720, saving model to checkpoints/Word-gen-text-weights-354-0.4072.hdf5\n",
      "4908/4908 [==============================] - 1s - loss: 0.4072     \n",
      "Epoch 356/500\n",
      "4736/4908 [===========================>..] - ETA: 0s - loss: 0.4425Epoch 00355: loss did not improve\n",
      "4908/4908 [==============================] - 1s - loss: 0.4383     \n",
      "Epoch 357/500\n",
      "4800/4908 [============================>.] - ETA: 0s - loss: 0.4182Epoch 00356: loss did not improve\n",
      "4908/4908 [==============================] - 1s - loss: 0.4191     \n",
      "Epoch 358/500\n",
      "4864/4908 [============================>.] - ETA: 0s - loss: 0.4165Epoch 00357: loss did not improve\n",
      "4908/4908 [==============================] - 1s - loss: 0.4162     \n",
      "Epoch 359/500\n",
      "4864/4908 [============================>.] - ETA: 0s - loss: 0.4247Epoch 00358: loss did not improve\n",
      "4908/4908 [==============================] - 1s - loss: 0.4260     \n",
      "Epoch 360/500\n",
      "4864/4908 [============================>.] - ETA: 0s - loss: 0.4238Epoch 00359: loss did not improve\n",
      "4908/4908 [==============================] - 1s - loss: 0.4228     \n",
      "Epoch 361/500\n",
      "4864/4908 [============================>.] - ETA: 0s - loss: 0.4076Epoch 00360: loss improved from 0.40720 to 0.40693, saving model to checkpoints/Word-gen-text-weights-360-0.4069.hdf5\n",
      "4908/4908 [==============================] - 1s - loss: 0.4069     \n",
      "Epoch 362/500\n",
      "4800/4908 [============================>.] - ETA: 0s - loss: 0.4237Epoch 00361: loss did not improve\n",
      "4908/4908 [==============================] - 1s - loss: 0.4245     \n",
      "Epoch 363/500\n",
      "4864/4908 [============================>.] - ETA: 0s - loss: 0.4034Epoch 00362: loss improved from 0.40693 to 0.40255, saving model to checkpoints/Word-gen-text-weights-362-0.4025.hdf5\n",
      "4908/4908 [==============================] - 1s - loss: 0.4025     \n",
      "Epoch 364/500\n",
      "4800/4908 [============================>.] - ETA: 0s - loss: 0.4120Epoch 00363: loss did not improve\n",
      "4908/4908 [==============================] - 1s - loss: 0.4145     \n",
      "Epoch 365/500\n",
      "4736/4908 [===========================>..] - ETA: 0s - loss: 0.4125Epoch 00364: loss did not improve\n",
      "4908/4908 [==============================] - 1s - loss: 0.4138     \n",
      "Epoch 366/500\n",
      "4864/4908 [============================>.] - ETA: 0s - loss: 0.4160Epoch 00365: loss did not improve\n",
      "4908/4908 [==============================] - 1s - loss: 0.4175     \n",
      "Epoch 367/500\n",
      "4864/4908 [============================>.] - ETA: 0s - loss: 0.4237Epoch 00366: loss did not improve\n",
      "4908/4908 [==============================] - 1s - loss: 0.4253     \n",
      "Epoch 368/500\n",
      "4864/4908 [============================>.] - ETA: 0s - loss: 0.4197Epoch 00367: loss did not improve\n",
      "4908/4908 [==============================] - 1s - loss: 0.4194     \n",
      "Epoch 369/500\n",
      "4864/4908 [============================>.] - ETA: 0s - loss: 0.4246Epoch 00368: loss did not improve\n",
      "4908/4908 [==============================] - 1s - loss: 0.4249     \n",
      "Epoch 370/500\n",
      "4736/4908 [===========================>..] - ETA: 0s - loss: 0.3996Epoch 00369: loss did not improve\n",
      "4908/4908 [==============================] - 1s - loss: 0.4041     \n",
      "Epoch 371/500\n",
      "4736/4908 [===========================>..] - ETA: 0s - loss: 0.4167Epoch 00370: loss did not improve\n",
      "4908/4908 [==============================] - 1s - loss: 0.4209     \n",
      "Epoch 372/500\n",
      "4864/4908 [============================>.] - ETA: 0s - loss: 0.4221Epoch 00371: loss did not improve\n",
      "4908/4908 [==============================] - 1s - loss: 0.4219     \n",
      "Epoch 373/500\n",
      "4864/4908 [============================>.] - ETA: 0s - loss: 0.4220Epoch 00372: loss did not improve\n",
      "4908/4908 [==============================] - 1s - loss: 0.4231     \n",
      "Epoch 374/500\n",
      "4736/4908 [===========================>..] - ETA: 0s - loss: 0.4159Epoch 00373: loss did not improve\n",
      "4908/4908 [==============================] - 1s - loss: 0.4156     \n",
      "Epoch 375/500\n",
      "4800/4908 [============================>.] - ETA: 0s - loss: 0.4062Epoch 00374: loss did not improve\n",
      "4908/4908 [==============================] - 1s - loss: 0.4074     \n",
      "Epoch 376/500\n",
      "4800/4908 [============================>.] - ETA: 0s - loss: 0.3976Epoch 00375: loss improved from 0.40255 to 0.39900, saving model to checkpoints/Word-gen-text-weights-375-0.3990.hdf5\n",
      "4908/4908 [==============================] - 1s - loss: 0.3990     \n",
      "Epoch 377/500\n",
      "4864/4908 [============================>.] - ETA: 0s - loss: 0.4185Epoch 00376: loss did not improve\n",
      "4908/4908 [==============================] - 1s - loss: 0.4193     \n",
      "Epoch 378/500\n",
      "4864/4908 [============================>.] - ETA: 0s - loss: 0.4096Epoch 00377: loss did not improve\n",
      "4908/4908 [==============================] - 1s - loss: 0.4086     \n",
      "Epoch 379/500\n",
      "4864/4908 [============================>.] - ETA: 0s - loss: 0.4000Epoch 00378: loss did not improve\n",
      "4908/4908 [==============================] - 1s - loss: 0.3997     \n",
      "Epoch 380/500\n",
      "4800/4908 [============================>.] - ETA: 0s - loss: 0.4032Epoch 00379: loss did not improve\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4908/4908 [==============================] - 1s - loss: 0.4031     \n",
      "Epoch 381/500\n",
      "4736/4908 [===========================>..] - ETA: 0s - loss: 0.4119Epoch 00380: loss did not improve\n",
      "4908/4908 [==============================] - 1s - loss: 0.4116     \n",
      "Epoch 382/500\n",
      "4864/4908 [============================>.] - ETA: 0s - loss: 0.4023Epoch 00381: loss did not improve\n",
      "4908/4908 [==============================] - 1s - loss: 0.4018     \n",
      "Epoch 383/500\n",
      "4864/4908 [============================>.] - ETA: 0s - loss: 0.4173Epoch 00382: loss did not improve\n",
      "4908/4908 [==============================] - 1s - loss: 0.4171     \n",
      "Epoch 384/500\n",
      "4864/4908 [============================>.] - ETA: 0s - loss: 0.3954Epoch 00383: loss improved from 0.39900 to 0.39515, saving model to checkpoints/Word-gen-text-weights-383-0.3952.hdf5\n",
      "4908/4908 [==============================] - 1s - loss: 0.3952     \n",
      "Epoch 385/500\n",
      "4864/4908 [============================>.] - ETA: 0s - loss: 0.4209Epoch 00384: loss did not improve\n",
      "4908/4908 [==============================] - 1s - loss: 0.4200     \n",
      "Epoch 386/500\n",
      "4736/4908 [===========================>..] - ETA: 0s - loss: 0.4065Epoch 00385: loss did not improve\n",
      "4908/4908 [==============================] - 1s - loss: 0.4055     \n",
      "Epoch 387/500\n",
      "4800/4908 [============================>.] - ETA: 0s - loss: 0.3920Epoch 00386: loss did not improve\n",
      "4908/4908 [==============================] - 1s - loss: 0.3952     \n",
      "Epoch 388/500\n",
      "4864/4908 [============================>.] - ETA: 0s - loss: 0.4040Epoch 00387: loss did not improve\n",
      "4908/4908 [==============================] - 1s - loss: 0.4041     \n",
      "Epoch 389/500\n",
      "4864/4908 [============================>.] - ETA: 0s - loss: 0.4028Epoch 00388: loss did not improve\n",
      "4908/4908 [==============================] - 1s - loss: 0.4033     \n",
      "Epoch 390/500\n",
      "4864/4908 [============================>.] - ETA: 0s - loss: 0.3976Epoch 00389: loss did not improve\n",
      "4908/4908 [==============================] - 1s - loss: 0.3998     \n",
      "Epoch 391/500\n",
      "4864/4908 [============================>.] - ETA: 0s - loss: 0.4276Epoch 00390: loss did not improve\n",
      "4908/4908 [==============================] - 1s - loss: 0.4276     \n",
      "Epoch 392/500\n",
      "4800/4908 [============================>.] - ETA: 0s - loss: 0.4141Epoch 00391: loss did not improve\n",
      "4908/4908 [==============================] - 1s - loss: 0.4128     \n",
      "Epoch 393/500\n",
      "4864/4908 [============================>.] - ETA: 0s - loss: 0.4013Epoch 00392: loss did not improve\n",
      "4908/4908 [==============================] - 1s - loss: 0.4014     \n",
      "Epoch 394/500\n",
      "4864/4908 [============================>.] - ETA: 0s - loss: 0.4030Epoch 00393: loss did not improve\n",
      "4908/4908 [==============================] - 1s - loss: 0.4028     \n",
      "Epoch 395/500\n",
      "4864/4908 [============================>.] - ETA: 0s - loss: 0.4136Epoch 00394: loss did not improve\n",
      "4908/4908 [==============================] - 1s - loss: 0.4130     \n",
      "Epoch 396/500\n",
      "4864/4908 [============================>.] - ETA: 0s - loss: 0.3977Epoch 00395: loss did not improve\n",
      "4908/4908 [==============================] - 1s - loss: 0.3971     \n",
      "Epoch 397/500\n",
      "4864/4908 [============================>.] - ETA: 0s - loss: 0.4093Epoch 00396: loss did not improve\n",
      "4908/4908 [==============================] - 1s - loss: 0.4093     \n",
      "Epoch 398/500\n",
      "4800/4908 [============================>.] - ETA: 0s - loss: 0.4028Epoch 00397: loss did not improve\n",
      "4908/4908 [==============================] - 1s - loss: 0.4043     \n",
      "Epoch 399/500\n",
      "4864/4908 [============================>.] - ETA: 0s - loss: 0.3994Epoch 00398: loss did not improve\n",
      "4908/4908 [==============================] - 1s - loss: 0.3994     \n",
      "Epoch 400/500\n",
      "4864/4908 [============================>.] - ETA: 0s - loss: 0.4075Epoch 00399: loss did not improve\n",
      "4908/4908 [==============================] - 1s - loss: 0.4072     \n",
      "Epoch 401/500\n",
      "4864/4908 [============================>.] - ETA: 0s - loss: 0.4152Epoch 00400: loss did not improve\n",
      "4908/4908 [==============================] - 1s - loss: 0.4134     \n",
      "Epoch 402/500\n",
      "4864/4908 [============================>.] - ETA: 0s - loss: 0.4116Epoch 00401: loss did not improve\n",
      "4908/4908 [==============================] - 1s - loss: 0.4139     \n",
      "Epoch 403/500\n",
      "4864/4908 [============================>.] - ETA: 0s - loss: 0.3961Epoch 00402: loss did not improve\n",
      "4908/4908 [==============================] - 1s - loss: 0.3971     \n",
      "Epoch 404/500\n",
      "4864/4908 [============================>.] - ETA: 0s - loss: 0.4041- ETA: 0s - loss:Epoch 00403: loss did not improve\n",
      "4908/4908 [==============================] - 1s - loss: 0.4041     \n",
      "Epoch 405/500\n",
      "4864/4908 [============================>.] - ETA: 0s - loss: 0.3953Epoch 00404: loss improved from 0.39515 to 0.39496, saving model to checkpoints/Word-gen-text-weights-404-0.3950.hdf5\n",
      "4908/4908 [==============================] - 1s - loss: 0.3950     \n",
      "Epoch 406/500\n",
      "4736/4908 [===========================>..] - ETA: 0s - loss: 0.3994Epoch 00405: loss did not improve\n",
      "4908/4908 [==============================] - 1s - loss: 0.4022     \n",
      "Epoch 407/500\n",
      "4800/4908 [============================>.] - ETA: 0s - loss: 0.3883Epoch 00406: loss improved from 0.39496 to 0.38675, saving model to checkpoints/Word-gen-text-weights-406-0.3867.hdf5\n",
      "4908/4908 [==============================] - 1s - loss: 0.3867     \n",
      "Epoch 408/500\n",
      "4864/4908 [============================>.] - ETA: 0s - loss: 0.3981Epoch 00407: loss did not improve\n",
      "4908/4908 [==============================] - 1s - loss: 0.3985     \n",
      "Epoch 409/500\n",
      "4736/4908 [===========================>..] - ETA: 0s - loss: 0.3978Epoch 00408: loss did not improve\n",
      "4908/4908 [==============================] - 1s - loss: 0.3990     \n",
      "Epoch 410/500\n",
      "4864/4908 [============================>.] - ETA: 0s - loss: 0.3955Epoch 00409: loss did not improve\n",
      "4908/4908 [==============================] - 1s - loss: 0.3981     \n",
      "Epoch 411/500\n",
      "4800/4908 [============================>.] - ETA: 0s - loss: 0.4038Epoch 00410: loss did not improve\n",
      "4908/4908 [==============================] - 1s - loss: 0.4036     \n",
      "Epoch 412/500\n",
      "4736/4908 [===========================>..] - ETA: 0s - loss: 0.3965- ETA: 0s - loss: 0Epoch 00411: loss did not improve\n",
      "4908/4908 [==============================] - 1s - loss: 0.3953     \n",
      "Epoch 413/500\n",
      "4736/4908 [===========================>..] - ETA: 0s - loss: 0.4006Epoch 00412: loss did not improve\n",
      "4908/4908 [==============================] - 1s - loss: 0.3997     \n",
      "Epoch 414/500\n",
      "4736/4908 [===========================>..] - ETA: 0s - loss: 0.4021- ETA: 0s -Epoch 00413: loss did not improve\n",
      "4908/4908 [==============================] - 1s - loss: 0.4026     \n",
      "Epoch 415/500\n",
      "4864/4908 [============================>.] - ETA: 0s - loss: 0.4109Epoch 00414: loss did not improve\n",
      "4908/4908 [==============================] - 1s - loss: 0.4125     \n",
      "Epoch 416/500\n",
      "4864/4908 [============================>.] - ETA: 0s - loss: 0.3908Epoch 00415: loss did not improve\n",
      "4908/4908 [==============================] - 1s - loss: 0.3898     \n",
      "Epoch 417/500\n",
      "4864/4908 [============================>.] - ETA: 0s - loss: 0.3910Epoch 00416: loss did not improve\n",
      "4908/4908 [==============================] - 1s - loss: 0.3917     \n",
      "Epoch 418/500\n",
      "4736/4908 [===========================>..] - ETA: 0s - loss: 0.4004Epoch 00417: loss did not improve\n",
      "4908/4908 [==============================] - 1s - loss: 0.3959     \n",
      "Epoch 419/500\n",
      "4800/4908 [============================>.] - ETA: 0s - loss: 0.3986Epoch 00418: loss did not improve\n",
      "4908/4908 [==============================] - 1s - loss: 0.3964     \n",
      "Epoch 420/500\n",
      "4800/4908 [============================>.] - ETA: 0s - loss: 0.4058Epoch 00419: loss did not improve\n",
      "4908/4908 [==============================] - 1s - loss: 0.4032     \n",
      "Epoch 421/500\n",
      "4864/4908 [============================>.] - ETA: 0s - loss: 0.3858Epoch 00420: loss did not improve\n",
      "4908/4908 [==============================] - 1s - loss: 0.3877     \n",
      "Epoch 422/500\n",
      "4800/4908 [============================>.] - ETA: 0s - loss: 0.4022- ETA: 0s - loEpoch 00421: loss did not improve\n",
      "4908/4908 [==============================] - 1s - loss: 0.3993     \n",
      "Epoch 423/500\n",
      "4864/4908 [============================>.] - ETA: 0s - loss: 0.3948Epoch 00422: loss did not improve\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4908/4908 [==============================] - 1s - loss: 0.3957     \n",
      "Epoch 424/500\n",
      "4864/4908 [============================>.] - ETA: 0s - loss: 0.3956Epoch 00423: loss did not improve\n",
      "4908/4908 [==============================] - 1s - loss: 0.3957     \n",
      "Epoch 425/500\n",
      "4864/4908 [============================>.] - ETA: 0s - loss: 0.3996Epoch 00424: loss did not improve\n",
      "4908/4908 [==============================] - 1s - loss: 0.3992     \n",
      "Epoch 426/500\n",
      "4864/4908 [============================>.] - ETA: 0s - loss: 0.3977Epoch 00425: loss did not improve\n",
      "4908/4908 [==============================] - 1s - loss: 0.3981     \n",
      "Epoch 427/500\n",
      "4800/4908 [============================>.] - ETA: 0s - loss: 0.3931Epoch 00426: loss did not improve\n",
      "4908/4908 [==============================] - 1s - loss: 0.3925     \n",
      "Epoch 428/500\n",
      "4864/4908 [============================>.] - ETA: 0s - loss: 0.3985Epoch 00427: loss did not improve\n",
      "4908/4908 [==============================] - 1s - loss: 0.3987     \n",
      "Epoch 429/500\n",
      "4864/4908 [============================>.] - ETA: 0s - loss: 0.3922Epoch 00428: loss did not improve\n",
      "4908/4908 [==============================] - 1s - loss: 0.3942     \n",
      "Epoch 430/500\n",
      "4864/4908 [============================>.] - ETA: 0s - loss: 0.3880Epoch 00429: loss did not improve\n",
      "4908/4908 [==============================] - 1s - loss: 0.3873     \n",
      "Epoch 431/500\n",
      "4864/4908 [============================>.] - ETA: 0s - loss: 0.4018Epoch 00430: loss did not improve\n",
      "4908/4908 [==============================] - 1s - loss: 0.4009     \n",
      "Epoch 432/500\n",
      "4864/4908 [============================>.] - ETA: 0s - loss: 0.3878Epoch 00431: loss did not improve\n",
      "4908/4908 [==============================] - 1s - loss: 0.3869     \n",
      "Epoch 433/500\n",
      "4864/4908 [============================>.] - ETA: 0s - loss: 0.4118Epoch 00432: loss did not improve\n",
      "4908/4908 [==============================] - 1s - loss: 0.4118     \n",
      "Epoch 434/500\n",
      "4864/4908 [============================>.] - ETA: 0s - loss: 0.3888Epoch 00433: loss did not improve\n",
      "4908/4908 [==============================] - 1s - loss: 0.3891     \n",
      "Epoch 435/500\n",
      "4864/4908 [============================>.] - ETA: 0s - loss: 0.4002Epoch 00434: loss did not improve\n",
      "4908/4908 [==============================] - 1s - loss: 0.3998     \n",
      "Epoch 436/500\n",
      "4864/4908 [============================>.] - ETA: 0s - loss: 0.3882- ETA: 0s - loss: 0.386Epoch 00435: loss did not improve\n",
      "4908/4908 [==============================] - 1s - loss: 0.3875     \n",
      "Epoch 437/500\n",
      "4864/4908 [============================>.] - ETA: 0s - loss: 0.3974Epoch 00436: loss did not improve\n",
      "4908/4908 [==============================] - 1s - loss: 0.3978     \n",
      "Epoch 438/500\n",
      "4864/4908 [============================>.] - ETA: 0s - loss: 0.3912Epoch 00437: loss did not improve\n",
      "4908/4908 [==============================] - 1s - loss: 0.3900     \n",
      "Epoch 439/500\n",
      "4864/4908 [============================>.] - ETA: 0s - loss: 0.3993Epoch 00438: loss did not improve\n",
      "4908/4908 [==============================] - 1s - loss: 0.3996     \n",
      "Epoch 440/500\n",
      "4864/4908 [============================>.] - ETA: 0s - loss: 0.3807Epoch 00439: loss improved from 0.38675 to 0.38006, saving model to checkpoints/Word-gen-text-weights-439-0.3801.hdf5\n",
      "4908/4908 [==============================] - 1s - loss: 0.3801     \n",
      "Epoch 441/500\n",
      "4864/4908 [============================>.] - ETA: 0s - loss: 0.4098Epoch 00440: loss did not improve\n",
      "4908/4908 [==============================] - 1s - loss: 0.4089     \n",
      "Epoch 442/500\n",
      "4864/4908 [============================>.] - ETA: 0s - loss: 0.4010Epoch 00441: loss did not improve\n",
      "4908/4908 [==============================] - 1s - loss: 0.3996     \n",
      "Epoch 443/500\n",
      "4864/4908 [============================>.] - ETA: 0s - loss: 0.3949Epoch 00442: loss did not improve\n",
      "4908/4908 [==============================] - 1s - loss: 0.3941     \n",
      "Epoch 444/500\n",
      "4864/4908 [============================>.] - ETA: 0s - loss: 0.3844Epoch 00443: loss did not improve\n",
      "4908/4908 [==============================] - 1s - loss: 0.3847     \n",
      "Epoch 445/500\n",
      "4864/4908 [============================>.] - ETA: 0s - loss: 0.3851Epoch 00444: loss did not improve\n",
      "4908/4908 [==============================] - 1s - loss: 0.3851     \n",
      "Epoch 446/500\n",
      "4864/4908 [============================>.] - ETA: 0s - loss: 0.4007Epoch 00445: loss did not improve\n",
      "4908/4908 [==============================] - 1s - loss: 0.4001     \n",
      "Epoch 447/500\n",
      "4864/4908 [============================>.] - ETA: 0s - loss: 0.3853Epoch 00446: loss did not improve\n",
      "4908/4908 [==============================] - 1s - loss: 0.3856     \n",
      "Epoch 448/500\n",
      "4800/4908 [============================>.] - ETA: 0s - loss: 0.3982Epoch 00447: loss did not improve\n",
      "4908/4908 [==============================] - 1s - loss: 0.3974     \n",
      "Epoch 449/500\n",
      "4864/4908 [============================>.] - ETA: 0s - loss: 0.3969Epoch 00448: loss did not improve\n",
      "4908/4908 [==============================] - 1s - loss: 0.3968     \n",
      "Epoch 450/500\n",
      "4864/4908 [============================>.] - ETA: 0s - loss: 0.3968Epoch 00449: loss did not improve\n",
      "4908/4908 [==============================] - 1s - loss: 0.3981     \n",
      "Epoch 451/500\n",
      "4864/4908 [============================>.] - ETA: 0s - loss: 0.4007Epoch 00450: loss did not improve\n",
      "4908/4908 [==============================] - 1s - loss: 0.4023     \n",
      "Epoch 452/500\n",
      "4864/4908 [============================>.] - ETA: 0s - loss: 0.3895Epoch 00451: loss did not improve\n",
      "4908/4908 [==============================] - 1s - loss: 0.3883     \n",
      "Epoch 453/500\n",
      "4800/4908 [============================>.] - ETA: 0s - loss: 0.3788- ETA: 1sEpoch 00452: loss improved from 0.38006 to 0.37757, saving model to checkpoints/Word-gen-text-weights-452-0.3776.hdf5\n",
      "4908/4908 [==============================] - 1s - loss: 0.3776     \n",
      "Epoch 454/500\n",
      "4864/4908 [============================>.] - ETA: 0s - loss: 0.3787Epoch 00453: loss did not improve\n",
      "4908/4908 [==============================] - 1s - loss: 0.3782     \n",
      "Epoch 455/500\n",
      "4864/4908 [============================>.] - ETA: 0s - loss: 0.4061Epoch 00454: loss did not improve\n",
      "4908/4908 [==============================] - 1s - loss: 0.4049     \n",
      "Epoch 456/500\n",
      "4800/4908 [============================>.] - ETA: 0s - loss: 0.3867- ETA: 0s - loss: 0Epoch 00455: loss did not improve\n",
      "4908/4908 [==============================] - 1s - loss: 0.3887     \n",
      "Epoch 457/500\n",
      "4800/4908 [============================>.] - ETA: 0s - loss: 0.3843Epoch 00456: loss did not improve\n",
      "4908/4908 [==============================] - 1s - loss: 0.3855     \n",
      "Epoch 458/500\n",
      "4864/4908 [============================>.] - ETA: 0s - loss: 0.3904Epoch 00457: loss did not improve\n",
      "4908/4908 [==============================] - 1s - loss: 0.3923     \n",
      "Epoch 459/500\n",
      "4800/4908 [============================>.] - ETA: 0s - loss: 0.3907Epoch 00458: loss did not improve\n",
      "4908/4908 [==============================] - 1s - loss: 0.3913     \n",
      "Epoch 460/500\n",
      "4736/4908 [===========================>..] - ETA: 0s - loss: 0.3778Epoch 00459: loss improved from 0.37757 to 0.37690, saving model to checkpoints/Word-gen-text-weights-459-0.3769.hdf5\n",
      "4908/4908 [==============================] - 1s - loss: 0.3769     \n",
      "Epoch 461/500\n",
      "4864/4908 [============================>.] - ETA: 0s - loss: 0.3825Epoch 00460: loss did not improve\n",
      "4908/4908 [==============================] - 1s - loss: 0.3827     \n",
      "Epoch 462/500\n",
      "4864/4908 [============================>.] - ETA: 0s - loss: 0.3894Epoch 00461: loss did not improve\n",
      "4908/4908 [==============================] - 1s - loss: 0.3896     \n",
      "Epoch 463/500\n",
      "4864/4908 [============================>.] - ETA: 0s - loss: 0.4008Epoch 00462: loss did not improve\n",
      "4908/4908 [==============================] - 1s - loss: 0.3997     \n",
      "Epoch 464/500\n",
      "4800/4908 [============================>.] - ETA: 0s - loss: 0.3911Epoch 00463: loss did not improve\n",
      "4908/4908 [==============================] - 1s - loss: 0.3923     \n",
      "Epoch 465/500\n",
      "4864/4908 [============================>.] - ETA: 0s - loss: 0.4004Epoch 00464: loss did not improve\n",
      "4908/4908 [==============================] - 1s - loss: 0.4008     \n",
      "Epoch 466/500\n",
      "4864/4908 [============================>.] - ETA: 0s - loss: 0.3819Epoch 00465: loss did not improve\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4908/4908 [==============================] - 1s - loss: 0.3807     \n",
      "Epoch 467/500\n",
      "4864/4908 [============================>.] - ETA: 0s - loss: 0.3928Epoch 00466: loss did not improve\n",
      "4908/4908 [==============================] - 1s - loss: 0.3923     \n",
      "Epoch 468/500\n",
      "4800/4908 [============================>.] - ETA: 0s - loss: 0.3890Epoch 00467: loss did not improve\n",
      "4908/4908 [==============================] - 1s - loss: 0.3909     \n",
      "Epoch 469/500\n",
      "4864/4908 [============================>.] - ETA: 0s - loss: 0.3835Epoch 00468: loss did not improve\n",
      "4908/4908 [==============================] - 1s - loss: 0.3840     \n",
      "Epoch 470/500\n",
      "4864/4908 [============================>.] - ETA: 0s - loss: 0.3966Epoch 00469: loss did not improve\n",
      "4908/4908 [==============================] - 1s - loss: 0.3955     \n",
      "Epoch 471/500\n",
      "4864/4908 [============================>.] - ETA: 0s - loss: 0.4015Epoch 00470: loss did not improve\n",
      "4908/4908 [==============================] - 1s - loss: 0.4003     \n",
      "Epoch 472/500\n",
      "4864/4908 [============================>.] - ETA: 0s - loss: 0.4082Epoch 00471: loss did not improve\n",
      "4908/4908 [==============================] - 1s - loss: 0.4085     \n",
      "Epoch 473/500\n",
      "4864/4908 [============================>.] - ETA: 0s - loss: 0.3890Epoch 00472: loss did not improve\n",
      "4908/4908 [==============================] - 1s - loss: 0.3891     \n",
      "Epoch 474/500\n",
      "4864/4908 [============================>.] - ETA: 0s - loss: 0.3950Epoch 00473: loss did not improve\n",
      "4908/4908 [==============================] - 1s - loss: 0.3966     \n",
      "Epoch 475/500\n",
      "4864/4908 [============================>.] - ETA: 0s - loss: 0.3777Epoch 00474: loss improved from 0.37690 to 0.37668, saving model to checkpoints/Word-gen-text-weights-474-0.3767.hdf5\n",
      "4908/4908 [==============================] - 1s - loss: 0.3767     \n",
      "Epoch 476/500\n",
      "4864/4908 [============================>.] - ETA: 0s - loss: 0.3859Epoch 00475: loss did not improve\n",
      "4908/4908 [==============================] - 1s - loss: 0.3856     \n",
      "Epoch 477/500\n",
      "4800/4908 [============================>.] - ETA: 0s - loss: 0.3916Epoch 00476: loss did not improve\n",
      "4908/4908 [==============================] - 1s - loss: 0.3908     \n",
      "Epoch 478/500\n",
      "4864/4908 [============================>.] - ETA: 0s - loss: 0.3860Epoch 00477: loss did not improve\n",
      "4908/4908 [==============================] - 1s - loss: 0.3879     \n",
      "Epoch 479/500\n",
      "4864/4908 [============================>.] - ETA: 0s - loss: 0.4008Epoch 00478: loss did not improve\n",
      "4908/4908 [==============================] - 1s - loss: 0.4011     \n",
      "Epoch 480/500\n",
      "4800/4908 [============================>.] - ETA: 0s - loss: 0.3829Epoch 00479: loss did not improve\n",
      "4908/4908 [==============================] - 1s - loss: 0.3816     \n",
      "Epoch 481/500\n",
      "4864/4908 [============================>.] - ETA: 0s - loss: 0.3999Epoch 00480: loss did not improve\n",
      "4908/4908 [==============================] - 1s - loss: 0.3991     \n",
      "Epoch 482/500\n",
      "4864/4908 [============================>.] - ETA: 0s - loss: 0.3832Epoch 00481: loss did not improve\n",
      "4908/4908 [==============================] - 1s - loss: 0.3847     \n",
      "Epoch 483/500\n",
      "4864/4908 [============================>.] - ETA: 0s - loss: 0.3887Epoch 00482: loss did not improve\n",
      "4908/4908 [==============================] - 1s - loss: 0.3868     \n",
      "Epoch 484/500\n",
      "4864/4908 [============================>.] - ETA: 0s - loss: 0.3945Epoch 00483: loss did not improve\n",
      "4908/4908 [==============================] - 1s - loss: 0.3950     \n",
      "Epoch 485/500\n",
      "4864/4908 [============================>.] - ETA: 0s - loss: 0.3925Epoch 00484: loss did not improve\n",
      "4908/4908 [==============================] - 1s - loss: 0.3914     \n",
      "Epoch 486/500\n",
      "4864/4908 [============================>.] - ETA: 0s - loss: 0.3822Epoch 00485: loss did not improve\n",
      "4908/4908 [==============================] - 1s - loss: 0.3809     \n",
      "Epoch 487/500\n",
      "4864/4908 [============================>.] - ETA: 0s - loss: 0.3826Epoch 00486: loss did not improve\n",
      "4908/4908 [==============================] - 1s - loss: 0.3846     \n",
      "Epoch 488/500\n",
      "4864/4908 [============================>.] - ETA: 0s - loss: 0.3857Epoch 00487: loss did not improve\n",
      "4908/4908 [==============================] - 1s - loss: 0.3866     \n",
      "Epoch 489/500\n",
      "4864/4908 [============================>.] - ETA: 0s - loss: 0.3856Epoch 00488: loss did not improve\n",
      "4908/4908 [==============================] - 1s - loss: 0.3851     \n",
      "Epoch 490/500\n",
      "4864/4908 [============================>.] - ETA: 0s - loss: 0.3933Epoch 00489: loss did not improve\n",
      "4908/4908 [==============================] - 1s - loss: 0.3919     \n",
      "Epoch 491/500\n",
      "4864/4908 [============================>.] - ETA: 0s - loss: 0.3873Epoch 00490: loss did not improve\n",
      "4908/4908 [==============================] - 1s - loss: 0.3879     \n",
      "Epoch 492/500\n",
      "4864/4908 [============================>.] - ETA: 0s - loss: 0.3828Epoch 00491: loss did not improve\n",
      "4908/4908 [==============================] - 1s - loss: 0.3832     \n",
      "Epoch 493/500\n",
      "4800/4908 [============================>.] - ETA: 0s - loss: 0.3815Epoch 00492: loss did not improve\n",
      "4908/4908 [==============================] - 1s - loss: 0.3824     \n",
      "Epoch 494/500\n",
      "4864/4908 [============================>.] - ETA: 0s - loss: 0.3755Epoch 00493: loss improved from 0.37668 to 0.37665, saving model to checkpoints/Word-gen-text-weights-493-0.3766.hdf5\n",
      "4908/4908 [==============================] - 1s - loss: 0.3766     \n",
      "Epoch 495/500\n",
      "4864/4908 [============================>.] - ETA: 0s - loss: 0.3962Epoch 00494: loss did not improve\n",
      "4908/4908 [==============================] - 1s - loss: 0.3982     \n",
      "Epoch 496/500\n",
      "4800/4908 [============================>.] - ETA: 0s - loss: 0.3892Epoch 00495: loss did not improve\n",
      "4908/4908 [==============================] - 1s - loss: 0.3913     \n",
      "Epoch 497/500\n",
      "4864/4908 [============================>.] - ETA: 0s - loss: 0.3756Epoch 00496: loss did not improve\n",
      "4908/4908 [==============================] - 1s - loss: 0.3767     \n",
      "Epoch 498/500\n",
      "4864/4908 [============================>.] - ETA: 0s - loss: 0.3829Epoch 00497: loss did not improve\n",
      "4908/4908 [==============================] - 1s - loss: 0.3852     \n",
      "Epoch 499/500\n",
      "4864/4908 [============================>.] - ETA: 0s - loss: 0.3883Epoch 00498: loss did not improve\n",
      "4908/4908 [==============================] - 1s - loss: 0.3889     \n",
      "Epoch 500/500\n",
      "4736/4908 [===========================>..] - ETA: 0s - loss: 0.3896Epoch 00499: loss did not improve\n",
      "4908/4908 [==============================] - 1s - loss: 0.3917     \n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x7f92a0f01ac8>"
      ]
     },
     "execution_count": 99,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "generative_model.fit(X, y, epochs=500, batch_size=64, callbacks=callbacks_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "generative_model.save('Text_gen_01_words_no_embedding')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total Patterns:  4908\n"
     ]
    }
   ],
   "source": [
    "# prepare the dataset of input to output pairs encoded as integers\n",
    "seq_length = 30\n",
    "dataX = []\n",
    "dataY = []\n",
    "split_text = raw_text.split()\n",
    "for i in range(0, len(words) - seq_length, 1):\n",
    "    seq_in = split_text[i:i + seq_length]\n",
    "    seq_out = split_text[i + seq_length]\n",
    "    dataX.append([word_to_int[word] for word in seq_in])\n",
    "    dataY.append(word_to_int[seq_out])\n",
    "n_patterns = len(dataX)\n",
    "print(\"Total Patterns: \", n_patterns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# pick a random seed\n",
    "start = np.random.randint(0, len(dataX)-1)\n",
    "pattern = dataX[start]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "## Create a seed from scratch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['and', 'a', 'then', 'alice', 'looked', 'at', 'the', 'cat', 'and', 'the', 'mad', 'hatter', 'and', 'said', \"'how\", 'dare', 'you', \"rabbit'\", 'cried', 'alice', 'again,', 'for', 'this', 'time', 'the', 'mouse', 'was', 'bristling', 'all', 'over', 'every']\n"
     ]
    }
   ],
   "source": [
    "sam_text = \"And a then Alice looked at the cat and the mad hatter and said 'how dare you rabbit' cried alice again, for this time the mouse was bristling all over every \"\n",
    "len(sam_text.split())\n",
    "print(sam_text.lower().split())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "31\n"
     ]
    }
   ],
   "source": [
    "sam_pattern= []\n",
    "for word in sam_text.lower().split():\n",
    "    lookup = word_to_int[word]\n",
    "    sam_pattern.append(lookup)\n",
    "    \n",
    "print(len(sam_pattern))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 145,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[1,\n",
       " 3,\n",
       " 56,\n",
       " 14,\n",
       " 80,\n",
       " 16,\n",
       " 0,\n",
       " 229,\n",
       " 1,\n",
       " 0,\n",
       " 580,\n",
       " 152,\n",
       " 1,\n",
       " 6,\n",
       " 222,\n",
       " 626,\n",
       " 10,\n",
       " 3208,\n",
       " 200,\n",
       " 14,\n",
       " 112,\n",
       " 23,\n",
       " 31,\n",
       " 75,\n",
       " 0,\n",
       " 179,\n",
       " 9,\n",
       " 3144,\n",
       " 19,\n",
       " 117,\n",
       " 295]"
      ]
     },
     "execution_count": 145,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sam_pattern"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[111, 12, 4, 646, 2794, 35, 8, 0, 1608, 63, 4, 3266, 2749, 24, 1523, 61, 0, 863, 4720, 15, 1892, 1, 3331, 15, 3661, 4, 47, 87, 3, 658]\n",
      "\" herself, as she swam lazily about in the pool, 'and she sits purring so nicely by the fire, licking her paws and washing her face--and she is such a nice \"\n"
     ]
    }
   ],
   "source": [
    "print(pattern)\n",
    "print(\"\\\"\", ' '.join([int_to_word[value] for value in pattern]), \"\\\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Done.\n"
     ]
    }
   ],
   "source": [
    "generated_text = []\n",
    "\n",
    "# generate characters\n",
    "for i in range(100):\n",
    "    x = np.reshape(pattern, (1, len(pattern), 1))\n",
    "    x = x / float(n_vocab_words)\n",
    "    prediction = generative_model.predict(x, verbose=0)\n",
    "    index = np.argmax(prediction)\n",
    "    #print(index)\n",
    "    result = int_to_word[index]\n",
    "    seq_in = [int_to_word[value] for value in pattern]\n",
    "    #print(result)\n",
    "    pattern.append(index)\n",
    "    generated_text.append(index)\n",
    "    pattern = pattern[1:len(pattern)]\n",
    "print(\"\\nDone.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[186, 221, 325, 5, 30, 4339, 1, 647, 4315, 1, 4104, 1, 154, 0, 3, 1235, 2, 1494, 1, 0, 427, 791, 0, 11, 885, 1425, 11, 115, 309, 2]\n",
      "\" 'i'm sure those of not soft and generally hung and pegs. and took the a struck to dog and the house (alice the i indeed change, i can glad to \"\n",
      "\" felt fallen she it: i was a duck and a dodo, she lory can't was eaglet, and several other curious creatures. and led and way, and the whole party swam the the shore. chapter iii. and caucus-race and a french tale the australia?' (and the tried capital curtsey i she think!' (dinah the the cat.) 'i hope they'll remember the saucer with milk and tea-time. dinah the foot, smiling jaws!' 'i'm sure those of not soft and generally hung and pegs. and took the a struck to dog and the house (alice the i indeed change, i can glad to \"\n"
     ]
    }
   ],
   "source": [
    "print(pattern)\n",
    "print(\"\\\"\", ' '.join([int_to_word[value] for value in pattern]), \"\\\"\")\n",
    "print(\"\\\"\", ' '.join([int_to_word[value] for value in generated_text]), \"\\\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Done.\n"
     ]
    }
   ],
   "source": [
    "generated_text = []\n",
    "\n",
    "# generate characters\n",
    "for i in range(100):\n",
    "    x = np.reshape(pattern, (1, len(pattern), 1))\n",
    "    x = x / float(n_vocab_words)\n",
    "    prediction = generative_model.predict(x, verbose=0)\n",
    "    index = np.argmax(prediction)\n",
    "    #print(index)\n",
    "    result = int_to_word[index]\n",
    "    seq_in = [int_to_word[value] for value in pattern]\n",
    "    #print(result)\n",
    "    pattern.append(index)\n",
    "    generated_text.append(index)\n",
    "    pattern = pattern[1:len(pattern)]\n",
    "print(\"\\nDone.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[186, 221, 325, 5, 30, 4339, 1, 647, 4315, 1, 4104, 1, 154, 0, 3, 1235, 2, 1494, 1, 0, 427, 791, 0, 11, 885, 1425, 11, 115, 309, 2]\n",
      "\" 'i'm sure those of not soft and generally hung and pegs. and took the a struck to dog and the house (alice the i indeed change, i can glad to \"\n",
      "\" felt fallen she it: i was a duck and a dodo, she lory can't was eaglet, and several other curious creatures. and led and way, and the whole party swam the the shore. chapter iii. and caucus-race and a french tale the australia?' (and the tried capital curtsey i she think!' (dinah the the cat.) 'i hope they'll remember the saucer with milk and tea-time. dinah the foot, smiling jaws!' 'i'm sure those of not soft and generally hung and pegs. and took the a struck to dog and the house (alice the i indeed change, i can glad to \"\n"
     ]
    }
   ],
   "source": [
    "print(pattern)\n",
    "print(\"\\\"\", ' '.join([int_to_word[value] for value in pattern]), \"\\\"\")\n",
    "print(\"\\\"\", ' '.join([int_to_word[value] for value in generated_text]), \"\\\"\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Overfitting ?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Done.\n"
     ]
    }
   ],
   "source": [
    "generated_text = []\n",
    "\n",
    "# generate characters\n",
    "for i in range(100):\n",
    "    x = np.reshape(pattern, (1, len(pattern), 1))\n",
    "    x = x / float(n_vocab_words)\n",
    "    prediction = generative_model.predict(x, verbose=0)\n",
    "    index = np.argmax(prediction)\n",
    "    #print(index)\n",
    "    result = int_to_word[index]\n",
    "    seq_in = [int_to_word[value] for value in pattern]\n",
    "    #print(result)\n",
    "    pattern.append(index)\n",
    "    generated_text.append(index)\n",
    "    pattern = pattern[1:len(pattern)]\n",
    "print(\"\\nDone.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[111, 12, 4, 646, 2794, 35, 8, 0, 1608, 63, 4, 3266, 2749, 24, 1523, 61, 0, 863, 4720, 15, 1892, 1, 3331, 15, 3661, 4, 47, 87, 3, 658]\n",
      "\" herself, as she swam lazily about in the pool, 'and she sits purring so nicely by the fire, licking her paws and washing her face--and she is such a nice \"\n",
      "\" 'not like cats!' cried the mouse, in a shrill, passionate voice. 'would you like cats if you were me?' 'well, perhaps not,' said alice in a soothing tone: 'don't be angry about it. and yet i wish i could show you our cat dinah: i think you'd take a fancy to cats if you could only see her. she is such a dear quiet thing,' alice went on, half to herself, as she swam lazily about in the pool, 'and she sits purring so nicely by the fire, licking her paws and washing her face--and she is such a nice \"\n"
     ]
    }
   ],
   "source": [
    "print(pattern)\n",
    "print(\"\\\"\", ' '.join([int_to_word[value] for value in pattern]), \"\\\"\")\n",
    "print(\"\\\"\", ' '.join([int_to_word[value] for value in generated_text]), \"\\\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 146,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1, 3, 56, 14, 80, 16, 0, 229, 1, 0, 580, 152, 1, 6, 222, 626, 10, 3208, 200, 14, 112, 23, 31, 75, 0, 179, 9, 3144, 19, 117, 295]\n",
      "\" and a then alice looked at the cat and the mad hatter and said 'how dare you rabbit' cried alice again, for this time the mouse was bristling all over every \"\n"
     ]
    }
   ],
   "source": [
    "print(sam_pattern)\n",
    "print(\"\\\"\", ' '.join([int_to_word[value] for value in sam_pattern]), \"\\\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 147,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "Error when checking : expected input_2 to have shape (None, 30, 1) but got array with shape (1, 31, 1)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-147-ad7f6088578d>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      6\u001b[0m     \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreshape\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msam_pattern\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msam_pattern\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m     \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mx\u001b[0m \u001b[0;34m/\u001b[0m \u001b[0mfloat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mn_vocab_words\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 8\u001b[0;31m     \u001b[0mprediction\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mgenerative_model\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mverbose\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      9\u001b[0m     \u001b[0mindex\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0margmax\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mprediction\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     10\u001b[0m     \u001b[0;31m#print(index)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/samwit/anaconda3/envs/keras2/lib/python3.5/site-packages/keras/engine/training.py\u001b[0m in \u001b[0;36mpredict\u001b[0;34m(self, x, batch_size, verbose)\u001b[0m\n\u001b[1;32m   1497\u001b[0m         x = _standardize_input_data(x, self._feed_input_names,\n\u001b[1;32m   1498\u001b[0m                                     \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_feed_input_shapes\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1499\u001b[0;31m                                     check_batch_axis=False)\n\u001b[0m\u001b[1;32m   1500\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstateful\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1501\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0mbatch_size\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m%\u001b[0m \u001b[0mbatch_size\u001b[0m \u001b[0;34m!=\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/samwit/anaconda3/envs/keras2/lib/python3.5/site-packages/keras/engine/training.py\u001b[0m in \u001b[0;36m_standardize_input_data\u001b[0;34m(data, names, shapes, check_batch_axis, exception_prefix)\u001b[0m\n\u001b[1;32m    138\u001b[0m                             \u001b[0;34m' to have shape '\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mshapes\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m+\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    139\u001b[0m                             \u001b[0;34m' but got array with shape '\u001b[0m \u001b[0;34m+\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 140\u001b[0;31m                             str(array.shape))\n\u001b[0m\u001b[1;32m    141\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0marrays\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    142\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: Error when checking : expected input_2 to have shape (None, 30, 1) but got array with shape (1, 31, 1)"
     ]
    }
   ],
   "source": [
    "#sam_pattern.append(0)\n",
    "generated_text = []\n",
    "\n",
    "# generate characters\n",
    "for i in range(100):\n",
    "    x = np.reshape(sam_pattern, (1, len(sam_pattern), 1))\n",
    "    x = x / float(n_vocab_words)\n",
    "    prediction = generative_model.predict(x, verbose=0)\n",
    "    index = np.argmax(prediction)\n",
    "    #print(index)\n",
    "    result = int_to_word[index]\n",
    "    seq_in = [int_to_word[value] for value in pattern]\n",
    "    #print(result)\n",
    "    sam_pattern.append(index)\n",
    "    generated_text.append(index)\n",
    "    sam_pattern = pattern[1:len(pattern)]\n",
    "print(\"\\nDone.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[111, 12, 4, 646, 2794, 35, 8, 0, 1608, 63, 4, 3266, 2749, 24, 1523, 61, 0, 863, 4720, 15, 1892, 1, 3331, 15, 3661, 4, 47, 87, 3, 658]\n",
      "\" herself, as she swam lazily about in the pool, 'and she sits purring so nicely by the fire, licking her paws and washing her face--and she is such a nice \"\n",
      "\"  \"\n"
     ]
    }
   ],
   "source": [
    "print(pattern)\n",
    "print(\"\\\"\", ' '.join([int_to_word[value] for value in pattern]), \"\\\"\")\n",
    "print(\"\\\"\", ' '.join([int_to_word[value] for value in generated_text]), \"\\\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
