{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import sys\n",
    "\n",
    "import keras\n",
    "from keras.models import Sequential, Model, load_model\n",
    "from keras.layers import Dense, LSTM, Dropout, Input\n",
    "from keras.optimizers import RMSprop, Adam\n",
    "\n",
    "from keras.callbacks import ModelCheckpoint\n",
    "from keras.utils import np_utils\n",
    "\n",
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.2.1\n",
      "2.0.6\n"
     ]
    }
   ],
   "source": [
    "print(tf.__version__)\n",
    "print(keras.__version__)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load the corpus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# load ascii text and covert to lowercase\n",
    "filename = \"wonderland.txt\"\n",
    "raw_text = open(filename).read()\n",
    "raw_text = raw_text.lower()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Make look up tables based on characters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total Characters:  144343\n",
      "Total Unique chars:  44\n"
     ]
    }
   ],
   "source": [
    "# create mapping of unique chars to integers\n",
    "chars = sorted(list(set(raw_text)))\n",
    "char_to_int = dict((c, i) for i, c in enumerate(chars))\n",
    "int_to_char = dict((i, c) for i, c in enumerate(chars))\n",
    "\n",
    "# summarize the loaded data\n",
    "n_chars = len(raw_text)\n",
    "n_vocab = len(chars)\n",
    "print(\"Total Characters: \", n_chars)\n",
    "print(\"Total Unique chars: \", n_vocab)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#raw_text = raw_text[1:]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Make the sequences\n",
    "\n",
    "This shows you an example of making sequences sampled from the overall text data. \n",
    "\n",
    "We are creating sequences that are 100 characters long"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total sequences:  144243\n"
     ]
    }
   ],
   "source": [
    "# create input and output pairs\n",
    "seq_length = 100\n",
    "dataX = []\n",
    "dataY = []\n",
    "for i in range(0, n_chars - seq_length, 1):\n",
    "    seq_in = raw_text[i:i + seq_length]\n",
    "    seq_out = raw_text[i + seq_length]\n",
    "    dataX.append([char_to_int[char] for char in seq_in])\n",
    "    dataY.append(char_to_int[seq_out])\n",
    "n_patterns = len(dataX)\n",
    "print(\"Total sequences: \", n_patterns)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Lets examine some of these sequences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[43, 19, 24, 17, 32, 36, 21, 34, 1, 25, 10, 1, 20, 31, 39, 30, 1, 36, 24, 21, 1, 34, 17, 18, 18, 25, 36, 9, 24, 31, 28, 21, 0, 0, 17, 28, 25, 19, 21, 1, 39, 17, 35, 1, 18, 21, 23, 25, 30, 30, 25, 30, 23, 1, 36, 31, 1, 23, 21, 36, 1, 38, 21, 34, 41, 1, 36, 25, 34, 21, 20, 1, 31, 22, 1, 35, 25, 36, 36, 25, 30, 23, 1, 18, 41, 1, 24, 21, 34, 1, 35, 25, 35, 36, 21, 34, 1, 31, 30, 1]\n",
      "36\n"
     ]
    }
   ],
   "source": [
    "print(dataX[0])\n",
    "print(dataY[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\" ï»¿chapter i. down the rabbit-hole\n",
      "\n",
      "alice was beginning to get very tired of sitting by her sister on  \"\n",
      "t\n"
     ]
    }
   ],
   "source": [
    "print(\"\\\"\", ''.join([int_to_char[value] for value in dataX[0]]), \"\\\"\")\n",
    "print(int_to_char[dataY[0]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[19, 24, 17, 32, 36, 21, 34, 1, 25, 10, 1, 20, 31, 39, 30, 1, 36, 24, 21, 1, 34, 17, 18, 18, 25, 36, 9, 24, 31, 28, 21, 0, 0, 17, 28, 25, 19, 21, 1, 39, 17, 35, 1, 18, 21, 23, 25, 30, 30, 25, 30, 23, 1, 36, 31, 1, 23, 21, 36, 1, 38, 21, 34, 41, 1, 36, 25, 34, 21, 20, 1, 31, 22, 1, 35, 25, 36, 36, 25, 30, 23, 1, 18, 41, 1, 24, 21, 34, 1, 35, 25, 35, 36, 21, 34, 1, 31, 30, 1, 36]\n",
      "24\n"
     ]
    }
   ],
   "source": [
    "print(dataX[1])\n",
    "print(dataY[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\" chapter i. down the rabbit-hole\n",
      "\n",
      "alice was beginning to get very tired of sitting by her sister on t \"\n",
      "h\n"
     ]
    }
   ],
   "source": [
    "print(\"\\\"\", ''.join([int_to_char[value] for value in dataX[1]]), \"\\\"\")\n",
    "print(int_to_char[dataY[1]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\" pter i. down the rabbit-hole\n",
      "\n",
      "alice was beginning to get very tired of sitting by her sister on the\n",
      " \"\n",
      "b\n"
     ]
    }
   ],
   "source": [
    "print(\"\\\"\", ''.join([int_to_char[value] for value in dataX[4]]), \"\\\"\")\n",
    "print(int_to_char[dataY[4]])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Reshaping the sequences to become timesteps into the LSTM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(144243, 100, 1)\n"
     ]
    }
   ],
   "source": [
    "# reshape X to be [samples, time steps, features]\n",
    "X = np.reshape(dataX, (n_patterns, seq_length, 1))\n",
    "print(X.shape)\n",
    "\n",
    "# normalize\n",
    "X = X / float(n_vocab)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 0.97727273]\n",
      " [ 0.43181818]\n",
      " [ 0.54545455]\n",
      " [ 0.38636364]\n",
      " [ 0.72727273]\n",
      " [ 0.81818182]\n",
      " [ 0.47727273]\n",
      " [ 0.77272727]\n",
      " [ 0.02272727]\n",
      " [ 0.56818182]]\n",
      "36\n"
     ]
    }
   ],
   "source": [
    "print(X[0][:10])\n",
    "print(dataY[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# one hot encode the output variable\n",
    "y = np_utils.to_categorical(dataY)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 0.97727273]\n",
      " [ 0.43181818]\n",
      " [ 0.54545455]\n",
      " [ 0.38636364]\n",
      " [ 0.72727273]\n",
      " [ 0.81818182]\n",
      " [ 0.47727273]\n",
      " [ 0.77272727]\n",
      " [ 0.02272727]\n",
      " [ 0.56818182]]\n",
      "[ 0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.\n",
      "  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.\n",
      "  1.  0.  0.  0.  0.  0.  0.]\n"
     ]
    }
   ],
   "source": [
    "print(X[0][:10])\n",
    "print(y[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Creating our model\n",
    "\n",
    "We will use the return sequences = true to pass the sequence up to the 2nd LSTM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "our input shape is  (100, 1)\n"
     ]
    }
   ],
   "source": [
    "# define the input shape\n",
    "inp = Input(shape=(X.shape[1], X.shape[2]))\n",
    "print('our input shape is ',(X.shape[1], X.shape[2]) )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "x = LSTM(256, return_sequences = True)(inp) \n",
    "#x = Dropout(0.2)(x)\n",
    "x = LSTM(256)(x)\n",
    "#x = Dropout(0.2)(x)\n",
    "output = Dense(y.shape[1], activation ='softmax')(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "generative_model = Model(inputs = inp, outputs=output )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "optimizer = RMSprop(lr=0.01)\n",
    "generative_model.compile(loss='categorical_crossentropy', optimizer='adam')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# define the checkpoint\n",
    "filepath=\"checkpoints/weights-improvement-{epoch:02d}-{loss:.4f}-gentext-CharRNN.hdf5\"\n",
    "checkpoint = ModelCheckpoint(filepath, monitor='loss', verbose=1, save_best_only=True, mode='min')\n",
    "callbacks_list = [checkpoint]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "144192/144243 [============================>.] - ETA: 0s - loss: 2.7150Epoch 00000: loss improved from inf to 2.71497, saving model to checkpoints/weights-improvement-00-2.7150-gentext-CharRNN.hdf5\n",
      "144243/144243 [==============================] - 315s - loss: 2.7150   \n",
      "Epoch 2/10\n",
      "144192/144243 [============================>.] - ETA: 0s - loss: 2.2816Epoch 00001: loss improved from 2.71497 to 2.28165, saving model to checkpoints/weights-improvement-01-2.2817-gentext-CharRNN.hdf5\n",
      "144243/144243 [==============================] - 314s - loss: 2.2817   \n",
      "Epoch 3/10\n",
      "144192/144243 [============================>.] - ETA: 0s - loss: 2.0552Epoch 00002: loss improved from 2.28165 to 2.05535, saving model to checkpoints/weights-improvement-02-2.0553-gentext-CharRNN.hdf5\n",
      "144243/144243 [==============================] - 311s - loss: 2.0553   \n",
      "Epoch 4/10\n",
      "144192/144243 [============================>.] - ETA: 0s - loss: 1.9099Epoch 00003: loss improved from 2.05535 to 1.90994, saving model to checkpoints/weights-improvement-03-1.9099-gentext-CharRNN.hdf5\n",
      "144243/144243 [==============================] - 312s - loss: 1.9099   \n",
      "Epoch 5/10\n",
      "108992/144243 [=====================>........] - ETA: 76s - loss: 1.8108"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-37-2bdf80f1dcb0>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mgenerative_model\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mepochs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m10\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m64\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcallbacks\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcallbacks_list\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m/home/samwit/anaconda3/envs/keras2/lib/python3.5/site-packages/keras/engine/training.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, **kwargs)\u001b[0m\n\u001b[1;32m   1428\u001b[0m                               \u001b[0mval_f\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mval_f\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mval_ins\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mval_ins\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mshuffle\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mshuffle\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1429\u001b[0m                               \u001b[0mcallback_metrics\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcallback_metrics\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1430\u001b[0;31m                               initial_epoch=initial_epoch)\n\u001b[0m\u001b[1;32m   1431\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1432\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mevaluate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m32\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mverbose\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msample_weight\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/samwit/anaconda3/envs/keras2/lib/python3.5/site-packages/keras/engine/training.py\u001b[0m in \u001b[0;36m_fit_loop\u001b[0;34m(self, f, ins, out_labels, batch_size, epochs, verbose, callbacks, val_f, val_ins, shuffle, callback_metrics, initial_epoch)\u001b[0m\n\u001b[1;32m   1077\u001b[0m                 \u001b[0mbatch_logs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'size'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatch_ids\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1078\u001b[0m                 \u001b[0mcallbacks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mon_batch_begin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatch_index\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_logs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1079\u001b[0;31m                 \u001b[0mouts\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mins_batch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1080\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mouts\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlist\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1081\u001b[0m                     \u001b[0mouts\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mouts\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/samwit/anaconda3/envs/keras2/lib/python3.5/site-packages/keras/backend/tensorflow_backend.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, inputs)\u001b[0m\n\u001b[1;32m   2266\u001b[0m         updated = session.run(self.outputs + [self.updates_op],\n\u001b[1;32m   2267\u001b[0m                               \u001b[0mfeed_dict\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mfeed_dict\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2268\u001b[0;31m                               **self.session_kwargs)\n\u001b[0m\u001b[1;32m   2269\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mupdated\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moutputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2270\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/samwit/anaconda3/envs/keras2/lib/python3.5/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36mrun\u001b[0;34m(self, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m    787\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    788\u001b[0m       result = self._run(None, fetches, feed_dict, options_ptr,\n\u001b[0;32m--> 789\u001b[0;31m                          run_metadata_ptr)\n\u001b[0m\u001b[1;32m    790\u001b[0m       \u001b[0;32mif\u001b[0m \u001b[0mrun_metadata\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    791\u001b[0m         \u001b[0mproto_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf_session\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTF_GetBuffer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrun_metadata_ptr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/samwit/anaconda3/envs/keras2/lib/python3.5/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_run\u001b[0;34m(self, handle, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m    995\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mfinal_fetches\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mfinal_targets\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    996\u001b[0m       results = self._do_run(handle, final_targets, final_fetches,\n\u001b[0;32m--> 997\u001b[0;31m                              feed_dict_string, options, run_metadata)\n\u001b[0m\u001b[1;32m    998\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    999\u001b[0m       \u001b[0mresults\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/samwit/anaconda3/envs/keras2/lib/python3.5/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_do_run\u001b[0;34m(self, handle, target_list, fetch_list, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m   1130\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mhandle\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1131\u001b[0m       return self._do_call(_run_fn, self._session, feed_dict, fetch_list,\n\u001b[0;32m-> 1132\u001b[0;31m                            target_list, options, run_metadata)\n\u001b[0m\u001b[1;32m   1133\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1134\u001b[0m       return self._do_call(_prun_fn, self._session, handle, feed_dict,\n",
      "\u001b[0;32m/home/samwit/anaconda3/envs/keras2/lib/python3.5/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_do_call\u001b[0;34m(self, fn, *args)\u001b[0m\n\u001b[1;32m   1137\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0m_do_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1138\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1139\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1140\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0merrors\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mOpError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1141\u001b[0m       \u001b[0mmessage\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcompat\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mas_text\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmessage\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/samwit/anaconda3/envs/keras2/lib/python3.5/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_run_fn\u001b[0;34m(session, feed_dict, fetch_list, target_list, options, run_metadata)\u001b[0m\n\u001b[1;32m   1119\u001b[0m         return tf_session.TF_Run(session, options,\n\u001b[1;32m   1120\u001b[0m                                  \u001b[0mfeed_dict\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfetch_list\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget_list\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1121\u001b[0;31m                                  status, run_metadata)\n\u001b[0m\u001b[1;32m   1122\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1123\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_prun_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msession\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhandle\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeed_dict\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfetch_list\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "generative_model.fit(X, y, epochs=10, batch_size=64, callbacks=callbacks_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "generative_model.save('Text_gen_01-CharRNN_no_embedding')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "144192/144243 [============================>.] - ETA: 0s - loss: 1.7414Epoch 00000: loss improved from 1.90994 to 1.74151, saving model to checkpoints/weights-improvement-00-1.7415-gentext-CharRNN.hdf5\n",
      "144243/144243 [==============================] - 313s - loss: 1.7415   \n",
      "Epoch 2/10\n",
      "144192/144243 [============================>.] - ETA: 0s - loss: 1.6662Epoch 00001: loss improved from 1.74151 to 1.66620, saving model to checkpoints/weights-improvement-01-1.6662-gentext-CharRNN.hdf5\n",
      "144243/144243 [==============================] - 313s - loss: 1.6662   \n",
      "Epoch 3/10\n",
      "144192/144243 [============================>.] - ETA: 0s - loss: 1.6029Epoch 00002: loss improved from 1.66620 to 1.60287, saving model to checkpoints/weights-improvement-02-1.6029-gentext-CharRNN.hdf5\n",
      "144243/144243 [==============================] - 313s - loss: 1.6029   \n",
      "Epoch 4/10\n",
      "144192/144243 [============================>.] - ETA: 0s - loss: 1.5466Epoch 00003: loss improved from 1.60287 to 1.54662, saving model to checkpoints/weights-improvement-03-1.5466-gentext-CharRNN.hdf5\n",
      "144243/144243 [==============================] - 313s - loss: 1.5466   \n",
      "Epoch 5/10\n",
      "144192/144243 [============================>.] - ETA: 0s - loss: 1.4981Epoch 00004: loss improved from 1.54662 to 1.49814, saving model to checkpoints/weights-improvement-04-1.4981-gentext-CharRNN.hdf5\n",
      "144243/144243 [==============================] - 313s - loss: 1.4981   \n",
      "Epoch 6/10\n",
      "144192/144243 [============================>.] - ETA: 0s - loss: 1.4513Epoch 00005: loss improved from 1.49814 to 1.45128, saving model to checkpoints/weights-improvement-05-1.4513-gentext-CharRNN.hdf5\n",
      "144243/144243 [==============================] - 314s - loss: 1.4513   \n",
      "Epoch 7/10\n",
      "144192/144243 [============================>.] - ETA: 0s - loss: 1.4078Epoch 00006: loss improved from 1.45128 to 1.40787, saving model to checkpoints/weights-improvement-06-1.4079-gentext-CharRNN.hdf5\n",
      "144243/144243 [==============================] - 313s - loss: 1.4079   \n",
      "Epoch 8/10\n",
      "144192/144243 [============================>.] - ETA: 0s - loss: 1.3701Epoch 00007: loss improved from 1.40787 to 1.37014, saving model to checkpoints/weights-improvement-07-1.3701-gentext-CharRNN.hdf5\n",
      "144243/144243 [==============================] - 313s - loss: 1.3701   \n",
      "Epoch 9/10\n",
      "144192/144243 [============================>.] - ETA: 0s - loss: 1.3343Epoch 00008: loss improved from 1.37014 to 1.33446, saving model to checkpoints/weights-improvement-08-1.3345-gentext-CharRNN.hdf5\n",
      "144243/144243 [==============================] - 314s - loss: 1.3345   \n",
      "Epoch 10/10\n",
      "144192/144243 [============================>.] - ETA: 0s - loss: 1.3005Epoch 00009: loss improved from 1.33446 to 1.30045, saving model to checkpoints/weights-improvement-09-1.3005-gentext-CharRNN.hdf5\n",
      "144243/144243 [==============================] - 313s - loss: 1.3005   \n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x7fdb7d5035c0>"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "generative_model.fit(X, y, epochs=10, batch_size=64, callbacks=callbacks_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "generative_model.save('Text_gen_01-CharRNN_no_embedding')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#generative_model = load_model('Text_gen_01_no_embedding')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total Characters:  144343\n",
      "Total Vocab:  44\n"
     ]
    }
   ],
   "source": [
    "# create mapping of unique chars to integers, and a reverse mapping\n",
    "chars = sorted(list(set(raw_text)))\n",
    "char_to_int = dict((c, i) for i, c in enumerate(chars))\n",
    "int_to_char = dict((i, c) for i, c in enumerate(chars))\n",
    "\n",
    "# summarize the loaded data\n",
    "n_chars = len(raw_text)\n",
    "n_vocab = len(chars)\n",
    "print(\"Total Characters: \", n_chars)\n",
    "print(\"Total Vocab: \", n_vocab)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[8, 1, 36, 24, 17, 36, 1, 29, 17, 27, 21, 35, 1, 36, 24, 21, 1, 39, 31, 34, 28, 20, 1, 23, 31, 1, 34, 31, 37, 30, 20, 2, 3, 4, 0, 0, 4, 35, 31, 29, 21, 18, 31, 20, 41, 1, 35, 17, 25, 20, 8, 4, 1, 17, 28, 25, 19, 21, 1, 39, 24, 25, 35, 32, 21, 34, 21, 20, 8, 1, 4, 36, 24, 17, 36, 1, 25, 36, 4, 35, 1, 20, 31, 30, 21, 1, 18, 41, 1, 21, 38, 21, 34, 41, 18, 31, 20, 41, 1, 29]\n",
      "Seed pattern:\n",
      "\" , that makes the world go round!\"'\n",
      "\n",
      "'somebody said,' alice whispered, 'that it's done by everybody m \"\n"
     ]
    }
   ],
   "source": [
    "# pick a random seed\n",
    "start = np.random.randint(0, len(dataX)-1)\n",
    "pattern = dataX[start]\n",
    "seed = dataX[start]\n",
    "print(pattern)\n",
    "print(\"Seed pattern:\")\n",
    "print(\"\\\"\", ''.join([int_to_char[value] for value in pattern]), \"\\\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Done.\n"
     ]
    }
   ],
   "source": [
    "generated_text = []\n",
    "\n",
    "# generate characters\n",
    "for i in range(100):\n",
    "    x = np.reshape(pattern, (1, len(pattern), 1))\n",
    "    x = x / float(n_vocab)\n",
    "    prediction = generative_model.predict(x, verbose=0)\n",
    "    index = np.argmax(prediction)\n",
    "    result = int_to_char[index]\n",
    "    seq_in = [int_to_char[value] for value in pattern]\n",
    "    pattern.append(index)\n",
    "    generated_text.append(index)\n",
    "    pattern = pattern[1:len(pattern)]\n",
    "print(\"\\nDone.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[25, 30, 21, 1, 17, 18, 31, 37, 36, 1, 36, 24, 21, 1, 39, 24, 25, 36, 21, 1, 23, 17, 20, 1, 17, 1, 18, 31, 30, 27, 1, 36, 34, 1, 36, 24, 21, 1, 39, 31, 34, 20, 35, 1, 4, 1, 0, 4, 25, 1, 20, 31, 30, 4, 36, 1, 27, 30, 31, 39, 1, 36, 24, 21, 1, 39, 24, 25, 36, 21, 1, 34, 37, 21, 21, 34, 1, 35, 24, 21, 1, 39, 17, 41, 1, 25, 36, 1, 25, 17, 20, 1, 36, 31, 1, 35, 21, 21, 1, 36]\n",
      "\" , that makes the world go round!\"'\n",
      "\n",
      "'somebody said,' alice whispered, 'that it's done by everybody mi \"\n",
      "\" ine about the white gad a bonk tr the words ' \n",
      "'i don't know the white rueer she way it iad to see t \"\n"
     ]
    }
   ],
   "source": [
    "print(pattern)\n",
    "print(\"\\\"\", ''.join([int_to_char[value] for value in seed]), \"\\\"\")\n",
    "print(\"\\\"\", ''.join([int_to_char[value] for value in generated_text]), \"\\\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[24, 21, 1, 39, 17, 41, 1, 25, 30, 36, 31, 1, 36, 24, 21, 1, 21, 17, 30, 19, 21, 13, 0, 1, 1, 39, 31, 37, 28, 20, 1, 41, 31, 37, 1, 36, 21, 28, 28, 1, 29, 21, 1, 41, 31, 37, 1, 23, 17, 20, 1, 36, 31, 1, 35, 21, 21, 1, 36, 24, 21, 1, 39, 17, 41, 1, 25, 30, 1, 36, 24, 21, 1, 39, 17, 41, 1, 25, 30, 1, 36, 24, 21, 1, 39, 17, 41, 1, 25, 30, 1, 36, 24, 21, 1, 39, 17, 41, 1, 25]\n",
      "\" , that makes the world go round!\"'\n",
      "\n",
      "'somebody said,' alice whispered, 'that it's done by everybody mi \"\n",
      "\" he way into the eance?\n",
      "  would you tell me you gad to see the way in the way in the way in the way i \"\n"
     ]
    }
   ],
   "source": [
    "generated_text = []\n",
    "\n",
    "# generate characters\n",
    "for i in range(100):\n",
    "    x = np.reshape(pattern, (1, len(pattern), 1))\n",
    "    x = x / float(n_vocab)\n",
    "    prediction = generative_model.predict(x, verbose=0)\n",
    "    index = np.argmax(prediction)\n",
    "    result = int_to_char[index]\n",
    "    seq_in = [int_to_char[value] for value in pattern]\n",
    "    pattern.append(index)\n",
    "    generated_text.append(index)\n",
    "    pattern = pattern[1:len(pattern)]\n",
    "print(pattern)\n",
    "print(\"\\\"\", ''.join([int_to_char[value] for value in seed]), \"\\\"\")\n",
    "print(\"\\\"\", ''.join([int_to_char[value] for value in generated_text]), \"\\\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1, 18, 21, 1, 17, 1, 32, 21, 36, 19, 34, 1, 24, 34, 31, 39, 1, 36, 24, 21, 1, 39, 24, 25, 36, 21, 1, 31, 17, 20, 21, 1, 31, 22, 1, 36, 24, 21, 1, 35, 31, 31, 28, 8, 1, 17, 30, 20, 1, 36, 24, 21, 1, 29, 31, 19, 35, 36, 21, 34, 35, 1, 25, 1, 24, 17, 38, 21, 1, 20, 31, 30, 21, 1, 36, 31, 1, 36, 24, 21, 1, 39, 31, 34, 20, 35, 1, 24, 31, 36, 1, 36, 31, 1, 36, 24, 21, 1, 39, 31]\n",
      "\" , that makes the world go round!\"'\n",
      "\n",
      "'somebody said,' alice whispered, 'that it's done by everybody mi \"\n",
      "\" n the way in the way of starpelte that she had to sto the white rabbit heard the white rabbit, who was she white rabbit reterely at the white rabbit, whth a soor lany and frownen to the words the whole party things at the way of say that she was not and seneating the white rabbit, who was she white rabbit reterpadly, 'i should think you con't know the white giddres,' she said to herself, 'it would be a petcr hrow the white oade of the sool, and the mocsters i have done to the words hot to the wo \"\n"
     ]
    }
   ],
   "source": [
    "generated_text = []\n",
    "\n",
    "# generate characters\n",
    "for i in range(500):\n",
    "    x = np.reshape(pattern, (1, len(pattern), 1))\n",
    "    x = x / float(n_vocab)\n",
    "    prediction = generative_model.predict(x, verbose=0)\n",
    "    index = np.argmax(prediction)\n",
    "    result = int_to_char[index]\n",
    "    seq_in = [int_to_char[value] for value in pattern]\n",
    "    pattern.append(index)\n",
    "    generated_text.append(index)\n",
    "    pattern = pattern[1:len(pattern)]\n",
    "print(pattern)\n",
    "print(\"\\\"\", ''.join([int_to_char[value] for value in seed]), \"\\\"\")\n",
    "print(\"\\\"\", ''.join([int_to_char[value] for value in generated_text]), \"\\\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
