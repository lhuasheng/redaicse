{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Addition\n",
    "\n",
    "This coursework consist of two parts. The first part includes adding two decimal numbers then generating a binary number output. That did not work so well during test time.\n",
    "\n",
    "The second part is a conventional classifier using two decimal numbers then generating a 201 class y_train output. It takes a really long time to train."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 9 38  0 82 31 59 80 31 13 33]\n",
      "980100\n",
      "[ 0.37  0.55]\n",
      "1\n",
      "(980100, 2)\n",
      "(980100, 8)\n",
      "['loss', 'acc']\n",
      "Epoch 1/5\n",
      "980100/980100 [==============================] - 19s - loss: 0.1943 - acc: 0.8061    \n",
      "Epoch 2/5\n",
      "980100/980100 [==============================] - 16s - loss: 0.1690 - acc: 0.8979    \n",
      "Epoch 3/5\n",
      "980100/980100 [==============================] - 15s - loss: 0.1617 - acc: 0.8834    \n",
      "Epoch 4/5\n",
      "980100/980100 [==============================] - 13s - loss: 0.1570 - acc: 0.8857    \n",
      "Epoch 5/5\n",
      "980100/980100 [==============================] - 14s - loss: 0.1540 - acc: 0.8825    \n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "Error when checking : expected input_1 to have shape (None, 2) but got array with shape (2, 1)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-1-b80adbed5223>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m     69\u001b[0m \u001b[0mK\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mset_value\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0moptimizer\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mlr\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m0.001\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     70\u001b[0m \u001b[0mmodel\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mxtrain\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mytrain\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mbatch_size\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m512\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mepochs\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m5\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mcallbacks\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mreduce_lr\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 71\u001b[1;33m \u001b[0mmodel\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m81\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32mC:\\Users\\HuaSheng\\AppData\\Local\\conda\\conda\\envs\\tfkeras\\lib\\site-packages\\keras\\engine\\training.py\u001b[0m in \u001b[0;36mpredict\u001b[1;34m(self, x, batch_size, verbose)\u001b[0m\n\u001b[0;32m   1574\u001b[0m         x = _standardize_input_data(x, self._feed_input_names,\n\u001b[0;32m   1575\u001b[0m                                     \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_feed_input_shapes\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1576\u001b[1;33m                                     check_batch_axis=False)\n\u001b[0m\u001b[0;32m   1577\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mstateful\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1578\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[0mx\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m>\u001b[0m \u001b[0mbatch_size\u001b[0m \u001b[1;32mand\u001b[0m \u001b[0mx\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m%\u001b[0m \u001b[0mbatch_size\u001b[0m \u001b[1;33m!=\u001b[0m \u001b[1;36m0\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\Users\\HuaSheng\\AppData\\Local\\conda\\conda\\envs\\tfkeras\\lib\\site-packages\\keras\\engine\\training.py\u001b[0m in \u001b[0;36m_standardize_input_data\u001b[1;34m(data, names, shapes, check_batch_axis, exception_prefix)\u001b[0m\n\u001b[0;32m    137\u001b[0m                             \u001b[1;34m' to have shape '\u001b[0m \u001b[1;33m+\u001b[0m \u001b[0mstr\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mshapes\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mi\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m+\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    138\u001b[0m                             \u001b[1;34m' but got array with shape '\u001b[0m \u001b[1;33m+\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 139\u001b[1;33m                             str(array.shape))\n\u001b[0m\u001b[0;32m    140\u001b[0m     \u001b[1;32mreturn\u001b[0m \u001b[0marrays\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    141\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mValueError\u001b[0m: Error when checking : expected input_1 to have shape (None, 2) but got array with shape (2, 1)"
     ]
    }
   ],
   "source": [
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import keras\n",
    "from keras.models import Model, Sequential\n",
    "from keras.layers import *\n",
    "from keras.optimizers import Adam\n",
    "from keras.callbacks import ReduceLROnPlateau\n",
    "\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "\n",
    "def addDS():\n",
    "    total = []\n",
    "    src1 =[]\n",
    "    src2 =[]\n",
    "    #x1 = np.arange(0,101)\n",
    "    #x2 = np.arange(0,101)\n",
    "    x1 = np.random.randint(0,100,size=(1000))\n",
    "    x2 = np.random.randint(0,100,size=(1000))\n",
    "    r = np.where(x1==81)\n",
    "    x1=np.delete(x1,r,axis=0)\n",
    "    x2=np.delete(x2,r,axis=0)\n",
    "    for u  in x1:\n",
    "        for v in x2:\n",
    "            singleInst = [int(d) for d in str(np.binary_repr(u+v,width=8))]\n",
    "            total += [singleInst]\n",
    "            src1 += [u]\n",
    "            src2 += [v]\n",
    "    return total,src1,src2\n",
    "\n",
    "x1 = np.random.randint(0,100,size=(10))\n",
    "print(x1)\n",
    "\n",
    "    \n",
    "\n",
    "total,x1,x2 = addDS()\n",
    "\n",
    "print(len(x1))\n",
    "\n",
    "dataIdx = np.random.permutation(len(total))\n",
    "x1 = np.array(x1)\n",
    "x2 = np.array(x2)\n",
    "total = np.array(total)\n",
    "xtrain = np.hstack((x1[dataIdx,np.newaxis],x2[dataIdx,np.newaxis]))\n",
    "ytrain = total[dataIdx,:]\n",
    "xtrain = np.divide(xtrain,100)\n",
    "\n",
    "print(xtrain[50])\n",
    "print(np.argmax(ytrain[50]))\n",
    "\n",
    "\n",
    "print(xtrain.shape)\n",
    "print(ytrain.shape)\n",
    "\n",
    "inp = Input(shape=(2,))\n",
    "x = Dense(1,activation='relu')(inp)\n",
    "x = Dense(200,activation='relu')(x)\n",
    "x = Dropout(0.5)(x)\n",
    "x = Dense(200,activation='relu')(x)\n",
    "out = Dense(8,activation = 'sigmoid')(x)\n",
    "\n",
    "model = Model(inp,out)\n",
    "model.compile(optimizer='adam',loss='mse',metrics=['acc'])\n",
    "\n",
    "print(model.metrics_names)\n",
    "reduce_lr = ReduceLROnPlateau(monitor='loss', factor=0.2,epsilon=0.01,cooldown=1,\n",
    "              patience=2, min_lr=0.0001)\n",
    "K.set_value(model.optimizer.lr, 0.001)\n",
    "model.fit(xtrain,ytrain,batch_size=512,epochs=5,callbacks=[reduce_lr])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "out = model.predict(np.array([[81,1]]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "13\n"
     ]
    }
   ],
   "source": [
    "def bool2int(x):\n",
    "    y = 0\n",
    "    for i,j in enumerate(x):\n",
    "        y += j<<i\n",
    "    return y\n",
    "\n",
    "print(bool2int(np.round(out[0]).astype(int)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 0.43  0.3 ]\n",
      "73\n"
     ]
    }
   ],
   "source": [
    "#test 2\n",
    "from keras.layers import *\n",
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import keras\n",
    "from keras.models import Model, Sequential\n",
    "from keras.optimizers import Adam\n",
    "from keras.callbacks import ReduceLROnPlateau\n",
    "\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "\n",
    "def addDS():\n",
    "    total = []\n",
    "    src1 =[]\n",
    "    src2 =[]\n",
    "    #x1 = np.arange(0,101)\n",
    "    #x2 = np.arange(0,101)\n",
    "    x1 = np.random.randint(0,100,size=(1000))\n",
    "    x2 = np.random.randint(0,100,size=(1000))\n",
    "    r = np.where(x1==81)\n",
    "    x1=np.delete(x1,r,axis=0)\n",
    "    x2=np.delete(x2,r,axis=0)\n",
    "    for u  in x1:\n",
    "        for v in x2:\n",
    "            total += [u+v]\n",
    "            src1 += [u]\n",
    "            src2 += [v]\n",
    "    return total,src1,src2\n",
    "total,x1,x2 = addDS()\n",
    "\n",
    "dataIdx = np.random.permutation(len(total))\n",
    "x1 = np.array(x1)\n",
    "x2 = np.array(x2)\n",
    "total = np.array(total)\n",
    "xtrain = np.hstack((x1[dataIdx,np.newaxis],x2[dataIdx,np.newaxis]))\n",
    "ytrain = total[dataIdx]\n",
    "xtrain = np.divide(xtrain,100)\n",
    "\n",
    "print(xtrain[50])\n",
    "print(ytrain[50])\n",
    "ytrain = keras.utils.to_categorical(ytrain,201)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['loss', 'acc']\n",
      "Epoch 1/5\n",
      "978121/978121 [==============================] - 207s - loss: 2.6435 - acc: 0.1288   \n",
      "Epoch 2/5\n",
      "978121/978121 [==============================] - 201s - loss: 2.3515 - acc: 0.1606   \n",
      "Epoch 3/5\n",
      "978121/978121 [==============================] - 204s - loss: 2.2419 - acc: 0.1778   \n",
      "Epoch 4/5\n",
      "978121/978121 [==============================] - 205s - loss: 2.1385 - acc: 0.1989   \n",
      "Epoch 5/5\n",
      "978121/978121 [==============================] - 205s - loss: 2.0379 - acc: 0.2189   \n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x1760a0ea550>"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def denseBNActDr(hu,x):\n",
    "    x = Dense(hu)(x)\n",
    "    x = BatchNormalization()(x)\n",
    "    x = Activation(activation='relu')(x)\n",
    "    x = Dropout(0.1)(x)\n",
    "    return x\n",
    "\n",
    "inp = Input(shape=(2,))\n",
    "x = Dense(16)(inp)\n",
    "x = Activation(activation='relu')(x)\n",
    "x = Dropout(0.1)(x)\n",
    "x = denseBNActDr(256,x)\n",
    "x = denseBNActDr(1024,x)\n",
    "x = denseBNActDr(2048,x)\n",
    "out = Dense(201,activation = 'softmax',name='out')(x)\n",
    "\n",
    "model = Model(inp,[out])\n",
    "model.compile(optimizer='adam',loss='categorical_crossentropy',metrics=['acc'])\n",
    "\n",
    "print(model.metrics_names)\n",
    "reduce_lr = ReduceLROnPlateau(monitor='loss', factor=0.2,epsilon=0.01,cooldown=1,\n",
    "              patience=2, min_lr=0.0001)\n",
    "y_train = {'out': ytrain}\n",
    "model.fit(xtrain,y_train,batch_size=512,epochs=5,callbacks=[])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/5\n",
      "978121/978121 [==============================] - 205s - loss: 1.9300 - acc: 0.2451   \n",
      "Epoch 2/5\n",
      "978121/978121 [==============================] - 204s - loss: 1.8498 - acc: 0.2671   \n",
      "Epoch 3/5\n",
      "978121/978121 [==============================] - 205s - loss: 1.7695 - acc: 0.2894   \n",
      "Epoch 4/5\n",
      "978121/978121 [==============================] - 206s - loss: 1.6807 - acc: 0.3188   \n",
      "Epoch 5/5\n",
      "978121/978121 [==============================] - 204s - loss: 1.6126 - acc: 0.3405   \n",
      "195\n"
     ]
    }
   ],
   "source": [
    "model.fit(xtrain,ytrain,batch_size=512,epochs=5,callbacks=[],shuffle=True)\n",
    "out = model.predict(np.array([[81,1]]))\n",
    "print(np.argmax(out[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
