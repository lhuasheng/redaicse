{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.0.5\n"
     ]
    }
   ],
   "source": [
    "import keras\n",
    "from __future__ import print_function\n",
    "from functools import reduce\n",
    "import re\n",
    "import tarfile\n",
    "import numpy as np\n",
    "from keras.utils.data_utils import get_file\n",
    "from keras.utils import to_categorical\n",
    "from keras.layers.embeddings import Embedding\n",
    "from keras import layers\n",
    "from keras.layers import *\n",
    "from keras.models import Model\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "from keras.callbacks import ReduceLROnPlateau\n",
    "print(keras.__version__)\n",
    "\n",
    "def tokenize(sent):\n",
    "    '''Return the tokens of a sentence including punctuation.\n",
    "    >>> tokenize('Bob dropped the apple. Where is the apple?')\n",
    "    ['Bob', 'dropped', 'the', 'apple', '.', 'Where', 'is', 'the', 'apple', '?']\n",
    "    '''\n",
    "    return [x.strip() for x in re.split('(\\W+)?', sent) if x.strip()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RNN / Embed / Sent / Query = <class 'keras.layers.recurrent.LSTM'>, 50, 100, 100\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\HuaSheng\\AppData\\Local\\conda\\conda\\envs\\tfKrNv\\lib\\re.py:212: FutureWarning: split() requires a non-empty pattern match.\n",
      "  return _compile(pattern, flags).split(string, maxsplit)\n"
     ]
    }
   ],
   "source": [
    "def parse_stories(lines, only_supporting=False):\n",
    "    '''Parse stories provided in the bAbi tasks format\n",
    "    If only_supporting is true,\n",
    "    only the sentences that support the answer are kept.\n",
    "    '''\n",
    "    data = []\n",
    "    story = []\n",
    "    for line in lines:\n",
    "        #print(line)\n",
    "        #line = line.decode('utf-8').strip()\n",
    "        nid, line = line.split(' ', 1)\n",
    "        #print([nid,line])\n",
    "        nid = int(nid)\n",
    "        if nid == 1:\n",
    "            story = []\n",
    "        if '\\t' in line:\n",
    "            q, a, supporting = line.split('\\t')\n",
    "            q = tokenize(q)\n",
    "            substory = None\n",
    "            if only_supporting:\n",
    "                # Only select the related substory\n",
    "                supporting = map(int, supporting.split())\n",
    "                substory = [story[i - 1] for i in supporting]\n",
    "            else:\n",
    "                # Provide all the substories\n",
    "                substory = [ x for x in story if x]\n",
    "            data.append((substory, q, a))\n",
    "            story.append('')\n",
    "        else:\n",
    "            sent = tokenize(line)\n",
    "            story.append(sent)\n",
    "    return data\n",
    "\n",
    "\n",
    "def get_stories(f, only_supporting=False, max_length=None):\n",
    "    '''Given a file name, read the file, retrieve the stories,\n",
    "    and then convert the sentences into a single story.\n",
    "    If max_length is supplied,\n",
    "    any stories longer than max_length tokens will be discarded.\n",
    "    '''\n",
    "    data = parse_stories(f.readlines(), only_supporting=only_supporting)\n",
    "    #print(data[0])\n",
    "    flatten = lambda data: reduce(lambda x, y: x + y, data)\n",
    "    data = [(flatten(story), q, answer) for story, q, answer in data if not max_length or len(flatten(story)) < max_length]\n",
    "    #print(data)\n",
    "    return data\n",
    "\n",
    "\n",
    "def vectorize_stories(data, word_idx, story_maxlen, query_maxlen):\n",
    "    xs = []\n",
    "    xqsi = []\n",
    "    xqso = []\n",
    "    ys = []\n",
    "    for story, query, answer in data:\n",
    "        x = [word_idx[w] for w in story]\n",
    "        x += [word_idx['\\n']]\n",
    "        xqi = [word_idx['\\t']]\n",
    "        xqi += [word_idx[w] for w in query]\n",
    "        xqo = xqi +[word_idx['\\n']]\n",
    "        y=word_idx[answer]\n",
    "        xs.append(x)\n",
    "        xqsi.append(xqi)\n",
    "        xqso.append(xqo)\n",
    "        ys.append(y)\n",
    "    return pad_sequences(xs, maxlen=story_maxlen,padding='post'), pad_sequences(xqsi, maxlen=query_maxlen,padding='post'),pad_sequences(xqso, maxlen=query_maxlen), np.array(ys)\n",
    "\n",
    "RNN = recurrent.LSTM\n",
    "EMBED_HIDDEN_SIZE = 50\n",
    "SENT_HIDDEN_SIZE = 100\n",
    "QUERY_HIDDEN_SIZE = 100\n",
    "BATCH_SIZE = 32\n",
    "EPOCHS = 40\n",
    "print('RNN / Embed / Sent / Query = {}, {}, {}, {}'.format(RNN,\n",
    "                                                           EMBED_HIDDEN_SIZE,\n",
    "                                                           SENT_HIDDEN_SIZE,\n",
    "                                                           QUERY_HIDDEN_SIZE))\n",
    "trainFile = open('./qa5_three-arg-relations_train.txt','r')\n",
    "#print(trainFile.readlines())\n",
    "train = get_stories(trainFile)\n",
    "test = get_stories(open('./qa5_three-arg-relations_test.txt'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(['Fred',\n",
       "  'picked',\n",
       "  'up',\n",
       "  'the',\n",
       "  'football',\n",
       "  'there',\n",
       "  '.',\n",
       "  'Fred',\n",
       "  'gave',\n",
       "  'the',\n",
       "  'football',\n",
       "  'to',\n",
       "  'Jeff',\n",
       "  '.'],\n",
       " ['What', 'did', 'Fred', 'give', 'to', 'Jeff', '?'],\n",
       " 'football')"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "vocab = set()\n",
    "vocab |=set(('\\t','\\n'))\n",
    "for story, q, answer in train + test:\n",
    "    vocab |= set(story + q + [answer])\n",
    "vocab = sorted(vocab)\n",
    "\n",
    "# Reserve 0 for masking via pad_sequences\n",
    "vocab_size = len(vocab) + 1\n",
    "word_idx = dict((c, i + 1) for i, c in enumerate(vocab))\n",
    "inv_map = {v: k for k, v in word_idx.items()}\n",
    "story_maxlen = max(map(len, (x for x, _, _ in train + test)))\n",
    "query_maxlen = max(map(len, (x for _, x, _ in train + test)))\n",
    "\n",
    "x, xqi,xqo, y = vectorize_stories(train, word_idx, story_maxlen, query_maxlen)\n",
    "tx, txqi,txqo, ty = vectorize_stories(test, word_idx, story_maxlen, query_maxlen)\n",
    "temp = np.zeros((x.shape[0], query_maxlen, vocab_size),dtype='float32')\n",
    "\n",
    "def token2OHE(xqo):\n",
    "    for i,idx in enumerate(xqo):\n",
    "        for u,v in enumerate(idx):\n",
    "            temp [i,u, v] = 1.\n",
    "    return temp\n",
    "\n",
    "xqo=token2OHE(xqo)\n",
    "txqo=token2OHE(txqo)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Bill', 'travelled', 'to', 'the', 'office', '.', 'Bill', 'picked', 'up', 'the', 'football', 'there', '.', 'Bill', 'went', 'to', 'the', 'bedroom', '.', 'Bill', 'gave', 'the', 'football', 'to', 'Fred', '.', 'Fred', 'handed', 'the', 'football', 'to', 'Bill', '.', 'Jeff', 'went', 'back', 'to', 'the', 'office', '.', '\\n']\n",
      "['\\t', 'Who', 'received', 'the', 'football', '?']\n",
      "Bill\n"
     ]
    }
   ],
   "source": [
    "print([inv_map[i] for i in x[1] if i !=0])\n",
    "print([inv_map[i] for i in xqi[1] if i !=0])\n",
    "print(inv_map[y[1]] )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "vocab = ['\\t', '\\n', '.', '?', 'Bill', 'Fred', 'Jeff', 'Mary', 'What', 'Who', 'apple', 'back', 'bathroom', 'bedroom', 'did', 'discarded', 'down', 'dropped', 'football', 'garden', 'gave', 'give', 'got', 'grabbed', 'hallway', 'handed', 'journeyed', 'kitchen', 'left', 'milk', 'moved', 'office', 'passed', 'picked', 'put', 'received', 'the', 'there', 'to', 'took', 'travelled', 'up', 'went']\n",
      "x.shape = (1000, 627)\n",
      "xq.shape = (1000, 8)\n",
      "xq.shape = (1000, 8, 44)\n",
      "y.shape = (1000,)\n",
      "story_maxlen, query_maxlen = 627, 8\n",
      "Tensor(\"input_4:0\", shape=(?, 627), dtype=float32)\n",
      "Tensor(\"embedding_5/Gather:0\", shape=(?, 627, 80), dtype=float32)\n",
      "Tensor(\"LSTM_01_2/transpose_1:0\", shape=(?, ?, 50), dtype=float32)\n",
      "Tensor(\"time_distributed_2/Reshape_1:0\", shape=(?, 8, 44), dtype=float32)\n",
      "____________________________________________________________________________________________________\n",
      "Layer (type)                     Output Shape          Param #     Connected to                     \n",
      "====================================================================================================\n",
      "input_4 (InputLayer)             (None, 627)           0                                            \n",
      "____________________________________________________________________________________________________\n",
      "decoder_inputs (InputLayer)      (None, 8)             0                                            \n",
      "____________________________________________________________________________________________________\n",
      "embedding_5 (Embedding)          (None, 627, 80)       3520        input_4[0][0]                    \n",
      "____________________________________________________________________________________________________\n",
      "embedding_6 (Embedding)          (None, 8, 80)         3520        decoder_inputs[0][0]             \n",
      "____________________________________________________________________________________________________\n",
      "lstm_2 (LSTM)                    [(None, 50), (None, 5 26200       embedding_5[0][0]                \n",
      "____________________________________________________________________________________________________\n",
      "LSTM_01 (LSTM)                   [(None, 8, 50), (None 26200       embedding_6[0][0]                \n",
      "                                                                   lstm_2[0][1]                     \n",
      "                                                                   lstm_2[0][2]                     \n",
      "____________________________________________________________________________________________________\n",
      "time_distributed_2 (TimeDistribu (None, 8, 44)         2244        LSTM_01[0][0]                    \n",
      "====================================================================================================\n",
      "Total params: 61,684\n",
      "Trainable params: 61,684\n",
      "Non-trainable params: 0\n",
      "____________________________________________________________________________________________________\n",
      "Train on 800 samples, validate on 200 samples\n",
      "Epoch 1/50\n",
      "800/800 [==============================] - 14s - loss: 5.7835 - acc: 0.1048 - val_loss: 5.1098 - val_acc: 0.0869\n",
      "Epoch 2/50\n",
      "800/800 [==============================] - 9s - loss: 4.7108 - acc: 0.1200 - val_loss: 4.3927 - val_acc: 0.2119\n",
      "Epoch 3/50\n",
      "800/800 [==============================] - 9s - loss: 4.1863 - acc: 0.2466 - val_loss: 4.0377 - val_acc: 0.2988\n",
      "Epoch 4/50\n",
      "800/800 [==============================] - 8s - loss: 3.8922 - acc: 0.2783 - val_loss: 3.7932 - val_acc: 0.2119\n",
      "Epoch 5/50\n",
      "800/800 [==============================] - 8s - loss: 3.6637 - acc: 0.2900 - val_loss: 3.6008 - val_acc: 0.3794\n",
      "Epoch 6/50\n",
      "800/800 [==============================] - 8s - loss: 3.5002 - acc: 0.3023 - val_loss: 3.4812 - val_acc: 0.1963\n",
      "Epoch 7/50\n",
      "800/800 [==============================] - 8s - loss: 3.3608 - acc: 0.3153 - val_loss: 3.3461 - val_acc: 0.4112\n",
      "Epoch 8/50\n",
      "800/800 [==============================] - 8s - loss: 3.2604 - acc: 0.3438 - val_loss: 3.2261 - val_acc: 0.2862\n",
      "Epoch 9/50\n",
      "800/800 [==============================] - 8s - loss: 3.1768 - acc: 0.3439 - val_loss: 3.1686 - val_acc: 0.4113\n",
      "Epoch 10/50\n",
      "800/800 [==============================] - 8s - loss: 3.0873 - acc: 0.3625 - val_loss: 3.1878 - val_acc: 0.2613\n",
      "Epoch 11/50\n",
      "800/800 [==============================] - 9s - loss: 3.0308 - acc: 0.3570 - val_loss: 3.0456 - val_acc: 0.4113\n",
      "Epoch 12/50\n",
      "800/800 [==============================] - 8s - loss: 3.0075 - acc: 0.3583 - val_loss: 2.9791 - val_acc: 0.3244\n",
      "Epoch 13/50\n",
      "800/800 [==============================] - 8s - loss: 2.8994 - acc: 0.3886 - val_loss: 3.0101 - val_acc: 0.4150\n",
      "Epoch 14/50\n",
      "800/800 [==============================] - 10s - loss: 2.9166 - acc: 0.3723 - val_loss: 2.8768 - val_acc: 0.3563\n",
      "Epoch 15/50\n",
      "800/800 [==============================] - 11s - loss: 2.8068 - acc: 0.4133 - val_loss: 2.8605 - val_acc: 0.4512\n",
      "Epoch 16/50\n",
      "800/800 [==============================] - 13s - loss: 2.8311 - acc: 0.4069 - val_loss: 2.7938 - val_acc: 0.3925\n",
      "Epoch 17/50\n",
      "800/800 [==============================] - 16s - loss: 2.7143 - acc: 0.4480 - val_loss: 2.7461 - val_acc: 0.4513\n",
      "Epoch 18/50\n",
      "800/800 [==============================] - 10s - loss: 2.6779 - acc: 0.4598 - val_loss: 3.0443 - val_acc: 0.3044\n",
      "Epoch 19/50\n",
      "800/800 [==============================] - 10s - loss: 2.7174 - acc: 0.4245 - val_loss: 2.6757 - val_acc: 0.5125\n",
      "Epoch 20/50\n",
      "800/800 [==============================] - 10s - loss: 2.5937 - acc: 0.4659 - val_loss: 2.6486 - val_acc: 0.4231\n",
      "Epoch 21/50\n",
      "800/800 [==============================] - 10s - loss: 2.6081 - acc: 0.4500 - val_loss: 2.6540 - val_acc: 0.4513\n",
      "Epoch 22/50\n",
      "800/800 [==============================] - 10s - loss: 2.5317 - acc: 0.4528 - val_loss: 2.5752 - val_acc: 0.4312\n",
      "Epoch 23/50\n",
      "800/800 [==============================] - 9s - loss: 2.4880 - acc: 0.4509 - val_loss: 2.5525 - val_acc: 0.4756\n",
      "Epoch 24/50\n",
      "800/800 [==============================] - 13s - loss: 2.5328 - acc: 0.4709 - val_loss: 2.5123 - val_acc: 0.5044\n",
      "Epoch 25/50\n",
      "800/800 [==============================] - 11s - loss: 2.4244 - acc: 0.4877 - val_loss: 2.4820 - val_acc: 0.4900\n",
      "Epoch 26/50\n",
      "800/800 [==============================] - 11s - loss: 2.3910 - acc: 0.5025 - val_loss: 2.4544 - val_acc: 0.4513\n",
      "Epoch 27/50\n",
      "800/800 [==============================] - 9s - loss: 2.4300 - acc: 0.4625 - val_loss: 2.4429 - val_acc: 0.4825\n",
      "Epoch 28/50\n",
      "800/800 [==============================] - 10s - loss: 2.3323 - acc: 0.5206 - val_loss: 2.4009 - val_acc: 0.5000\n",
      "Epoch 29/50\n",
      "800/800 [==============================] - 9s - loss: 2.3030 - acc: 0.5216 - val_loss: 2.3781 - val_acc: 0.5375\n",
      "Epoch 30/50\n",
      "800/800 [==============================] - 9s - loss: 2.2781 - acc: 0.5366 - val_loss: 2.3786 - val_acc: 0.4800\n",
      "Epoch 31/50\n",
      "800/800 [==============================] - 10s - loss: 2.2992 - acc: 0.5053 - val_loss: 2.3377 - val_acc: 0.4925\n",
      "Epoch 32/50\n",
      "800/800 [==============================] - 10s - loss: 2.2277 - acc: 0.5225 - val_loss: 2.3079 - val_acc: 0.5375\n",
      "Epoch 33/50\n",
      "800/800 [==============================] - 10s - loss: 2.2068 - acc: 0.5377 - val_loss: 2.2891 - val_acc: 0.5350\n",
      "Epoch 34/50\n",
      "800/800 [==============================] - 10s - loss: 2.2261 - acc: 0.5525 - val_loss: 2.2839 - val_acc: 0.5213\n",
      "Epoch 35/50\n",
      "800/800 [==============================] - 12s - loss: 2.1694 - acc: 0.5581 - val_loss: 2.2618 - val_acc: 0.5325\n",
      "Epoch 36/50\n",
      "800/800 [==============================] - 16s - loss: 2.1515 - acc: 0.5411 - val_loss: 2.2413 - val_acc: 0.5600\n",
      "Epoch 37/50\n",
      "800/800 [==============================] - 16s - loss: 2.1420 - acc: 0.5591 - val_loss: 2.2476 - val_acc: 0.5250\n",
      "Epoch 38/50\n",
      "800/800 [==============================] - 19s - loss: 2.1265 - acc: 0.5559 - val_loss: 2.2180 - val_acc: 0.5794\n",
      "Epoch 39/50\n",
      "800/800 [==============================] - 12s - loss: 2.1397 - acc: 0.5689 - val_loss: 2.2028 - val_acc: 0.5781\n",
      "Epoch 40/50\n",
      "800/800 [==============================] - 15s - loss: 2.0967 - acc: 0.5883 - val_loss: 2.1947 - val_acc: 0.5794\n",
      "Epoch 41/50\n",
      "800/800 [==============================] - 17s - loss: 2.1027 - acc: 0.5781 - val_loss: 2.1884 - val_acc: 0.5525\n",
      "Epoch 42/50\n",
      "800/800 [==============================] - 15s - loss: 2.0765 - acc: 0.5848 - val_loss: 2.1848 - val_acc: 0.5788\n",
      "Epoch 43/50\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "800/800 [==============================] - 14s - loss: 2.0854 - acc: 0.5805 - val_loss: 2.1885 - val_acc: 0.5450\n",
      "Epoch 44/50\n",
      "800/800 [==============================] - 15s - loss: 2.0707 - acc: 0.5777 - val_loss: 2.2226 - val_acc: 0.5519\n",
      "Epoch 45/50\n",
      "800/800 [==============================] - 18s - loss: 2.0628 - acc: 0.5712 - val_loss: 2.1677 - val_acc: 0.5575\n",
      "Epoch 46/50\n",
      "800/800 [==============================] - 20s - loss: 2.0464 - acc: 0.5739 - val_loss: 2.1576 - val_acc: 0.5488\n",
      "Epoch 47/50\n",
      "800/800 [==============================] - 19s - loss: 2.0400 - acc: 0.5816 - val_loss: 2.1526 - val_acc: 0.5531\n",
      "Epoch 48/50\n",
      "800/800 [==============================] - 19s - loss: 2.0333 - acc: 0.5691 - val_loss: 2.1689 - val_acc: 0.5556\n",
      "Epoch 49/50\n",
      "800/800 [==============================] - 18s - loss: 2.0620 - acc: 0.5870 - val_loss: 2.1310 - val_acc: 0.5575\n",
      "Epoch 50/50\n",
      "800/800 [==============================] - 16s - loss: 2.0198 - acc: 0.5705 - val_loss: 2.1165 - val_acc: 0.5775\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x18eaea4cd68>"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print('vocab = {}'.format(vocab))\n",
    "print('x.shape = {}'.format(x.shape))\n",
    "print('xq.shape = {}'.format(xqi.shape))\n",
    "print('xq.shape = {}'.format(xqo.shape))\n",
    "print('y.shape = {}'.format(y.shape))\n",
    "print('story_maxlen, query_maxlen = {}, {}'.format(story_maxlen, query_maxlen))\n",
    "\n",
    "embeddingLayer =  Embedding(vocab_size, 80)\n",
    "\n",
    "encoder_inputs = Input(shape=( story_maxlen ,)) \n",
    "print(encoder_inputs)\n",
    "eInp = Embedding(vocab_size, 80)(encoder_inputs)\n",
    "print(eInp)\n",
    "encoder = LSTM(50,return_state=True)\n",
    "encoder_outputs, state_h, state_c = encoder(eInp)\n",
    "encoder_states = [state_h, state_c]\n",
    "\n",
    "decoder_inputs = Input(shape=(query_maxlen,),name='decoder_inputs')\n",
    "dInp = Embedding(vocab_size, 80)(decoder_inputs)\n",
    "decoder_lstm = LSTM(50, return_sequences=True, return_state=True,name='LSTM_01') \n",
    "decoder_outputs, _, _ = decoder_lstm(dInp , initial_state=encoder_states)\n",
    "print(decoder_outputs)\n",
    "decoder_dense = TimeDistributed(Dense(vocab_size, activation='softmax'))\n",
    "decoder_outputs = decoder_dense(decoder_outputs)\n",
    "print(decoder_outputs)\n",
    "model = Model([encoder_inputs, decoder_inputs], decoder_outputs)\n",
    "model.summary()\n",
    "model.compile(optimizer='rmsprop', loss='categorical_crossentropy',metrics=['accuracy'])\n",
    "reduce_LR = ReduceLROnPlateau(monitor='val_loss',factor = 0.9, patience=3,cooldown=2, min_lr = 0.00001)\n",
    "model.fit([x, xqi], xqo,\n",
    "          batch_size=128,\n",
    "          epochs=50,\n",
    "          callbacks=[reduce_LR],\n",
    "          validation_split=0.2)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 800 samples, validate on 200 samples\n",
      "Epoch 1/1\n",
      "800/800 [==============================] - 25s - loss: 2.0213 - acc: 0.5894 - val_loss: 2.1103 - val_acc: 0.5550\n",
      "1000/1000 [==============================] - 5s     \n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[2.0374812154769897, 0.56299998545646668]"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.fit([x, xqi], xqo,\n",
    "          batch_size=49,\n",
    "          epochs=1,\n",
    "          callbacks=[reduce_LR],\n",
    "          validation_split=0.2)\n",
    "model.evaluate([tx, txqi], txqo,\n",
    "          batch_size=49)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "encoder_model = Model(encoder_inputs, encoder_states)\n",
    "decoder_state_input_h = Input(shape=(50,))\n",
    "decoder_state_input_c = Input(shape=(50,))\n",
    "decoder_states_inputs = [decoder_state_input_h, decoder_state_input_c]\n",
    "decoder_outputs, state_h, state_c = decoder_lstm(dInp, initial_state=decoder_states_inputs)\n",
    "decoder_states = [state_h, state_c]\n",
    "decoder_outputs = decoder_dense(decoder_outputs)\n",
    "decoder_model = Model(\n",
    "    [decoder_inputs] + decoder_states_inputs,\n",
    "    [decoder_outputs] + decoder_states)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1, 627)\n",
      "(1, 8)\n",
      "0\n",
      "What\n",
      "1\n",
      "\n",
      "\n",
      "7\n"
     ]
    }
   ],
   "source": [
    "def decode_sequence(input_seq):\n",
    "    # Encode the input as state vectors.\n",
    "    states_value = encoder_model.predict(input_seq)\n",
    "\n",
    "    # Generate empty target sequence of length 1.\n",
    "    target_seq = np.zeros(( 1, query_maxlen))\n",
    "    \n",
    "    # Create and pass in the firsat token to start the predictions when combined with the decoeder state.\n",
    "    target_seq[ 0, 0] = word_idx['\\t']\n",
    "    print(target_seq.shape)\n",
    "    # Sampling loop for a batch of sequences\n",
    "    # (to simplify, here we assume a batch of size 1).\n",
    "    stop_condition = False #our stop condition will be '/n' which acts as <EOS>\n",
    "    decoded_sentence = ''\n",
    "    ct =0\n",
    "    while not stop_condition:\n",
    "        \n",
    "        print(ct)\n",
    "        output_tokens, h, c = decoder_model.predict([target_seq] + states_value)\n",
    "        #print(output_tokens)\n",
    "        # Sample a token\n",
    "        sampled_token_index = np.argmax(output_tokens[0,ct, :])\n",
    "        sampled_char = inv_map[sampled_token_index]\n",
    "        print(sampled_char)\n",
    "        decoded_sentence += \"-\" + sampled_char\n",
    "\n",
    "        # Exit condition: either hit max length\n",
    "        # or find stop character.\n",
    "        if (sampled_char == '\\n' or  len(decoded_sentence) > query_maxlen):\n",
    "            stop_condition = True\n",
    "            print( len(decoded_sentence))\n",
    "        ct+=1\n",
    "        # Update the target sequence (of length 1).\n",
    "        target_seq[0, ct] =sampled_token_index\n",
    "\n",
    "        # Update states\n",
    "        states_value = [h, c]\n",
    "\n",
    "\n",
    "    return decoded_sentence\n",
    "\n",
    "input_seq = np.expand_dims(x[0],axis=0)\n",
    "print(input_seq.shape)\n",
    "decoded_sentence = decode_sequence(input_seq)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1, 8)\n",
      "0\n",
      "9\n",
      "1\n",
      "2\n",
      "2\n",
      "2\n",
      "3\n",
      "2\n",
      "4\n",
      "15\n",
      "10\n"
     ]
    }
   ],
   "source": [
    " states_value = encoder_model.predict(input_seq)\n",
    "target_seq = np.zeros(( 1, query_maxlen))\n",
    "target_seq[ 0, 0] = word_idx['\\t']\n",
    "print(target_seq.shape)\n",
    "stop_condition = False\n",
    "decoded_sentence = ''\n",
    "ct =0\n",
    "while not stop_condition:\n",
    "    print(ct)\n",
    "    output_tokens, h, c = decoder_model.predict([target_seq] + states_value)\n",
    "    sampled_token_index = np.argmax(output_tokens[0,ct, :])\n",
    "    sampled_char = inv_map[sampled_token_index]\n",
    "    print(sampled_char)\n",
    "    decoded_sentence +=  sampled_char\n",
    "    if (  len(decoded_sentence) > query_maxlen):\n",
    "        stop_condition = True\n",
    "        print( len(decoded_sentence))\n",
    "    ct+=1\n",
    "    # Update the target sequence (of length 1).\n",
    "    target_seq[0, ct] =sampled_token_index\n",
    "\n",
    "    # Update states\n",
    "    states_value = [h, c]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(sampled_char)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 5 41 39 37 32  3  5 34 42 37 19 38  3  5 43 39 37 14  3  5 21 37 19 39\n",
      "   6  3  2  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0\n",
      "   0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0\n",
      "   0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0\n",
      "   0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0\n",
      "   0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0\n",
      "   0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0\n",
      "   0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0\n",
      "   0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0\n",
      "   0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0\n",
      "   0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0\n",
      "   0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0\n",
      "   0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0\n",
      "   0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0\n",
      "   0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0\n",
      "   0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0\n",
      "   0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0\n",
      "   0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0\n",
      "   0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0\n",
      "   0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0\n",
      "   0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0\n",
      "   0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0\n",
      "   0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0\n",
      "   0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0\n",
      "   0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0\n",
      "   0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0\n",
      "   0  0  0]]\n",
      "-\n",
      "\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "print(input_seq)\n",
    "print(decoded_sentence)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
